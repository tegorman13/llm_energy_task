@article{adamsTrustAutomatedSystems2003,
  title = {Trust in Automated Systems},
  author = {Adams, Barb and Bruyn, L.E. and Houde, S. and Angelopoulos, P. and Iwasa-Madge, K. and McCann, C.},
  date = {2003},
  journaltitle = {Ministry of National Defence},
  pages = {3--7},
  url = {https://cradpdf.drdc-rddc.gc.ca/PDFS/unc17/p520342.pdf},
  abstract = {This report reviews research literature pertaining to trust in automated systems. Based on the review, we argue that trust in automation has many similarities with trust in the interpersonal domain, but also several unique dynamics and influences. Existing research has focused primarily on trust in automation that has an executive or control function, and to a lesser extent, has considered trust in automation that is designed to present information to operators (e.g. decision aids). We maintain that although there are many similarities between trust in automation and interpersonal trust, the dynamics of trust in automation also have some distinct qualities. Several models related to trust in automation have already been developed; in this report, a comprehensive -- although still preliminary -model of trust in military automation is proposed. Several sets of factors are likely to impact on the development of trust in automation, including properties of the automation, properties of the operator, and properties of the context in which interaction with automation occurs. The consequences of trust in automation have yet to be fully explored. Based on this review, measures and methods to study trust in automation are considered, and a program of research to study trust in automated systems is described.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/QNTXHDX2/Adams - TRUST IN AUTOMATED SYSTEMS.pdf}
}

@article{anayatAugmentAutomateImpact2025,
  title = {To Augment or to Automate: Impact of Anthropomorphism on Users’ Choice of Decision Delegation to {{AI-powered}} Agents},
  shorttitle = {To Augment or to Automate},
  author = {Anayat, Shaista and Kaushik, Arun},
  date = {2025-04-29},
  journaltitle = {Behaviour \& Information Technology},
  shortjournal = {Behaviour \& Information Technology},
  pages = {1--19},
  issn = {0144-929X, 1362-3001},
  doi = {10.1080/0144929X.2025.2497441},
  url = {https://www.tandfonline.com/doi/full/10.1080/0144929X.2025.2497441},
  urldate = {2025-05-07},
  abstract = {Understanding how anthropomorphic design influences user decision-making is critical as anthropomorphic technologies become more integrated into daily life and work environments. The existing literature on AI-anthropomorphism mainly focuses on physical or voice-based anthropomorphism and its impact on adoption. However, it ignores mind-based (cognitive and affective) anthropomorphism and its impact on users’ decision delegation (automated or augmented) to AI-systems. This study examines how mind-based anthropomorphism influences users’ decisions to delegate tasks to AI-powered agents, using a survey of 243 actual AI agent users and PLS-SEM for data analysis. Results revealed that users’ choice for augmented decision delegation to AI-powered agents is significantly increased by cognitive anthropomorphism. Also, users’ choice for automated decision delegation to AI-powered agents is positively influenced by affective anthropomorphism. Trust emerged as a crucial mediator between anthropomorphism and automated decision delegation, encouraging users to hand over the decision-making completely to AI-powered agents. Results also revealed decreased reliance on trust when users prefer to retain a certain level of control in the decision-making process. The study significantly contributes to the literature on human-AI interaction by enhancing the understanding of users’ psychology towards anthropomorphism and their choice for decision delegation to AI-powered agents.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/BP7D35I7/Anayat and Kaushik - 2025 - To augment or to automate impact of anthropomorphism on users’ choice of decision delegation to AI-.pdf}
}

@article{attariEnergyConservationGoals2016,
  title = {Energy Conservation Goals: {{What}} People Adopt, What They Recommend, and Why},
  shorttitle = {Energy Conservation Goals},
  author = {Attari, Shahzeen Z. and Krantz, David H. and Weber, Elke U.},
  date = {2016-07},
  journaltitle = {Judgment and Decision Making},
  shortjournal = {Judgm. decis. mak.},
  volume = {11},
  number = {4},
  pages = {342--351},
  issn = {1930-2975},
  doi = {10.1017/S1930297500003776},
  url = {https://www.cambridge.org/core/product/identifier/S1930297500003776/type/journal_article},
  urldate = {2025-05-05},
  abstract = {Failures to reduce greenhouse gas emissions by adopting policies, technologies, and lifestyle changes have led the world to the brink of crisis, or likely beyond. Here we use Internet surveys to attempt to understand these failures by studying factors that affect the adoption of personal energy conservation behaviors and also endorsement of energy conservation goals proposed for others. We demonstrate an asymmetry between goals for self and others (“I’ll do the easy thing, you do the hard thing”), but we show that this asymmetry is partly produced by actor/observer differences: people know what they do already (and generally do not propose those actions as personal goals) and also know their own situational constraints that are barriers to action. We also show, however, that endorsement of conservation goals decreases steeply as a function of perceived difficulty; this suggests a role for motivated cognition as a barrier to conservation: difficult things are perceived as less applicable to one’s situation.},
  langid = {english},
  annotation = {https://www.szattari.com/publications},
  file = {/Users/thomasgorman/Zotero/storage/9K3EKL3E/Attari et al. - 2016 - Energy conservation goals What people adopt, what they recommend, and why.pdf}
}

@article{attariPerceptionsWaterUse2014,
  title = {Perceptions of Water Use},
  author = {Attari, Shahzeen Z.},
  date = {2014-04-08},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {111},
  number = {14},
  pages = {5129--5134},
  doi = {10.1073/pnas.1316402111},
  url = {https://www.pnas.org/doi/10.1073/pnas.1316402111},
  urldate = {2025-05-05},
  abstract = {In a national online survey, 1,020 participants reported their perceptions of water use for household activities. When asked for the most effective strategy they could implement to conserve water in their lives, or what other Americans could do, most participants mentioned curtailment (e.g., taking shorter showers, turning off the water while brushing teeth) rather than efficiency improvements (e.g., replacing toilets, retrofitting washers). This contrasts with expert recommendations. Additionally, some participants are more likely to list curtailment actions for themselves, but list efficiency actions for other Americans. For a sample of 17 activities, participants underestimated water use by a factor of 2 on average, with large underestimates for high water-use activities. An additional ranking task showed poor discrimination of low vs. high embodied water content in food products. High numeracy scores, older age, and male sex were associated with more accurate perceptions of water use. Overall, perception of water use is more accurate than the perception of energy consumption and savings previously reported. Well-designed efforts to improve public understanding of household water use could pay large dividends for behavioral adaptation to temporary or long-term decreases in availability of fresh water.},
  annotation = {https://www.szattari.com/publications\\
\\
https://www.pnas.org/doi/suppl/10.1073/pnas.1316402111/suppl\_file/pnas.201316402si.pdf\\
\\
\\
https://www.pnas.org/doi/10.1073/pnas.1316402111\#supplementary-materials},
  file = {/Users/thomasgorman/Zotero/storage/P5G22WWH/Attari - 2014 - Perceptions of water use.pdf}
}

@article{attariPublicPerceptionsEnergy2010,
  title = {Public Perceptions of Energy Consumption and Savings},
  author = {Attari, Shahzeen Z. and DeKay, Michael L. and Davidson, Cliff I. and Bruine De Bruin, Wändi},
  date = {2010-09-14},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {107},
  number = {37},
  pages = {16054--16059},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1001509107},
  url = {https://pnas.org/doi/full/10.1073/pnas.1001509107},
  urldate = {2024-12-13},
  abstract = {In a national online survey, 505 participants reported their perceptions of energy consumption and savings for a variety of household, transportation, and recycling activities. When asked for the most effective strategy they could implement to conserve energy, most participants mentioned curtailment (e.g., turning off lights, driving less) rather than efficiency improvements (e.g., installing more efficient light bulbs and appliances), in contrast to experts’ recommendations. For a sample of 15 activities, participants underestimated energy use and savings by a factor of 2.8 on average, with small overestimates for low-energy activities and large underestimates for high-energy activities. Additional estimation and ranking tasks also yielded relatively flat functions for perceived energy use and savings. Across several tasks, participants with higher numeracy scores and stronger proenvironmental attitudes had more accurate perceptions. The serious deficiencies highlighted by these results suggest that well-designed efforts to improve the public's understanding of energy use and savings could pay large dividends.},
  langid = {english},
  annotation = {survey items: https://www.pnas.org/action/downloadSupplement?doi=10.1073\%2Fpnas.1001509107\&file=sapp.pdf\\
\\
supplemental: https://www.pnas.org/action/downloadSupplement?doi=10.1073\%2Fpnas.1001509107\&file=pnas.201001509SI.pdf},
  file = {/Users/thomasgorman/Zotero/storage/6UKLYDAK/Attari et al. - 2010 - Public perceptions of energy consumption and savings.pdf}
}

@article{attariReasonsCooperationDefection2014,
  title = {Reasons for Cooperation and Defection in Real-World Social Dilemmas},
  author = {Attari, Shahzeen Z. and Krantz, David H. and Weber, Elke U.},
  date = {2014-07},
  journaltitle = {Judgment and Decision Making},
  volume = {9},
  number = {4},
  pages = {316--334},
  issn = {1930-2975},
  doi = {10.1017/S1930297500006197},
  url = {https://www.cambridge.org/core/journals/judgment-and-decision-making/article/reasons-for-cooperation-and-defection-in-realworld-social-dilemmas/1FDE3EDF6D10DCFDA6215F71D796897C},
  urldate = {2025-05-05},
  abstract = {Interventions to increase cooperation in social dilemmas depend on understanding decision makers’ motivations for cooperation or defection. We examined these in five real-world social dilemmas: situations where private interests are at odds with collective ones. An online survey (N = 929) asked respondents whether or not they cooperated in each social dilemma and then elicited both open-ended reports of reasons for their choices and endorsements of a provided list of reasons. The dilemmas chosen were ones that permit individual action rather than voting or advocacy: (1) conserving energy, (2) donating blood, (3) getting a flu vaccination, (4) donating to National Public Radio (NPR), and (5) buying green electricity. Self-reported cooperation is weakly but positively correlated across these dilemmas. Cooperation in each dilemma correlates fairly strongly with self-reported altruism and with punitive attitudes toward defectors. Some strong domain-specific behaviors and beliefs also correlate with cooperation. The strongest example is frequency of listening to NPR, which predicts donation. Socio-demographic variables relate only weakly to cooperation. Respondents who self-report cooperation usually cite social reasons (including reciprocity) for their choice. Defectors often give self-interest reasons but there are also some domain-specific reasons—some report that they are not eligible to donate blood; some cannot buy green electricity because they do not pay their own electric bills. Cooperators generally report that several of the provided reasons match their actual reasons fairly well, but most defectors endorse none or at most one of the provided reasons for defection. In particular, defectors often view cooperation as costly but do not endorse free riding as a reason for defection. We tentatively conclude that cooperation in these settings is based mostly on pro-social norms and defection on a mixture of self-interest and the possibly motivated perception that situational circumstances prevent cooperation in the given situation.},
  langid = {english},
  keywords = {cooperation,self-interest,social dilemmas,social norms},
  annotation = {https://www.szattari.com/publications},
  file = {/Users/thomasgorman/Zotero/storage/STK898ID/Attari et al. - 2014 - Reasons for cooperation and defection in real-world social dilemmas.pdf}
}

@article{attariReplyFrederickAnchoring2011,
  title = {Reply to {{Frederick}} et al.: {{Anchoring}} Effects on Energy Perceptions},
  shorttitle = {Reply to {{Frederick}} et Al.},
  author = {Attari, Shahzeen Z. and DeKay, Michael L. and Davidson, Cliff I. and De Bruin, Wändi Bruine},
  date = {2011-02-22},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {108},
  number = {8},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1019040108},
  url = {https://pnas.org/doi/full/10.1073/pnas.1019040108},
  urldate = {2025-05-05},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/5CVRJ5AZ/Attari et al. - 2011 - Reply to Frederick et al. Anchoring effects on energy perceptions.pdf}
}

@article{ayanwaleTeachersReadinessIntention2022,
  title = {Teachers’ Readiness and Intention to Teach Artificial Intelligence in Schools},
  author = {Ayanwale, Musa Adekunle and Sanusi, Ismaila Temitayo and Adelana, Owolabi Paul and Aruleba, Kehinde D. and Oyelere, Solomon Sunday},
  date = {2022-01-01},
  journaltitle = {Computers and Education: Artificial Intelligence},
  shortjournal = {Computers and Education: Artificial Intelligence},
  volume = {3},
  pages = {100099},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2022.100099},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000546},
  urldate = {2025-05-02},
  abstract = {The emergence of artificial intelligence (AI) as a subject to be incorporated into K-12 educational levels places new demand on relevant stakeholders, especially teachers that drive the teaching and learning process. It is therefore important to understand how ready teachers are to teach the emerging subject as the success of AI education would probably be closely dependent on the readiness of teachers. As a result, this study presents an insight into factors influencing the behavioural intention and readiness of Nigerian in-service teachers to teach artificial intelligence. A total of 368 teachers, from elementary to high school participated in the study. We utilised quantitative methodology using variance-based structural equation modelling to understand the relationship among the eight variables (AI anxiety, perceived usefulness, AI for social good, Attitude towards using AI, perceived confidence in teaching AI, relevance of AI, AI readiness, and behavioural intention) considered in the study. The result indicated that confidence in teaching AI predicts intention to teach AI while AI relevance strongly predicts readiness to teach AI. While other factors influence the teaching of AI, anxiety and social good could not predict teachers' intention and readiness to implement AI in classrooms respectively. We discussed the implication of our findings in relation to AI implementation in schools and highlight future directions.},
  keywords = {Behavioural intention,K-12,Nigerian schools,Teacher readiness,Teaching artificial intelligence},
  file = {/Users/thomasgorman/Zotero/storage/2YSYWJQ3/Ayanwale et al. - 2022 - Teachers’ readiness and intention to teach artificial intelligence in schools.pdf;/Users/thomasgorman/Zotero/storage/GCQLT4UH/S2666920X22000546.html}
}

@article{babikerAttitudeAIPotential2024,
  title = {Attitude {{Towards AI}}: {{Potential Influence}} of {{Conspiracy Belief}}, {{XAI Experience}} and {{Locus}} of {{Control}}},
  shorttitle = {Attitude {{Towards AI}}},
  author = {Babiker, Areej and Alshakhsi, Sameha and Al-Thani, Dena and Montag, Christian and Ali, Raian},
  date = {2024-10-04},
  journaltitle = {International Journal of Human–Computer Interaction},
  pages = {1--13},
  publisher = {Informa UK Limited},
  issn = {1044-7318, 1532-7590},
  doi = {10.1080/10447318.2024.2401249},
  url = {https://www.tandfonline.com/doi/full/10.1080/10447318.2024.2401249},
  urldate = {2025-05-01},
  abstract = {The proliferation of Artificial Intelligence (AI) technologies, exemplified by Large Language Models (LLM), has ushered in a transformative era across various fields. As the AI revolution will impact societies in complex and uncertain ways, it is likely that persons tending towards belief of conspiracy theories also tend to form more negative and less positive attitudes towards AI. Such persons might believe that some evil force will use AI to destroy human mankind. Drawing on the Interplay of Modality, Person, Area, Country/Culture, and Transparency categories (IMPACT) framework, this study aims to investigate the interplay of locus of control (LOC), belief in conspiracy theories, and the perception of the importance and availability of eXplainable AI (XAI) on attitudes towards AI (measured via AI acceptance and fear). The study used an online survey with 281 participants from the UK and 281 from the Arab world. Statistical analysis revealed that in the UK but not in the Arab sample, female participants reported higher fear of AI and lower acceptance of AI compared to males. The regression results consistently confirmed the role of internal LOC, perceived XAI importance, and perceived availability of XAI in fostering AI acceptance, as well as the role of belief in conspiracy theories, external LOC, and perceiving availability of XAI as being low in increasing fear of AI. The perceived availability of XAI emerges as a crucial influencing factor; addressing it appropriately could enhance societal awareness and acceptance of AI while reducing fear. Personal factors and XAI influence attitudes towards AI in both Arab and UK cultures, enhancing result robustness and revealing nuanced differences.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/LSANB5CZ/Babiker et al. - 2024 - Attitude Towards AI Potential Influence of Conspiracy Belief, XAI Experience and Locus of Control.pdf}
}

@article{bewersdorffAIAdvocatesCautious2025,
  title = {{{AI}} Advocates and Cautious Critics: {{How AI}} Attitudes, {{AI}} Interest, Use of {{AI}}, and {{AI}} Literacy Build University Students' {{AI}} Self-Efficacy},
  shorttitle = {{{AI}} Advocates and Cautious Critics},
  author = {Bewersdorff, Arne and Hornberger, Marie and Nerdel, Claudia and Schiff, Daniel S.},
  date = {2025-06-01},
  journaltitle = {Computers and Education: Artificial Intelligence},
  shortjournal = {Computers and Education: Artificial Intelligence},
  volume = {8},
  pages = {100340},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2024.100340},
  url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001437},
  urldate = {2025-05-02},
  abstract = {This study investigates how cognitive, affective, and behavioral variables related to artificial intelligence (AI) build AI self-efficacy among university students. Based on these variables, we identify three meaningful student groups, which can guide educational initiatives. We recruited 1465 undergraduate and graduate students from the United States, the United Kingdom, and Germany and measured their AI self-efficacy, AI literacy, interest in AI, attitudes towards AI, and AI use. Using a path model, we examine the correlations and paths among these variables. Results reveal that AI usage and positive AI attitudes significantly predict interest in AI, which in turn and together with AI literacy, enhance AI self-efficacy. Moreover, using Gaussian Mixture Models, we identify three groups of students: 'AI Advocates,' 'Cautious Critics,' and 'Pragmatic Observers,' each exhibiting unique patterns of AI-related cognitive, affective, and behavioral traits. Our findings demonstrate the necessity of educational strategies that not only focus on AI literacy but also aim to foster students' AI attitudes, usage, and interest to effectively promote AI self-efficacy. Furthermore, we argue that educators who aim to design inclusive AI educational programs should take into account the distinct needs of different student groups identified in this study.},
  keywords = {AI education,AI literacy,AI self-efficacy,Artificial intelligence,Gaussian mixture model,Higher education,Structural equation model},
  file = {/Users/thomasgorman/Zotero/storage/MSKS6VGF/Bewersdorff et al. - 2025 - AI advocates and cautious critics How AI attitudes, AI interest, use of AI, and AI literacy build u.pdf;/Users/thomasgorman/Zotero/storage/L7QZGD9I/S2666920X24001437.html}
}

@article{brandsmaOneAllImpact2019,
  title = {One for All? – {{The}} Impact of Different Types of Energy Feedback and Goal Setting on Individuals’ Motivation to Conserve Electricity},
  shorttitle = {One for All?},
  author = {Brandsma, Jeroen S. and Blasch, Julia E.},
  date = {2019-12-01},
  journaltitle = {Energy Policy},
  shortjournal = {Energy Policy},
  volume = {135},
  pages = {110992},
  issn = {0301-4215},
  doi = {10.1016/j.enpol.2019.110992},
  url = {https://www.sciencedirect.com/science/article/pii/S0301421519305798},
  urldate = {2025-03-16},
  abstract = {We investigate how different types of energy feedback, combined with goal setting, impact on consumers' motivation to conserve electricity. Using an online survey, we test the influence of energy feedback in physical units (kWh), monetary values (EUR) and environmental values (avoided CO2 emissions). We asked participants to set themselves either a high, low or no energy conservation goal. In addition, we assess the respondents’ value types - hedonic, egoistic, altruistic and biospheric – to test predictions derived from goal framing theory. In general, individuals scoring high on biospheric values were more motivated to conserve electricity and their motivation did not increase in response to setting an energy conservation goal. Individuals with egoistic values seem less willing to reduce their electricity consumption, unless in the monetary feedback or high goal conditions. A high conservation goal was only found to be effective in combination with monetary feedback: it increased the motivation to save electricity by 6.7 percentage points in comparison to the low goal condition and 6.6 percentage points in comparison to the control condition.},
  keywords = {Energy conservation behaviour,Energy feedback,Goal framing,Goal setting,Value orientation},
  annotation = {https://data.mendeley.com/datasets/zgy44yc22r/1},
  file = {/Users/thomasgorman/Zotero/storage/B8DRDJWQ/Brandsma and Blasch - 2019 - One for all – The impact of different types of energy feedback and goal setting on individuals’ mot.pdf;/Users/thomasgorman/Zotero/storage/9XHXBST5/S0301421519305798.html}
}

@article{camilleriConsumersUnderestimateEmissions2019,
  title = {Consumers Underestimate the Emissions Associated with Food but Are Aided by Labels},
  author = {Camilleri, Adrian R. and Larrick, Richard P. and Hossain, Shajuti and Patino-Echeverri, Dalia},
  date = {2019-01},
  journaltitle = {Nature Climate Change},
  shortjournal = {Nature Clim Change},
  volume = {9},
  number = {1},
  pages = {53--58},
  issn = {1758-6798},
  doi = {10.1038/s41558-018-0354-z},
  url = {https://www.nature.com/articles/s41558-018-0354-z},
  urldate = {2025-05-07},
  abstract = {Food production is a major cause of energy use and GHG emissions, and therefore diet change is an important behavioural strategy for reducing associated environmental impacts. However, a severe obstacle to diet change may be consumers’ underestimation of the environmental impacts of different types of food. Here we show that energy consumption and GHG emission estimates are significantly underestimated for foods, suggesting a possible blind spot suitable for intervention. In a second study, we find that providing consumers with information regarding the GHG emissions associated with the life cycle of food, presented in terms of a familiar reference unit (light-bulb minutes), shifts their actual purchase choices away from higher-emission options. Thus, although consumers’ poor understanding of the food system is a barrier to reducing energy use and GHG emissions, it also represents a promising area for simple interventions such as a well-designed carbon label.},
  langid = {english},
  keywords = {Attribution,Carbon and energy,Decision making,Psychology},
  annotation = {https://osf.io/smj67/files/osfstorage\\
\\
\\
https://static-content.springer.com/esm/art\%3A10.1038\%2Fs41558-018-0354-z/MediaObjects/41558\_2018\_354\_MOESM1\_ESM.pdf},
  file = {/Users/thomasgorman/Zotero/storage/3GLM9TYY/Camilleri et al. - 2019 - Consumers underestimate the emissions associated with food but are aided by labels.pdf}
}

@article{canfieldPerceptionsElectricityuseCommunications2017,
  title = {Perceptions of Electricity-Use Communications: Effects of Information, Format, and Individual Differences},
  shorttitle = {Perceptions of Electricity-Use Communications},
  author = {Canfield, Casey and Bruine De Bruin, Wändi and Wong-Parodi, Gabrielle},
  date = {2017-09-02},
  journaltitle = {Journal of Risk Research},
  shortjournal = {Journal of Risk Research},
  volume = {20},
  number = {9},
  pages = {1132--1153},
  issn = {1366-9877, 1466-4461},
  doi = {10.1080/13669877.2015.1121909},
  url = {https://www.tandfonline.com/doi/full/10.1080/13669877.2015.1121909},
  urldate = {2024-10-02},
  abstract = {Electricity bills could be an effective strategy for improving communications about consumers' electricity use and promoting electricity savings. However, quantitative communications about electricity use may be difficult to understand, especially for consumers with low energy literacy. Here, we build on the health communication and graph comprehension literature to inform electricity bill design, with the goal of improving understanding, preferences for the presented communication, and intentions to save electricity. In a survey-based experiment, each participant saw a hypothetical electricity bill for a family with relatively high electricity use, covering information about (a) historical use, (b) comparisons to neighbors, and (c) historical use with appliance breakdown. Participants saw all information types in one of three formats including (a) tables, (b) bar graphs, and (c) icon graphs. We report on three main findings. First, consumers understood each type of electricity-use information the most when it was presented in a table, perhaps because tables facilitate simple point reading. Second, preferences and intentions to save electricity were the strongest for the historical use information, independent of format. Third, individuals with lower energy literacy understood all information less. We discuss implications for designing utility bills that are understandable, perceived as useful, and motivate consumers to save energy.},
  langid = {english},
  annotation = {docx supp file:\\
\\
https://www.tandfonline.com/doi/suppl/10.1080/13669877.2015.1121909?scroll=top},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Canfield et al_2017_Perceptions of electricity-use communications.pdf}
}

@article{carolusMAILSMetaAI2023,
  title = {{{MAILS}} - {{Meta AI}} Literacy Scale: {{Development}} and Testing of an {{AI}} Literacy Questionnaire Based on Well-Founded Competency Models and Psychological Change- and Meta-Competencies},
  shorttitle = {{{MAILS}} - {{Meta AI}} Literacy Scale},
  author = {Carolus, Astrid and Koch, Martin J. and Straka, Samantha and Latoschik, Marc Erich and Wienrich, Carolin},
  date = {2023-08-01},
  journaltitle = {Computers in Human Behavior: Artificial Humans},
  shortjournal = {Computers in Human Behavior: Artificial Humans},
  volume = {1},
  number = {2},
  pages = {100014},
  issn = {2949-8821},
  doi = {10.1016/j.chbah.2023.100014},
  url = {https://www.sciencedirect.com/science/article/pii/S2949882123000142},
  urldate = {2025-04-28},
  abstract = {Valid measurement of AI literacy is important for the selection of personnel, identification of shortages in skill and knowledge, and evaluation of AI literacy interventions. A questionnaire is missing that is deeply grounded in the existing literature on AI literacy, is modularly applicable depending on the goals, and includes further psychological competencies in addition to the typical facets of AIL. This paper presents the development and validation of a questionnaire considering the desiderata described above. We derived items to represent different facets of AI literacy and psychological competencies, such as problem-solving, learning, and emotion regulation in regard to AI. We collected data from 300 German-speaking adults to confirm the factorial structure. The result is the Meta AI Literacy Scale (MAILS) for AI literacy with the facets Use \& apply AI, Understand AI, Detect AI, and AI Ethics and the ability to Create AI as a separate construct, and AI Self-efficacy in learning and problem-solving and AI Self-management (i.e., AI persuasion literacy and emotion regulation). This study contributes to the research on AI literacy by providing a measurement instrument relying on profound competency models. Psychological competencies are included particularly important in the context of pervasive change through AI systems.},
  keywords = {AI literacy,Competence modeling,Psychological competencies,Questionnaire development,Self-efficacy},
  annotation = {https://hci.uni-wuerzburg.de/research/MAILS/\\
\\
https://downloads.hci.informatik.uni-wuerzburg.de/MAILS-qustionnaire-english.pdf\\
\\
https://downloads.hci.informatik.uni-wuerzburg.de/MAILS-short-questionnaire-english.pdf},
  file = {/Users/thomasgorman/Zotero/storage/7A66XBBY/Carolus et al. - 2023 - MAILS - Meta AI literacy scale Development and testing of an AI literacy questionnaire based on wel.pdf;/Users/thomasgorman/Zotero/storage/MDEFHI9M/S2949882123000142.html}
}

@online{casolinEvaluatingInfluencesExplanation2024,
  title = {Evaluating the {{Influences}} of {{Explanation Style}} on {{Human-AI Reliance}}},
  author = {Casolin, Emma and Salim, Flora D. and Newell, Ben},
  date = {2024-10-26},
  eprint = {2410.20067},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2410.20067},
  urldate = {2024-11-01},
  abstract = {Explainable AI (XAI) aims to support appropriate human-AI reliance by increasing the interpretability of complex model decisions. Despite the proliferation of proposed methods, there is mixed evidence surrounding the effects of different styles of XAI explanations on human-AI reliance. Interpreting these conflicting findings requires an understanding of the individual and combined qualities of different explanation styles that influence appropriate and inappropriate human-AI reliance, and the role of interpretability in this interaction. In this study, we investigate the influences of feature-based, example-based, and combined feature- and example-based XAI methods on human-AI reliance through a two-part experimental study with 274 participants comparing these explanation style conditions. Our findings suggest differences between feature-based and example-based explanation styles beyond interpretability that affect human-AI reliance patterns across differences in individual performance and task complexity. Our work highlights the importance of adapting explanations to their specific users and context over maximising broad interpretability.},
  pubstate = {prepublished},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/B3PSHMRP/Casolin et al. - 2024 - Evaluating the Influences of Explanation Style on Human-AI Reliance.pdf;/Users/thomasgorman/Zotero/storage/9IPQFWPM/2410.html}
}

@online{chenEvaluatingHumanTrust2025,
  title = {Evaluating {{Human Trust}} in {{LLM-Based Planners}}: {{A Preliminary Study}}},
  shorttitle = {Evaluating {{Human Trust}} in {{LLM-Based Planners}}},
  author = {Chen, Shenghui and Yang, Yunhao and Boggess, Kayla and Heo, Seongkook and Feng, Lu and Topcu, Ufuk},
  date = {2025-02-27},
  eprint = {2502.20284},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.20284},
  url = {http://arxiv.org/abs/2502.20284},
  urldate = {2025-05-04},
  abstract = {Large Language Models (LLMs) are increasingly used for planning tasks, offering unique capabilities not found in classical planners such as generating explanations and iterative refinement. However, trust--a critical factor in the adoption of planning systems--remains underexplored in the context of LLM-based planning tasks. This study bridges this gap by comparing human trust in LLM-based planners with classical planners through a user study in a Planning Domain Definition Language (PDDL) domain. Combining subjective measures, such as trust questionnaires, with objective metrics like evaluation accuracy, our findings reveal that correctness is the primary driver of trust and performance. Explanations provided by the LLM improved evaluation accuracy but had limited impact on trust, while plan refinement showed potential for increasing trust without significantly enhancing evaluation accuracy.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/4ZX8BZJW/Chen et al. - 2025 - Evaluating Human Trust in LLM-Based Planners A Preliminary Study.pdf;/Users/thomasgorman/Zotero/storage/UWH7P8F5/2502.html}
}

@online{chenMissingPiecesHow2025,
  title = {Missing {{Pieces}}: {{How Do Designs}} That {{Expose Uncertainty Longitudinally Impact Trust}} in {{AI Decision Aids}}? {{An In Situ Study}} of {{Gig Drivers}}},
  shorttitle = {Missing {{Pieces}}},
  author = {Chen, Rex and Wang, Ruiyi and Sadeh, Norman and Fang, Fei},
  date = {2025-04-16},
  eprint = {2404.06432},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.06432},
  url = {http://arxiv.org/abs/2404.06432},
  urldate = {2025-05-08},
  abstract = {Decision aids based on artificial intelligence (AI) induce a wide range of outcomes when they are deployed in uncertain environments. In this paper, we investigate how users' trust in recommendations from an AI decision aid is impacted over time by designs that expose uncertainty in predicted outcomes. Unlike previous work, we focus on gig driving - a real-world, repeated decision-making context. We report on a longitudinal mixed-methods study (\$n=51\$) where we measured gig drivers' trust as they interacted with an AI-based schedule recommendation tool. Our results show that participants' trust in the tool was shaped by both their first impressions of its accuracy and their longitudinal interactions with it; and that task-aligned framings of uncertainty improved trust by allowing participants to incorporate uncertainty into their decision-making processes. Additionally, we observed that trust depended on their characteristics as drivers, underscoring the need for more in situ studies of AI decision aids.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/SBRGNSS3/Chen et al. - 2025 - Missing Pieces How Do Designs that Expose Uncertainty Longitudinally Impact Trust in AI Decision Ai.pdf;/Users/thomasgorman/Zotero/storage/LGX2GZ32/2404.html}
}

@inproceedings{chenPortrayingLargeLanguage2025,
  title = {Portraying {{Large Language Models}} as {{Machines}}, {{Tools}}, or {{Companions Affects What Mental Capacities Humans Attribute}} to {{Them}}},
  booktitle = {Proceedings of the {{Extended Abstracts}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chen, Allison and Kim, Sunnie S. Y. and Dharmasiri, Amaya and Russakovsky, Olga and Fan, Judith E.},
  date = {2025-04-26},
  pages = {1--14},
  publisher = {ACM},
  location = {Yokohama Japan},
  doi = {10.1145/3706599.3719710},
  url = {https://dl.acm.org/doi/10.1145/3706599.3719710},
  urldate = {2025-04-30},
  abstract = {As large language models (LLMs) become increasingly popular and prevalent in media and daily conversations, individuals encounter different portrayals of LLMs from various sources. It is important to understand how these portrayals can shape their beliefs about LLMs as this can have downstream impacts on adoption and usage behaviors. In this work, we investigate what mental capacities individuals attribute to LLMs after being exposed to short videos adopting one of three portrayals: mechanistic (LLMs as machines), functional (LLMs as tools), and intentional (LLMs as companions). We find that the intentional portrayal increases the attribution of mental capacities to LLMs, and that individuals tend to attribute mind-related capacities the most, followed by heart- then bodyrelated capacities. We discuss the implications of these findings, provide recommendations on how to portray LLMs, and outline directions for future research.},
  eventtitle = {{{CHI EA}} '25: {{Extended Abstracts}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/VVVF3TQX/Chen et al. - 2025 - Portraying Large Language Models as Machines, Tools, or Companions Affects What Mental Capacities Hu.pdf}
}

@incollection{chisikImageElectricityUnderstanding2011,
  title = {An {{Image}} of {{Electricity}}: {{Towards}} an {{Understanding}} of {{How People Perceive Electricity}}},
  shorttitle = {An {{Image}} of {{Electricity}}},
  booktitle = {Human-{{Computer Interaction}} – {{INTERACT}} 2011},
  author = {Chisik, Yoram},
  editor = {Campos, Pedro and Graham, Nicholas and Jorge, Joaquim and Nunes, Nuno and Palanque, Philippe and Winckler, Marco},
  editora = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard},
  editoratype = {redactor},
  date = {2011},
  volume = {6949},
  pages = {100--117},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-23768-3_9},
  url = {http://link.springer.com/10.1007/978-3-642-23768-3_9},
  urldate = {2025-05-05},
  abstract = {Although an enormous amount of research effort has been devoted to understanding people’s energy consumption habits, visualizing their consumption and finding ways of motivating them towards more sustainable behaviours we are still in the dark with regards to people’s basic perception of electricity, their concept of what electricity is and their notion of the consumption rates of various electrical devices. In this study we have employed a sketching methodology to elicit people’s basic mental image of what electricity is, how they conceive of the electrical infrastructure in their home and which devices they think represent the largest drain on their wallets. Preliminary analysis of the results show that people do not have a clear mental model of electricity and tend to associate the size of the device and the duration of use with higher rates of consumption regardless of the type of device, the type of use it is put to and its actual consumption level.},
  isbn = {978-3-642-23767-6 978-3-642-23768-3},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/NEQC9UR2/Chisik - 2011 - An Image of Electricity Towards an Understanding of How People Perceive Electricity.pdf}
}

@article{choudhuryImpactPerformanceExpectancy2024,
  title = {The {{Impact}} of {{Performance Expectancy}}, {{Workload}}, {{Risk}}, and {{Satisfaction}} on {{Trust}} in {{ChatGPT}}: {{Cross-Sectional Survey Analysis}}},
  shorttitle = {The {{Impact}} of {{Performance Expectancy}}, {{Workload}}, {{Risk}}, and {{Satisfaction}} on {{Trust}} in {{ChatGPT}}},
  author = {Choudhury, Avishek and Shamszare, Hamid},
  date = {2024-05-27},
  journaltitle = {JMIR human factors},
  shortjournal = {JMIR Hum Factors},
  volume = {11},
  eprint = {38801658},
  eprinttype = {pubmed},
  pages = {e55399},
  issn = {2292-9495},
  doi = {10.2196/55399},
  abstract = {BACKGROUND: ChatGPT (OpenAI) is a powerful tool for a wide range of tasks, from entertainment and creativity to health care queries. There are potential risks and benefits associated with this technology. In the discourse concerning the deployment of ChatGPT and similar large language models, it is sensible to recommend their use primarily for tasks a human user can execute accurately. As we transition into the subsequent phase of ChatGPT deployment, establishing realistic performance expectations and understanding users' perceptions of risk associated with its use are crucial in determining the successful integration of this artificial intelligence (AI) technology. OBJECTIVE: The aim of the study is to explore how perceived workload, satisfaction, performance expectancy, and risk-benefit perception influence users' trust in ChatGPT. METHODS: A semistructured, web-based survey was conducted with 607 adults in the United States who actively use ChatGPT. The survey questions were adapted from constructs used in various models and theories such as the technology acceptance model, the theory of planned behavior, the unified theory of acceptance and use of technology, and research on trust and security in digital environments. To test our hypotheses and structural model, we used the partial least squares structural equation modeling method, a widely used approach for multivariate analysis. RESULTS: A total of 607 people responded to our survey. A significant portion of the participants held at least a high school diploma (n=204, 33.6\%), and the majority had a bachelor's degree (n=262, 43.1\%). The primary motivations for participants to use ChatGPT were for acquiring information (n=219, 36.1\%), amusement (n=203, 33.4\%), and addressing problems (n=135, 22.2\%). Some participants used it for health-related inquiries (n=44, 7.2\%), while a few others (n=6, 1\%) used it for miscellaneous activities such as brainstorming, grammar verification, and blog content creation. Our model explained 64.6\% of the variance in trust. Our analysis indicated a significant relationship between (1) workload and satisfaction, (2) trust and satisfaction, (3) performance expectations and trust, and (4) risk-benefit perception and trust. CONCLUSIONS: The findings underscore the importance of ensuring user-friendly design and functionality in AI-based applications to reduce workload and enhance user satisfaction, thereby increasing user trust. Future research should further explore the relationship between risk-benefit perception and trust in the context of AI chatbots.},
  langid = {english},
  pmcid = {PMC11165287},
  keywords = {Adult,algorithms,artificial intelligence,Artificial Intelligence,chatbots,ChatGPT,Cross-Sectional Studies,cross-sectional survey,decision-making,deep learning,Female,health care,health care decision-making,health care management,health-related decision-making,Humans,Male,Middle Aged,Personal Satisfaction,practical models,predictive analytics,predictive models,predictive system,Risk Assessment,Surveys and Questionnaires,Trust,United States,usability,usable,usableness,usefulness,user perception,Workload},
  file = {/Users/thomasgorman/Zotero/storage/34MP5ADR/Choudhury and Shamszare - 2024 - The Impact of Performance Expectancy, Workload, Risk, and Satisfaction on Trust in ChatGPT Cross-Se.pdf}
}

@article{choudhuryInvestigatingImpactUser2023,
  title = {Investigating the {{Impact}} of {{User Trust}} on the {{Adoption}} and {{Use}} of {{ChatGPT}}: {{Survey Analysis}}},
  shorttitle = {Investigating the {{Impact}} of {{User Trust}} on the {{Adoption}} and {{Use}} of {{ChatGPT}}},
  author = {Choudhury, Avishek and Shamszare, Hamid},
  date = {2023-06-14},
  journaltitle = {Journal of Medical Internet Research},
  shortjournal = {J Med Internet Res},
  volume = {25},
  eprint = {37314848},
  eprinttype = {pubmed},
  pages = {e47184},
  issn = {1439-4456},
  doi = {10.2196/47184},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10337387/},
  urldate = {2025-05-02},
  abstract = {Background ChatGPT (Chat Generative Pre-trained Transformer) has gained popularity for its ability to generate human-like responses. It is essential to note that overreliance or blind trust in ChatGPT, especially in high-stakes decision-making contexts, can have severe consequences. Similarly, lacking trust in the technology can lead to underuse, resulting in missed opportunities. Objective This study investigated the impact of users’ trust in ChatGPT on their intent and actual use of the technology. Four hypotheses were tested: (1) users’ intent to use ChatGPT increases with their trust in the technology; (2) the actual use of ChatGPT increases with users’ intent to use the technology; (3) the actual use of ChatGPT increases with users’ trust in the technology; and (4) users’ intent to use ChatGPT can partially mediate the effect of trust in the technology on its actual use. Methods This study distributed a web-based survey to adults in the United States who actively use ChatGPT (version 3.5) at least once a month between February 2023 through March 2023. The survey responses were used to develop 2 latent constructs: Trust and Intent to Use, with Actual Use being the outcome variable. The study used partial least squares structural equation modeling to evaluate and test the structural model and hypotheses. Results In the study, 607 respondents completed the survey. The primary uses of ChatGPT were for information gathering (n=219, 36.1\%), entertainment (n=203, 33.4\%), and problem-solving (n=135, 22.2\%), with a smaller number using it for health-related queries (n=44, 7.2\%) and other activities (n=6, 1\%). Our model explained 50.5\% and 9.8\% of the variance in Intent to Use and Actual Use, respectively, with path coefficients of 0.711 and 0.221 for Trust on Intent to Use and Actual Use, respectively. The bootstrapped results failed to reject all 4 null hypotheses, with Trust having a significant direct effect on both Intent to Use (β=0.711, 95\% CI 0.656-0.764) and Actual Use (β=0.302, 95\% CI 0.229-0.374). The indirect effect of Trust on Actual Use, partially mediated by Intent to Use, was also significant (β=0.113, 95\% CI 0.001-0.227). Conclusions Our results suggest that trust is critical to users’ adoption of ChatGPT. It remains crucial to highlight that ChatGPT was not initially designed for health care applications. Therefore, an overreliance on it for health-related advice could potentially lead to misinformation and subsequent health risks. Efforts must be focused on improving the ChatGPT’s ability to distinguish between queries that it can safely handle and those that should be redirected to human experts (health care professionals). Although risks are associated with excessive trust in artificial intelligence–driven chatbots such as ChatGPT, the potential risks can be reduced by advocating for shared accountability and fostering collaboration between developers, subject matter experts, and human factors researchers.},
  pmcid = {PMC10337387},
  file = {/Users/thomasgorman/Zotero/storage/LIPMFQN3/Choudhury and Shamszare - 2023 - Investigating the Impact of User Trust on the Adoption and Use of ChatGPT Survey Analysis.pdf}
}

@article{cottonReducingEnergyDemand2021,
  title = {Reducing Energy Demand in {{China}} and the {{United Kingdom}}: {{The}} Importance of Energy Literacy},
  shorttitle = {Reducing Energy Demand in {{China}} and the {{United Kingdom}}},
  author = {Cotton, D. R. E. and Zhai, J. and Miller, W. and Dalla Valle, L. and Winter, J.},
  date = {2021-01-01},
  journaltitle = {Journal of Cleaner Production},
  shortjournal = {Journal of Cleaner Production},
  volume = {278},
  pages = {123876},
  issn = {0959-6526},
  doi = {10.1016/j.jclepro.2020.123876},
  url = {https://www.sciencedirect.com/science/article/pii/S0959652620339214},
  urldate = {2025-05-02},
  abstract = {As the impacts of climate change become increasingly visible across the globe, awareness of the need for cleaner energy and demand reduction is growing. Energy literacy offers a strong potential for explaining and predicting energy-related behaviours, yet research and policies focused on this topic remain limited. In this study, energy literacy was measured in a sample of 2806 university students in the United Kingdom and China, in addition to their wider environmental attitudes using the New Ecological Paradigm scale. Findings indicate that energy literacy was relatively high overall, but there were significant differences between the knowledge, attitudes and behavioural intentions of participants in the two countries. Whilst the UK respondents rated themselves significantly more highly on perceived knowledge of energy issues, Chinese respondents provided significantly more correct answers in a knowledge test. UK respondents demonstrated more positive attitudes towards energy conservation than those from China, and were more likely to report energy-saving behaviours. However, Chinese respondents exhibited higher levels of trust in government and businesses to take action on energy issues. This paper provides a novel insight into cultural differences which may be crucial to policy and practice, and evidences the potential benefits of utilising a combination of educational and structural change to support transition to a cleaner, low-energy society.},
  keywords = {Attitude,Behaviour,Energy literacy,Higher education,Knowledge,Policy},
  file = {/Users/thomasgorman/Zotero/storage/7CFDWEMU/Cotton et al. - 2021 - Reducing energy demand in China and the United Kingdom The importance of energy literacy.pdf;/Users/thomasgorman/Zotero/storage/BDY7UNPH/S0959652620339214.html}
}

@article{dewatersDesigningEnergyLiteracy2013,
  title = {Designing an {{Energy Literacy Questionnaire}} for {{Middle}} and {{High School Youth}}},
  author = {DeWaters, Jan and Qaqish, Basil and Graham, Mary and Powers, Susan},
  date = {2013-01},
  journaltitle = {The Journal of Environmental Education},
  shortjournal = {The Journal of Environmental Education},
  volume = {44},
  number = {1},
  pages = {56--78},
  issn = {0095-8964, 1940-1892},
  doi = {10.1080/00958964.2012.682615},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00958964.2012.682615},
  urldate = {2025-04-30},
  abstract = {A measurement scale has been developed to assess secondary students’ energy literacy—a citizenship understanding of energy that includes cognitive as well as affective and behavioral items. Instrument development procedures followed psychometric principles from educational and social psychology research. Initial exploration of the measure yielded promising results: internal consistencies for the cognitive, affective, and behavioral subscales, measured by Cronbach's α, ranged from 0.75 to 0.83; average discrimination indices ranged from 0.27 to 0.46. The instrument's validity was supported with contrasted-groups and developmental-age progression comparisons, as well as factor analyses. The energy literacy questionnaire provides an opportunity to measure baseline levels of energy literacy and to assess broader impacts of educational interventions.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/BXGK3SBU/DeWaters et al. - 2013 - Designing an Energy Literacy Questionnaire for Middle and High School Youth.pdf}
}

@article{dewatersEnergyLiteracySecondary2011,
  title = {Energy Literacy of Secondary Students in {{New York State}} ({{USA}}): {{A}} Measure of Knowledge, Affect, and Behavior},
  shorttitle = {Energy Literacy of Secondary Students in {{New York State}} ({{USA}})},
  author = {DeWaters, Jan E. and Powers, Susan E.},
  date = {2011-03-01},
  journaltitle = {Energy Policy},
  shortjournal = {Energy Policy},
  volume = {39},
  number = {3},
  pages = {1699--1710},
  issn = {0301-4215},
  doi = {10.1016/j.enpol.2010.12.049},
  url = {https://www.sciencedirect.com/science/article/pii/S0301421511000073},
  urldate = {2024-12-11},
  abstract = {Energy literacy, which encompasses broad content knowledge as well as affective and behavioral characteristics, will empower people to make appropriate energy-related choices and embrace changes in the way we harness and consume energy. Energy literacy was measured with a written questionnaire completed by 3708 secondary students in New York State, USA. Results indicate that students are concerned about energy problems (affective subscale mean 73\% of the maximum attainable score), yet relatively low cognitive (42\% correct) and behavioral (65\% of the maximum) scores suggest that students may lack the knowledge and skills they need to effectively contribute toward solutions. High school (HS) students scored significantly better than middle school (MS) students on the cognitive subscale; gains were greatest on topics included in NY State educational standards, and less on topics related to “practical” energy knowledge such as ways to save energy. Despite knowledge gains, there was a significant drop in energy conservation behavior between the MS and HS students. Intercorrelations between groups of questions indicate energy-related behaviors are more strongly related to affect than to knowledge. These findings underscore the need for education that improves energy literacy by impacting student attitudes, values and behaviors, as well as broad content knowledge.},
  keywords = {Assessment,Energy education,Energy literacy},
  annotation = {https://ars-els-cdn-com.proxyiub.uits.iu.edu/content/image/1-s2.0-S0301421511000073-mmc1.doc},
  file = {/Users/thomasgorman/Zotero/storage/QKSQ2SPA/DeWaters and Powers - 2011 - Energy literacy of secondary students in New York State (USA) A measure of knowledge, affect, and b.pdf;/Users/thomasgorman/Zotero/storage/6W7G5PQ9/S0301421511000073.html}
}

@article{dewatersEstablishingMeasurementCriteria2013,
  title = {Establishing {{Measurement Criteria}} for an {{Energy Literacy Questionnaire}}},
  author = {DeWaters, Jan and Powers, Susan},
  date = {2013-01},
  journaltitle = {The Journal of Environmental Education},
  shortjournal = {The Journal of Environmental Education},
  volume = {44},
  number = {1},
  pages = {38--55},
  issn = {0095-8964, 1940-1892},
  doi = {10.1080/00958964.2012.711378},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00958964.2012.711378},
  urldate = {2025-04-30},
  abstract = {Energy literacy is a broad term encompassing content knowledge as well as a citizenship understanding of energy that includes affective and behavioral aspects. This article presents explicit criteria that will serve as a foundation for developing measurable objectives for energy literacy in three dimensions: cognitive (knowledge, cognitive skills), affective (attitude, values, personal responsibility); and behavioral. The outcome of this research is a framework from which a quantitative survey of energy literacy for secondary students in New York State, United States, can be created. Efforts supported by this research may help assess the broader impacts of educational programs in terms of their effectiveness for improving students’ energy literacy.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/SFF88D6A/DeWaters and Powers - 2013 - Establishing Measurement Criteria for an Energy Literacy Questionnaire.pdf}
}

@article{dingInteractivityHumannessTrust2024,
  title = {Interactivity, Humanness, and Trust: A Psychological Approach to {{AI}} Chatbot Adoption in e-Commerce},
  shorttitle = {Interactivity, Humanness, and Trust},
  author = {Ding, Yi and Najaf, Muzammil},
  date = {2024-10-28},
  journaltitle = {BMC Psychology},
  shortjournal = {BMC Psychology},
  volume = {12},
  number = {1},
  pages = {595},
  issn = {2050-7283},
  doi = {10.1186/s40359-024-02083-z},
  url = {https://doi.org/10.1186/s40359-024-02083-z},
  urldate = {2025-05-02},
  abstract = {This study aims to investigate the impact of interactivity and perceived humanness on trust toward AI chatbots in the e-commerce setting. Moreover, this study also aims to examine the mediation effect of trust toward AI chatbots in the relationship between interactivity and intention to adopt AI chatbots for e-commerce as well as in the relationship between perceived humanness and intention to adopt chatbots for e-commerce. This study used a time lag approach to collect the data from 343 customers from the southern region of China. The data were collected online through a questionnaire designed in Chinese language using a survey firm. The findings of this study indicated that there is a significant impact of interactivity and humanness on the trust toward chatbots. Moreover, the findings of this study indicated that there is a significant mediating effect of trust toward chatbots in the relationships of interactivity and perceived humanness to adopt chatbots for e-commerce. In addition, this study found a significant moderating influence on the perceived enjoyment of using chatbots in e-commerce settings. This study provides a unique perspective of expectation-confirmation theory for adopting emerging technologies for online shopping and also provides insights for designers and business firms to develop businesses to facilitate the AI chatbot feature for e-commerce.},
  keywords = {Expectation-confirmation theory,Intention to adopt Chatbot,Interactivity,Perceived enjoyment,Perceived humanness,Trust toward Chatbot},
  file = {/Users/thomasgorman/Zotero/storage/BT6VYK4V/Ding and Najaf - 2024 - Interactivity, humanness, and trust a psychological approach to AI chatbot adoption in e-commerce.pdf;/Users/thomasgorman/Zotero/storage/NHQB9LSQ/s40359-024-02083-z.html}
}

@online{duroMeasuringIdentifyingFactors2025,
  title = {Measuring and Identifying Factors of Individuals' Trust in {{Large Language Models}}},
  author = {Duro, Edoardo Sebastiano De and Veltri, Giuseppe Alessandro and Golino, Hudson and Stella, Massimo},
  date = {2025-03-05},
  eprint = {2502.21028},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.21028},
  url = {http://arxiv.org/abs/2502.21028},
  urldate = {2025-03-21},
  abstract = {Large Language Models (LLMs) can engage in human-looking conversational exchanges. Although conversations can elicit trust between users and LLMs, scarce empirical research has examined trust formation in human-LLM contexts, beyond LLMs' trustworthiness or human trust in AI in general. Here, we introduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure individuals' trust in LLMs, extending McAllister's cognitive and affective trust dimensions to LLM-human interactions. We developed TILLMI as a psychometric scale, prototyped with a novel protocol we called LLM-simulated validity. The LLM-based scale was then validated in a sample of 1,000 US respondents. Exploratory Factor Analysis identified a two-factor structure. Two items were then removed due to redundancy, yielding a final 6-item scale with a 2-factor structure. Confirmatory Factor Analysis on a separate subsample showed strong model fit (\$CFI = .995\$, \$TLI = .991\$, \$RMSEA = .046\$, \$p\_\{X\textasciicircum 2\} {$>$} .05\$). Convergent validity analysis revealed that trust in LLMs correlated positively with openness to experience, extraversion, and cognitive flexibility, but negatively with neuroticism. Based on these findings, we interpreted TILLMI's factors as "closeness with LLMs" (affective dimension) and "reliance on LLMs" (cognitive dimension). Younger males exhibited higher closeness with- and reliance on LLMs compared to older women. Individuals with no direct experience with LLMs exhibited lower levels of trust compared to LLMs' users. These findings offer a novel empirical foundation for measuring trust in AI-driven verbal communication, informing responsible design, and fostering balanced human-AI collaboration.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/RM9JL3CF/Duro et al. - 2025 - Measuring and identifying factors of individuals' trust in Large Language Models.pdf;/Users/thomasgorman/Zotero/storage/JFCQ2P4Q/2502.html}
}

@article{dvorakAdverseReactionsUse2025,
  title = {Adverse Reactions to the Use of Large Language Models in Social Interactions},
  author = {Dvorak, Fabian and Stumpf, Regina and Fehrler, Sebastian and Fischbacher, Urs},
  date = {2025-04-01},
  journaltitle = {PNAS Nexus},
  shortjournal = {PNAS Nexus},
  volume = {4},
  number = {4},
  pages = {pgaf112},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgaf112},
  url = {https://doi.org/10.1093/pnasnexus/pgaf112},
  urldate = {2025-04-30},
  abstract = {Large language models (LLMs) are poised to reshape the way individuals communicate and interact. While this form of AI has the potential to efficiently make many human decisions, there is limited understanding of how individuals will respond to its use in social interactions. In particular, it remains unclear how individuals interact with LLMs when the interaction has consequences for other people. Here, we report the results of a large-scale, preregistered online experiment (n=3,552) showing that human players’ fairness, trust, trustworthiness, cooperation, and coordination in economic two-player games decrease when the decision of the interaction partner is taken over by ChatGPT. On the contrary, we observe no adverse reactions when individuals are uncertain whether they are interacting with a human or a LLM. At the same time, participants often delegate decisions to the LLM, especially when the model’s involvement is not disclosed, and individuals have difficulty distinguishing between decisions made by humans and those made by AI.},
  file = {/Users/thomasgorman/Zotero/storage/I7ACP96Y/Dvorak et al. - 2025 - Adverse reactions to the use of large language models in social interactions.pdf}
}

@online{fangHowAIHuman2025,
  title = {How {{AI}} and {{Human Behaviors Shape Psychosocial Effects}} of {{Chatbot Use}}: {{A Longitudinal Randomized Controlled Study}}},
  shorttitle = {How {{AI}} and {{Human Behaviors Shape Psychosocial Effects}} of {{Chatbot Use}}},
  author = {Fang, Cathy Mengying and Liu, Auren R. and Danry, Valdemar and Lee, Eunhae and Chan, Samantha W. T. and Pataranutaporn, Pat and Maes, Pattie and Phang, Jason and Lampe, Michael and Ahmad, Lama and Agarwal, Sandhini},
  date = {2025-03-21},
  eprint = {2503.17473},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2503.17473},
  url = {http://arxiv.org/abs/2503.17473},
  urldate = {2025-04-30},
  abstract = {AI chatbots, especially those with voice capabilities, have become increasingly human-like, with more users seeking emotional support and companionship from them. Concerns are rising about how such interactions might impact users' loneliness and socialization with real people. We conducted a four-week randomized, controlled, IRB-approved experiment (n=981, {$>$}300K messages) to investigate how AI chatbot interaction modes (text, neutral voice, and engaging voice) and conversation types (open-ended, non-personal, and personal) influence psychosocial outcomes such as loneliness, social interaction with real people, emotional dependence on AI and problematic AI usage. Results showed that while voice-based chatbots initially appeared beneficial in mitigating loneliness and dependence compared with text-based chatbots, these advantages diminished at high usage levels, especially with a neutral-voice chatbot. Conversation type also shaped outcomes: personal topics slightly increased loneliness but tended to lower emotional dependence compared with open-ended conversations, whereas non-personal topics were associated with greater dependence among heavy users. Overall, higher daily usage - across all modalities and conversation types - correlated with higher loneliness, dependence, and problematic use, and lower socialization. Exploratory analyses revealed that those with stronger emotional attachment tendencies and higher trust in the AI chatbot tended to experience greater loneliness and emotional dependence, respectively. These findings underscore the complex interplay between chatbot design choices (e.g., voice expressiveness) and user behaviors (e.g., conversation content, usage frequency). We highlight the need for further research on whether chatbots' ability to manage emotional content without fostering dependence or replacing human relationships benefits overall well-being.},
  pubstate = {prepublished},
  keywords = {Computer Science - Human-Computer Interaction},
  annotation = {https://github.com/mitmedialab/chatbot-psychosocial-study},
  file = {/Users/thomasgorman/Zotero/storage/FYJJK8XH/Fang et al. - 2025 - How AI and Human Behaviors Shape Psychosocial Effects of Chatbot Use A Longitudinal Randomized Cont.pdf;/Users/thomasgorman/Zotero/storage/DIB2F23H/2503.html}
}

@article{frederickCharacterizingPerceptionsEnergy2011,
  title = {Characterizing Perceptions of Energy Consumption},
  author = {Frederick, Shane W. and Meyer, Andrew B. and Mochon, Daniel},
  date = {2011-02-22},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {8},
  pages = {E23-E23},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1014806108},
  url = {https://www.pnas.org/doi/full/10.1073/pnas.1014806108},
  urldate = {2025-05-05},
  abstract = {The adoption of energy-saving technologies is presumably deterred by underestimates of energy use and by corresponding underestimates of the difference between more-and lessefficient appliances. Thus, it is easy to grasp the potential policy significance of a recent study (1) concluding that Americans underestimate energy use by a factor of 2.8. However, the apparent precision of that statistic belies its arbitrary origins. By manipulating just two experimental details (the provided numeric referent and the units in which judgments were rendered), we show that one can readily reach qualitatively different conclusions.},
  file = {/Users/thomasgorman/Zotero/storage/XFTNG86D/Frederick et al. - 2011 - Characterizing perceptions of energy consumption.pdf}
}

@article{gerlichAIToolsSociety2025,
  title = {{{AI Tools}} in {{Society}}: {{Impacts}} on {{Cognitive Offloading}} and the {{Future}} of {{Critical Thinking}}},
  shorttitle = {{{AI Tools}} in {{Society}}},
  author = {Gerlich, Michael},
  date = {2025-01},
  journaltitle = {Societies},
  volume = {15},
  number = {1},
  pages = {6},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2075-4698},
  doi = {10.3390/soc15010006},
  url = {https://www.mdpi.com/2075-4698/15/1/6},
  urldate = {2025-01-27},
  abstract = {The proliferation of artificial intelligence (AI) tools has transformed numerous aspects of daily life, yet its impact on critical thinking remains underexplored. This study investigates the relationship between AI tool usage and critical thinking skills, focusing on cognitive offloading as a mediating factor. Utilising a mixed-method approach, we conducted surveys and in-depth interviews with 666 participants across diverse age groups and educational backgrounds. Quantitative data were analysed using ANOVA and correlation analysis, while qualitative insights were obtained through thematic analysis of interview transcripts. The findings revealed a significant negative correlation between frequent AI tool usage and critical thinking abilities, mediated by increased cognitive offloading. Younger participants exhibited higher dependence on AI tools and lower critical thinking scores compared to older participants. Furthermore, higher educational attainment was associated with better critical thinking skills, regardless of AI usage. These results highlight the potential cognitive costs of AI tool reliance, emphasising the need for educational strategies that promote critical engagement with AI technologies. This study contributes to the growing discourse on AI’s cognitive implications, offering practical recommendations for mitigating its adverse effects on critical thinking. The findings underscore the importance of fostering critical thinking in an AI-driven world, making this research essential reading for educators, policymakers, and technologists.},
  issue = {1},
  langid = {english},
  keywords = {AI,AI tools,AI trust,artificial intelligence,cognitive development,cognitive offloading,critical thinking,digital dependence,Halpern Critical Thinking Assessment,technology and education},
  file = {/Users/thomasgorman/Zotero/storage/MX7CXN88/Gerlich - 2025 - AI Tools in Society Impacts on Cognitive Offloading and the Future of Critical Thinking.pdf}
}

@article{gliksonHumanTrustArtificial2020,
  title = {Human {{Trust}} in {{Artificial Intelligence}}: {{Review}} of {{Empirical Research}}},
  shorttitle = {Human {{Trust}} in {{Artificial Intelligence}}},
  author = {Glikson, Ella and Woolley, Anita Williams},
  date = {2020-07},
  journaltitle = {Academy of Management Annals},
  shortjournal = {ANNALS},
  volume = {14},
  number = {2},
  pages = {627--660},
  issn = {1941-6520, 1941-6067},
  doi = {10.5465/annals.2018.0057},
  url = {http://journals.aom.org/doi/10.5465/annals.2018.0057},
  urldate = {2025-05-02},
  abstract = {Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into organizations critically depends on workers’ trust in AI technology. This review explains how AI differs from other technologies and presents the existing empirical research on the determinants of human “trust” in AI, conducted in multiple disciplines over the last 20 years. Based on the reviewed literature, we identify the form of AI representation (robot, virtual, and embedded) and its level of machine intelligence (i.e., its capabilities) as important antecedents to the development of trust and propose a framework that addresses the elements that shape users’ cognitive and emotional trust. Our review reveals the important role of AI’s tangibility, transparency, reliability, and immediacy behaviors in developing cognitive trust, and the role of AI’s anthropomorphism specifically for emotional trust. We also note several limitations in the current evidence base, such as the diversity of trust measures and overreliance on short-term, small sample, and experimental studies, where the development of trust is likely to be different than in longer-term, higher stakes field environments. Based on our review, we suggest the most promising paths for future research.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/UBRDBYYV/Glikson and Woolley - 2020 - Human Trust in Artificial Intelligence Review of Empirical Research.pdf}
}

@article{gnambsAttitudesExperiencesUsage2025,
  title = {Attitudes, Experiences, and Usage Intentions of Artificial Intelligence: {{A}} Population Study in {{Germany}}},
  shorttitle = {Attitudes, Experiences, and Usage Intentions of Artificial Intelligence},
  author = {Gnambs, Timo and Stein, Jan-Philipp and Zinn, Sabine and Griese, Florian and Appel, Markus},
  date = {2025-04-01},
  journaltitle = {Telematics and Informatics},
  shortjournal = {Telematics and Informatics},
  volume = {98},
  pages = {102265},
  issn = {0736-5853},
  doi = {10.1016/j.tele.2025.102265},
  url = {https://www.sciencedirect.com/science/article/pii/S0736585325000279},
  urldate = {2025-04-30},
  abstract = {Artificial intelligence (AI) increasingly affects individuals’ private and professional lives. Importantly, both the acceptance and adoption of new AI technologies in society is heavily impacted by the attitudes that people hold; yet, there is currently limited information on how people perceive and intend to use AI at the national and demographic levels. Therefore, this study examined a random sample of 1,098 German adults to assess their attitudes, experiences, and usage intentions regarding AI in work, healthcare, and education. The findings indicated that respondents generally held favorable attitudes towards AI, with AI applications in healthcare receiving more positive evaluations than AI in the context of work. Moreover, cognitive evaluations of AI were more positive than emotional or behavioral appraisals. Prior experiences with AI were, however, limited, particularly in healthcare and education. Demographic differences were generally small. Taken together, these findings demonstrate that, in Germany, AI is currently widely accepted in different domains, although most people have little first-hand experience with it. These insights can inform policymakers and stakeholders who care about the proliferation of AI in society.},
  keywords = {Artificial intelligence,Attitudes,Experience,Germany,Social survey},
  annotation = {https://osf.io/eha58/files/osfstorage\#\\
\\
https://ars-els-cdn-com.proxyiub.uits.iu.edu/content/image/1-s2.0-S0736585325000279-mmc1.pdf},
  file = {/Users/thomasgorman/Zotero/storage/5URMG7S8/Gnambs et al. - 2025 - Attitudes, experiences, and usage intentions of artificial intelligence A population study in Germa.pdf;/Users/thomasgorman/Zotero/storage/VAHFK223/S0736585325000279.html}
}

@article{grassiniDevelopmentValidationAI2023,
  title = {Development and Validation of the {{AI}} Attitude Scale ({{AIAS-4}}): A Brief Measure of General Attitude toward Artificial Intelligence},
  shorttitle = {Development and Validation of the {{AI}} Attitude Scale ({{AIAS-4}})},
  author = {Grassini, Simone},
  date = {2023-07-24},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {14},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2023.1191628},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1191628/full},
  urldate = {2025-05-01},
  abstract = {The rapid advancement of artificial intelligence (AI) has generated an increasing demand for tools that can assess public attitudes toward AI. This study proposes the development and the validation of the AI Attitude Scale (AIAS), a concise self-report instrument designed to evaluate public perceptions of AI technology. The first version of the AIAS that the present manuscript proposes comprises five items, including one reverse-scored item, which aims to gauge individuals’ beliefs about AI’s influence on their lives, careers, and humanity overall. The scale is designed to capture attitudes toward AI, focusing on the perceived utility and potential impact of technology on society and humanity. The psychometric properties of the scale were investigated using diverse samples in two separate studies. An exploratory factor analysis was initially conducted on a preliminary 5-item version of the scale. Such exploratory validation study revealed the need to divide the scale into two factors. While the results demonstrated satisfactory internal consistency for the overall scale and its correlation with related psychometric measures, separate analyses for each factor showed robust internal consistency for Factor 1 but insufficient internal consistency for Factor 2. As a result, a second version of the scale is developed and validated, omitting the item that displayed weak correlation with the remaining items in the questionnaire. The refined final 1-factor, 4-item AIAS demonstrated superior overall internal consistency compared to the initial 5-item scale and the proposed factors. Further confirmatory factor analyses, performed on a different sample of participants, confirmed that the 1-factor model (4-items) of the AIAS exhibited an adequate fit to the data, providing additional evidence for the scale’s structural validity and generalizability across diverse populations. In conclusion, the analyses reported in this article suggest that the developed and validated 4-items AIAS can be a valuable instrument for researchers and professionals working on AI development who seek to understand and study users’ general attitudes toward AI.},
  langid = {english},
  annotation = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1191628/full\#supplementary-material},
  file = {/Users/thomasgorman/Zotero/storage/92ZW78Q2/Grassini - 2023 - Development and validation of the AI attitude scale (AIAS-4) a brief measure of general attitude to.pdf}
}

@inproceedings{grassiniPsychometricValidationPAILQ62024,
  title = {A {{Psychometric Validation}} of the {{PAILQ-6}}: {{Perceived Artificial Intelligence Literacy Questionnaire}}},
  shorttitle = {A {{Psychometric Validation}} of the {{PAILQ-6}}},
  booktitle = {Nordic {{Conference}} on {{Human-Computer Interaction}}},
  author = {Grassini, Simone},
  date = {2024-10-13},
  pages = {1--10},
  publisher = {ACM},
  location = {Uppsala Sweden},
  doi = {10.1145/3679318.3685359},
  url = {https://dl.acm.org/doi/10.1145/3679318.3685359},
  urldate = {2025-05-02},
  abstract = {The present article introduces and implements an initial validation for the Perceived Artificial Intelligence Literacy Questionnaire (PAILQ-6), a brief tool designed to assess individuals' self-perceived AI literacy. Amidst the growing integration of AI in various aspects of life and its ethical implications, understanding AI becomes crucial for effective interaction with AI technologies. The PAILQ-6 emerges in response to the need for an accessible instrument that evaluates general AI literacy without compromising on clarity or depth, suitable for both academic and practical applications. This paper presents the development process of the PAILQ-6, consisting of six items derived from established components of AI literacy, structured as a seven-point Likert scale for easy administration and digital compatibility. The validation study was conducted from data of a gender-balanced sample of 232 UK adults. The article demonstrates the PAILQ-6's reliability and validity through exploratory factor analysis, showing a two-factor structure. The findings reveal the scale's good internal consistency and convergent validity. The study highlights demographic predictors of AI literacy perceptions, indicating a possible gender disparity and the positive influence of higher education on perceived AI competency.},
  eventtitle = {{{NordiCHI}} 2024: {{Nordic Conference}} on {{Human-Computer Interaction}}},
  isbn = {979-8-4007-0966-1},
  langid = {english},
  annotation = {https://zenodo.org/records/10888785/files/PAILQ-6\_questionnaire\_and\_scoring.pdf.},
  file = {/Users/thomasgorman/Zotero/storage/GH27XXUR/Grassini - 2024 - A Psychometric Validation of the PAILQ-6 Perceived Artificial Intelligence Literacy Questionnaire.pdf}
}

@article{gronauHowYouKnow2024,
  title = {How Do You Know That You Don’t Know?},
  author = {Gronau, Quentin F. and Steyvers, Mark and Brown, Scott D.},
  date = {2024-08-01},
  journaltitle = {Cognitive Systems Research},
  shortjournal = {Cognitive Systems Research},
  volume = {86},
  pages = {101232},
  issn = {1389-0417},
  doi = {10.1016/j.cogsys.2024.101232},
  url = {https://www.sciencedirect.com/science/article/pii/S1389041724000263},
  urldate = {2024-11-10},
  abstract = {Whenever someone in a team tries to help others, it is crucial that they have some understanding of other team members’ goals. In modern teams, this applies equally to human and artificial (“bot”) assistants. Understanding when one does not know something is crucial for stopping the execution of inappropriate behavior and, ideally, attempting to learn more appropriate actions. From a statistical point of view, this can be translated to assessing whether none of the hypotheses in a considered set is correct. Here we investigate a novel approach for making this assessment based on monitoring the maximum a posteriori probability (MAP) of a set of candidate hypotheses as new observations arrive. Simulation studies suggest that this is a promising approach, however, we also caution that there may be cases where this is more challenging. The problem we study and the solution we propose are general, with applications well beyond human–bot teaming, including for example the scientific process of theory development.},
  keywords = {Maximum a posteriori probability,Model mis-specification,Model uncertainty},
  annotation = {https://osf.io/t4n63/},
  file = {/Users/thomasgorman/Zotero/storage/HFNTHSID/Gronau et al. - 2024 - How do you know that you don’t know.pdf;/Users/thomasgorman/Zotero/storage/3DSW8BCL/S1389041724000263.html}
}

@article{gronerInvestigatingImpactUser2024,
  title = {Investigating the {{Impact}} of {{User Interface Designs}} on {{Expectations About Large Language Models}}’ {{Capabilities}}},
  author = {Gröner, Felix and Chiou, Erin K.},
  date = {2024-09-01},
  journaltitle = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  volume = {68},
  number = {1},
  pages = {155--161},
  publisher = {SAGE Publications Inc},
  issn = {1071-1813},
  doi = {10.1177/10711813241260399},
  url = {https://doi.org/10.1177/10711813241260399},
  urldate = {2025-04-30},
  abstract = {Large Language Models (LLMs) with their novel conversational interaction format could create incorrectly calibrated expectations about their capabilities. The present study investigates human expectations toward a generic LLM?s capabilities and limitations. Participants of an online study were shown a series of prompts that cover a wide range of tasks and asked to assess the likelihood of the LLM being able to help with those tasks. The result is a catalog of people?s general expectations of LLM capabilities across various task domains. Depending on the actual capabilities of a specific system, this could inform developers of potential over- or under-reliance on this technology due to these misconceptions. To explore a potential way of correcting misconceptions we also attempted to manipulate their expectations with three different interface designs. In most of the tested task domains, such as computation and text processing, however, these seem to be insufficient to overpower people?s initial expectations.},
  file = {/Users/thomasgorman/Zotero/storage/KDGIYGNT/Gröner and Chiou - 2024 - Investigating the Impact of User Interface Designs on Expectations About Large Language Models’ Capa.pdf}
}

@article{guExploringMechanismSustained2024,
  title = {Exploring the Mechanism of Sustained Consumer Trust in {{AI}} Chatbots after Service Failures: A Perspective Based on Attribution and {{CASA}} Theories},
  shorttitle = {Exploring the Mechanism of Sustained Consumer Trust in {{AI}} Chatbots after Service Failures},
  author = {Gu, Chenyu and Zhang, Yu and Zeng, Linhao},
  date = {2024-10-22},
  journaltitle = {Humanities and Social Sciences Communications},
  shortjournal = {Humanit Soc Sci Commun},
  volume = {11},
  number = {1},
  pages = {1--12},
  publisher = {Palgrave},
  issn = {2662-9992},
  doi = {10.1057/s41599-024-03879-5},
  url = {https://www.nature.com/articles/s41599-024-03879-5},
  urldate = {2025-05-02},
  abstract = {In recent years, artificial intelligence (AI) technology has been widely employed in brand customer service. However, the inherent limitations of computer-generated natural language content occasionally lead to failures in human-computer interactions, potentially damaging a company’s brand image. Therefore, it is crucial to explore how to maintain consumer trust after AI chatbots fail to provide successful service. This study constructs a model to examine the impact of social interaction cues and anthropomorphic factors on users’ sustained trust by integrating the Computers As Social Actors (CASA) theory with attribution theory. An empirical analysis of 462 survey responses reveals that CASA factors (perceived anthropomorphic characteristics, perceived empathic abilities, and perceived interaction quality) can effectively enhance user trust in AI customer service following interaction failures. This process of sustaining trust is mediated through different attributions of failure. Furthermore, AI anxiety, as a cognitive characteristic of users, not only negatively impacts sustained trust but also significantly moderates the effect of internal attributions on sustained trust. These findings expand the research domain of human-computer interaction and provide insights for the practical development of AI chatbots in communication and customer service fields.},
  langid = {english},
  keywords = {Business and management,Information systems and information technology,Psychology},
  file = {/Users/thomasgorman/Zotero/storage/B9TWLBCQ/Gu et al. - 2024 - Exploring the mechanism of sustained consumer trust in AI chatbots after service failures a perspec.pdf}
}

@inproceedings{guoExploringImpactAI2024,
  title = {Exploring the {{Impact}} of {{AI Value Alignment}} in {{Collaborative Ideation}}: {{Effects}} on {{Perception}}, {{Ownership}}, and {{Output}}},
  shorttitle = {Exploring the {{Impact}} of {{AI Value Alignment}} in {{Collaborative Ideation}}},
  booktitle = {Extended {{Abstracts}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Guo, Alicia and Pataranutaporn, Pat and Maes, Pattie},
  date = {2024-05-11},
  pages = {1--11},
  publisher = {ACM},
  location = {Honolulu HI USA},
  doi = {10.1145/3613905.3650892},
  url = {https://dl.acm.org/doi/10.1145/3613905.3650892},
  urldate = {2025-04-30},
  eventtitle = {{{CHI}} '24: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  annotation = {https://8values.github.io/},
  file = {/Users/thomasgorman/Zotero/storage/9CR4X2LH/Guo et al. - 2024 - Exploring the Impact of AI Value Alignment in Collaborative Ideation Effects on Perception, Ownersh.pdf}
}

@article{habibiasgarabadPromotingElectricityConservation2024,
  title = {Promoting Electricity Conservation through Behavior Change: {{A}} Study Protocol for a Web-Based Multiple-Arm Parallel Randomized Controlled Trial},
  shorttitle = {Promoting Electricity Conservation through Behavior Change},
  author = {Habibi Asgarabad, Mojtaba and Vesely, Stepan and Efe Biresselioglu, Mehmet and Caffaro, Federica and Carrus, Giuseppe and Hakan Demir, Muhittin and Kirchler, Benjamin and Kollmann, Andrea and Massullo, Chiara and Tiberio, Lorenza and Klöckner, Christian A.},
  date = {2024-03-14},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS One},
  volume = {19},
  number = {3},
  eprint = {38483850},
  eprinttype = {pubmed},
  pages = {e0293683},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0293683},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10939288/},
  urldate = {2024-12-18},
  abstract = {Background and aims As a part of the framework of the EU-funded Energy efficiency through Behavior CHANge Transition (ENCHANT) project, the present paper intends to provide a “Research Protocol” of a web-based trial to: (i) assess the effectiveness of behavioral intervention strategies––either single or in combination––on electricity saving, and (ii) unravel the psychological factors contributing to intervention effectiveness in households across Europe. Methods and materials Six distinct interventions (i.e., information provision, collective vs. individual message framing, social norms, consumption feedback, competitive elements, and commitment strategies) targeting electricity saving in households from six European countries (i.e., Austria, Germany, Italy, Norway, Romania, and Türkiye) are evaluated, with an initial expected samples of about 1500 households per country randomly assigned to 12 intervention groups and two control groups, and data is collected through an ad-hoc online platform. The primary outcome is the weekly electricity consumption normalized to the last seven days before measurement per person per household. Secondary outcomes are the peak consumption during the last day before measurement and the self-reported implementation of electricity saving behaviors (e.g., deicing the refrigerator). The underlying psychological factors expected to mediate and/or moderate the intervention effects on these outcomes are intentions to save electricity, perceived difficulty of saving energy, attitudes to electricity saving, electricity saving habit strength, social norms to save electricity, personal norms, collective efficacy, emotional reaction to electricity consumption, and national identity. The intervention effectiveness will be evaluated by comparing psychological factors and consumption variables before and after the intervention, leading to a 14 (groups including 2 control groups) × 6 (time) mixed factorial design, with one factor between (group) and one factor within subjects (time)–6 measurements of the psychological factors and 6 readings of the electricity meters, which gives then 5 weeks of electricity consumption. Results Data collection for the present RCT started in January 2023, and by October 2023 data collection will conclude. Discussion Upon establishing feasibility and effectiveness, the outcomes of this study will assist policymakers, municipalities, NGOs, and other communal entities in identifying impactful interventions tailored to their unique circumstances and available resources. Researchers will benefit from a flexible, structured tool that allows the design, implementation and monitoring of complex interventions protocols. Crucially, the intervention participants will benefit from electricity saving strategies, fostering immediate effectiveness of the interventions in real-life contexts. Trial registration This trial was preregistered in the Open Science Framework: https://osf.io/9vtn4.},
  pmcid = {PMC10939288},
  file = {/Users/thomasgorman/Zotero/storage/P4WRSJKZ/Habibi Asgarabad et al. - 2024 - Promoting electricity conservation through behavior change A study protocol for a web-based multipl.pdf}
}

@online{hoffmanMetricsExplainableAI2019,
  title = {Metrics for {{Explainable AI}}: {{Challenges}} and {{Prospects}}},
  shorttitle = {Metrics for {{Explainable AI}}},
  author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
  date = {2019-02-01},
  eprint = {1812.04608},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1812.04608},
  url = {http://arxiv.org/abs/1812.04608},
  urldate = {2025-05-03},
  abstract = {The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/thomasgorman/Zotero/storage/4F6QCBP4/Hoffman et al. - 2019 - Metrics for Explainable AI Challenges and Prospects.pdf;/Users/thomasgorman/Zotero/storage/WV6J267W/1812.html}
}

@article{houMeasuringUndergraduateStudents2025,
  title = {Measuring Undergraduate Students' Reliance on {{Generative AI}} during Problem-Solving: {{Scale}} Development and Validation},
  shorttitle = {Measuring Undergraduate Students' Reliance on {{Generative AI}} during Problem-Solving},
  author = {Hou, Chenyu and Zhu, Gaoxia and Sudarshan, Vidya and Lim, Fun Siong and Ong, Yew Soon},
  date = {2025-09-01},
  journaltitle = {Computers \& Education},
  shortjournal = {Computers \& Education},
  volume = {234},
  pages = {105329},
  issn = {0360-1315},
  doi = {10.1016/j.compedu.2025.105329},
  url = {https://www.sciencedirect.com/science/article/pii/S0360131525000971},
  urldate = {2025-04-30},
  abstract = {Reliance on AI describes the behavioral patterns of when and how individuals depend on AI suggestions, and appropriate reliance patterns are necessary to achieve effective human-AI collaboration. Traditional measures often link reliance to decision-making outcomes, which may not be suitable for complex problem-solving tasks where outcomes are not binary (i.e., correct or incorrect) or immediately clear. Therefore, this study aims to develop a scale to measure undergraduate students' behaviors of using Generative AI during problem-solving tasks without directly linking them to specific outcomes. We conducted an exploratory factor analysis on 800 responses collected after students finished one problem-solving activity, which revealed four distinct factors: reflective use, cautious use, thoughtless use, and collaborative use. The overall scale has reached sufficient internal reliability (Cronbach's alpha~=~.84). Two confirmatory factor analyses (CFAs) were conducted to validate the factors using the remaining 730 responses from this activity and 1173 responses from another problem-solving activity. CFA indices showed adequate model fit for data from both problem-solving tasks, suggesting that the scale can be applied to various human-AI problem-solving tasks. This study offers a validated scale to measure students' reliance behaviors in different human-AI problem-solving activities and provides implications for educators to responsively integrate Generative AI in higher education.},
  keywords = {Generative AI,Higher education,Human-AI collaboration,Problem-solving,Reliance on AI,Scale development},
  file = {/Users/thomasgorman/Zotero/storage/B2I2DJ4A/Hou et al. - 2025 - Measuring undergraduate students' reliance on Generative AI during problem-solving Scale developmen.pdf;/Users/thomasgorman/Zotero/storage/379UE2YJ/S0360131525000971.html}
}

@online{jinGLATGenerativeAI2024,
  title = {{{GLAT}}: {{The Generative AI Literacy Assessment Test}}},
  shorttitle = {{{GLAT}}},
  author = {Jin, Yueqiao and Martinez-Maldonado, Roberto and Gašević, Dragan and Yan, Lixiang},
  date = {2024-11-19},
  eprint = {2411.00283},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.00283},
  url = {http://arxiv.org/abs/2411.00283},
  urldate = {2025-05-02},
  abstract = {The rapid integration of generative artificial intelligence (GenAI) technology into education necessitates precise measurement of GenAI literacy to ensure that learners and educators possess the skills to engage with and critically evaluate this transformative technology effectively. Existing instruments often rely on self-reports, which may be biased. In this study, we present the GenAI Literacy Assessment Test (GLAT), a 20-item multiple-choice instrument developed following established procedures in psychological and educational measurement. Structural validity and reliability were confirmed with responses from 355 higher education students using classical test theory and item response theory, resulting in a reliable 2-parameter logistic (2PL) model (Cronbach's alpha = 0.80; omega total = 0.81) with a robust factor structure (RMSEA = 0.03; CFI = 0.97). Critically, GLAT scores were found to be significant predictors of learners' performance in GenAI-supported tasks, outperforming self-reported measures such as perceived ChatGPT proficiency and demonstrating external validity. These results suggest that GLAT offers a reliable and valid method for assessing GenAI literacy, with the potential to inform educational practices and policy decisions that aim to enhance learners' and educators' GenAI literacy, ultimately equipping them to navigate an AI-enhanced future.},
  pubstate = {prepublished},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/UEKP27M7/Jin et al. - 2024 - GLAT The Generative AI Literacy Assessment Test.pdf;/Users/thomasgorman/Zotero/storage/B7ZHQRME/2411.html}
}

@article{jiun-yinjianFoundationsEmpiricallyDetermined2000,
  title = {Foundations for an {{Empirically Determined Scale}} of {{Trust}} in {{Automated System}}},
  author = {{Jiun-Yin Jian} and Bisantz, Ann M. and Drury, Colin G.},
  date = {2000-01},
  journaltitle = {International Journal of Cognitive Ergonomics},
  volume = {4},
  number = {1},
  pages = {53},
  issn = {10886362},
  doi = {10.1207/S15327566IJCE0401_04},
  url = {https://search.ebscohost.com/login.aspx?direct=true&AuthType=sso&db=bth&AN=3171382&site=ehost-live&custid=purdue},
  urldate = {2025-04-30},
  abstract = {One component in the successful use of automated systems is the extent to which people trust the automation to perform effectively. In order to understand the relationship between trust in computerized systems and the use of those systems, we need to be able to effectively measure trust. Although questionnaires regarding trust have been used in prior studies, these questionnaires were theoretically rather than empirically generated and did not distinguish between 3 potentially different types of trust: human–human trust, human–machine trust, and trust in general. A 3-phased experiment, comprising a word elicitation study, a questionnaire study, and a paired comparison study, was performed to better understand similarities and differences in the concepts of trust and distrust, and among the different types of trust. Results indicated that trust and distrust can be considered opposites, rather than different concepts. Components of trust, in terms of words related to trust, were similar across the three types of trust. Results obtained from a cluster analysis were used to identify 12 potential factors of trust between people and automated systems. These 12 factors were then used to develop a proposed scale to measure trust in automation.},
  keywords = {Human-machine systems,Trust},
  annotation = {https://osf.io/7cdne/files/osfstorage?view\_only=ad812bc898154990959a50aaea43ca61\#\\
\\
https://osf.io/9ht7s?view\_only=ad812bc898154990959a50aaea43ca61},
  file = {/Users/thomasgorman/Zotero/storage/WGDQN6E2/Jiun-Yin Jian et al. - 2000 - Foundations for an Empirically Determined Scale of Trust in Automated System.pdf}
}

@article{kantenbacherBetterRulesJudging2021,
  title = {Better Rules for Judging Joules: {{Exploring}} How Experts Make Decisions about Household Energy Use},
  shorttitle = {Better Rules for Judging Joules},
  author = {Kantenbacher, Joseph and Attari, Shahzeen Z.},
  date = {2021-03-01},
  journaltitle = {Energy Research \& Social Science},
  shortjournal = {Energy Research \& Social Science},
  volume = {73},
  pages = {101911},
  issn = {2214-6296},
  doi = {10.1016/j.erss.2021.101911},
  url = {https://www.sciencedirect.com/science/article/pii/S2214629621000049},
  urldate = {2024-07-03},
  abstract = {Public understanding of home energy use is rife with biases and misunderstandings that can stymie the adoption of efficient technologies and conservation practices. Studying how energy experts make energy-related judgments can help design decision support tools to correct misperceptions held by novices. Here we conduct interviews with electrical engineers (n~=~10), physicists (n~=~10), and energy analysts (n~=~10) to document expert judgments about energy use and to identify their cognitive shortcuts (heuristics) for household energy decision making. Performance on an energy estimation task confirmed that energy experts have more accurate estimates of home energy use than novices. We document 24 unique expert heuristics related to device functions, components, and observable cues used by experts while making energy-use judgments. A follow-up survey with the experts indicated that these expert heuristics are generally more accurate than novice heuristics. The library of heuristics created in this study can be useful additions to education programs designed to improve public energy literacy and decision making.},
  keywords = {Cognitive shortcuts,Decision support,Estimation,Expert elicitation,Interviews,Perception},
  annotation = {https://www.szattari.com/publications},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kantenbacher_Attari_2021_Better rules for judging joules.pdf;/Users/thomasgorman/Zotero/storage/UML45G5G/S2214629621000049.html}
}

@online{kimImNotSure2024,
  title = {"{{I}}'m {{Not Sure}}, {{But}}...": {{Examining}} the {{Impact}} of {{Large Language Models}}' {{Uncertainty Expression}} on {{User Reliance}} and {{Trust}}},
  shorttitle = {"{{I}}'m {{Not Sure}}, {{But}}..."},
  author = {Kim, Sunnie S. Y. and Liao, Q. Vera and Vorvoreanu, Mihaela and Ballard, Stephanie and Vaughan, Jennifer Wortman},
  date = {2024-05-15},
  eprint = {2405.00623},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2405.00623},
  url = {http://arxiv.org/abs/2405.00623},
  urldate = {2024-10-12},
  abstract = {Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs' expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants' reliance, trust, and overall task performance. We find that first-person expressions (e.g., "I'm not sure, but...") decrease participants' confidence in the system and tendency to agree with the system's answers, while increasing participants' accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., "It's not clear, but..."), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  annotation = {https://osf.io/mnrp9},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kim et al_2024_I'm Not Sure, But.pdf;/Users/thomasgorman/Zotero/storage/484BYZDB/2405.html}
}

@article{kochMetaAILiteracy2024,
  title = {Meta {{AI}} Literacy Scale: {{Further}} Validation and Development of a Short Version},
  shorttitle = {Meta {{AI}} Literacy Scale},
  author = {Koch, Martin J. and Carolus, Astrid and Wienrich, Carolin and Latoschik, Marc E.},
  date = {2024-11},
  journaltitle = {Heliyon},
  shortjournal = {Heliyon},
  volume = {10},
  number = {21},
  pages = {e39686},
  issn = {24058440},
  doi = {10.1016/j.heliyon.2024.e39686},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2405844024157179},
  urldate = {2025-04-30},
  abstract = {The concept of AI literacy, its promotion, and measurement are important topics as they prepare society for the steadily advancing spread of AI technology. The first purpose of the current study is to advance the measurement of AI literacy by collecting evidence regarding the validity of the Meta AI Literacy Scale (MAILS) by Carolus and colleagues published in 2023: a self-assessment instrument for AI literacy and additional psychological competencies conducive for the use of AI. For this purpose, we first formulated the intended measurement purposes of the MAILS. In a second step, we derived empirically testable axioms and subaxioms from the purposes. We tested them in several already published and newly collected data sets. The results are presented in the form of three different empirical studies. We found overall evidence for the validity of the MAILS with some unexpected findings that require further research. We discuss the results for each study individually and also together. Also, avenues for future research are discussed. The study’s second purpose is to develop a short version (10 items) of the original instrument (34 items). It was possible to find a selection of ten items that represent the factors of the MAILS and show a good model fit when tested with confirmatory factor analysis. Further research will be needed to validate the short scale. This paper advances the knowledge about the validity and provides a short measure for AI literacy. However, more research will be necessary to further our understanding of the relationships between AI literacy and other constructs.},
  langid = {english},
  annotation = {https://downloads.hci.informatik.uni-wuerzburg.de/MAILS-qustionnaire-english.pdf\\
\\
https://downloads.hci.informatik.uni-wuerzburg.de/MAILS-short-questionnaire-english.pdf\\
\\
https://hci.uni-wuerzburg.de/research/MAILS/},
  file = {/Users/thomasgorman/Zotero/storage/4PXPCY23/Koch et al. - 2024 - Meta AI literacy scale Further validation and development of a short version.pdf}
}

@article{kohnMeasurementTrustAutomation2021,
  title = {Measurement of {{Trust}} in {{Automation}}: {{A Narrative Review}} and {{Reference Guide}}},
  shorttitle = {Measurement of {{Trust}} in {{Automation}}},
  author = {Kohn, Spencer C. and De Visser, Ewart J. and Wiese, Eva and Lee, Yi-Ching and Shaw, Tyler H.},
  date = {2021-10-19},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {12},
  pages = {604977},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2021.604977},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.604977/full},
  urldate = {2025-05-02},
  abstract = {With the rise of automated and autonomous agents, research examining Trust in Automation (TiA) has attracted considerable attention over the last few decades. Trust is a rich and complex construct which has sparked a multitude of measures and approaches to study and understand it. This comprehensive narrative review addresses known methods that have been used to capture TiA. We examined measurements deployed in existing empirical works, categorized those measures into self-report, behavioral, and physiological indices, and examined them within the context of an existing model of trust. The resulting work provides a reference guide for researchers, providing a list of available TiA measurement methods along with the model-derived constructs that they capture including judgments of trustworthiness, trust attitudes, and trusting behaviors. The article concludes with recommendations on how to improve the current state of TiA measurement.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/65AH2N7M/Kohn et al. - 2021 - Measurement of Trust in Automation A Narrative Review and Reference Guide.pdf}
}

@article{korberIntroductionMattersManipulating2018,
  title = {Introduction Matters: {{Manipulating}} Trust in Automation and Reliance in Automated Driving},
  shorttitle = {Introduction Matters},
  author = {Körber, Moritz and Baseler, Eva and Bengler, Klaus},
  date = {2018-01-01},
  journaltitle = {Applied Ergonomics},
  shortjournal = {Applied Ergonomics},
  volume = {66},
  pages = {18--31},
  issn = {0003-6870},
  doi = {10.1016/j.apergo.2017.07.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0003687017301606},
  urldate = {2025-05-02},
  abstract = {Trust in automation is a key determinant for the adoption of automated systems and their appropriate use. Therefore, it constitutes an essential research area for the introduction of automated vehicles to road traffic. In this study, we investigated the influence of trust promoting (Trust promoted group) and trust lowering (Trust lowered group) introductory information on reported trust, reliance behavior and take-over performance. Forty participants encountered three situations in a 17-min highway drive in a conditionally automated vehicle (SAE Level 3). Situation 1 and Situation 3 were non-critical situations where a take-over was optional. Situation 2 represented a critical situation where a take-over was necessary to avoid a collision. A non-driving-related task (NDRT) was presented between the situations to record the allocation of visual attention. Participants reporting a higher trust level spent less time looking at the road or instrument cluster and more time looking at the NDRT. The manipulation of introductory information resulted in medium differences in reported trust and influenced participants' reliance behavior. Participants of the Trust promoted group looked less at the road or instrument cluster and more at the NDRT. The odds of participants of the Trust promoted group to overrule the automated driving system in the non-critical situations were 3.65 times (Situation 1) to 5 times (Situation 3) higher. In Situation 2, the Trust promoted group's mean take-over time was extended by 1154~ms and the mean minimum time-to-collision was 933~ms shorter. Six participants from the Trust promoted group compared to no participant of the Trust lowered group collided with the obstacle. The results demonstrate that the individual trust level influences how much drivers monitor the environment while performing an NDRT. Introductory information influences this trust level, reliance on an automated driving system, and if a critical take-over situation can be successfully solved.},
  keywords = {Automated driving,Reliance,Trust in automation},
  annotation = {https://github.com/moritzkoerber/TiA\_Trust\_in\_Automation\_Questionnaire\\
\\
https://osf.io/xh3q5/},
  file = {/Users/thomasgorman/Zotero/storage/PFGZPC8A/Körber et al. - 2018 - Introduction matters Manipulating trust in automation and reliance in automated driving.pdf;/Users/thomasgorman/Zotero/storage/36MV7TYK/S0003687017301606.html}
}

@incollection{korberTheoreticalConsiderationsDevelopment2019,
  title = {Theoretical {{Considerations}} and {{Development}} of a {{Questionnaire}} to {{Measure Trust}} in {{Automation}}},
  booktitle = {Proceedings of the 20th {{Congress}} of the {{International Ergonomics Association}} ({{IEA}} 2018)},
  author = {Körber, Moritz},
  editor = {Bagnara, Sebastiano and Tartaglia, Riccardo and Albolino, Sara and Alexander, Thomas and Fujita, Yushi},
  date = {2019},
  volume = {823},
  pages = {13--30},
  publisher = {Springer International Publishing},
  location = {Cham},
  url = {http://link.springer.com/10.1007/978-3-319-96074-6_2},
  urldate = {2025-05-02},
  abstract = {The increasing number of interactions with automated systems has sparked the interest of researchers in trust in automation because it predicts not only whether but also how an operator interacts with an automation. In this work, a theoretical model of trust in automation is established and the development and evaluation of a corresponding questionnaire (Trust in Automation, TiA) are described.},
  isbn = {978-3-319-96073-9 978-3-319-96074-6},
  langid = {english},
  annotation = {https://github.com/moritzkoerber/TiA\_Trust\_in\_Automation\_Questionnaire\\
\\
https://osf.io/y3jn5/\\
\\
https://osf.io/zvhe3/},
  file = {/Users/thomasgorman/Zotero/storage/RNS9EY75/Körber - 2019 - Theoretical Considerations and Development of a Questionnaire to Measure Trust in Automation.pdf}
}

@article{kuperPsychologicalFactorsInfluencing2025,
  title = {Psychological {{Factors Influencing Appropriate Reliance}} on {{AI-enabled Clinical Decision Support Systems}}: {{Experimental Web-Based Study Among Dermatologists}}},
  shorttitle = {Psychological {{Factors Influencing Appropriate Reliance}} on {{AI-enabled Clinical Decision Support Systems}}},
  author = {Küper, Alisa and Lodde, Georg Christian and Livingstone, Elisabeth and Schadendorf, Dirk and Krämer, Nicole},
  date = {2025-04-04},
  journaltitle = {Journal of Medical Internet Research},
  volume = {27},
  number = {1},
  pages = {e58660},
  doi = {10.2196/58660},
  url = {https://www.jmir.org/2025/1/e58660},
  urldate = {2025-05-03},
  abstract = {Background: Artificial intelligence (AI)–enabled decision support systems are critical tools in medical practice; however, their reliability is not absolute, necessitating human oversight for final decision-making. Human reliance on such systems can vary, influenced by factors such as individual psychological factors and physician experience. Objective: This study aimed to explore the psychological factors influencing subjective trust and reliance on medical AI’s advice, specifically examining relative AI reliance and relative self-reliance to assess the appropriateness of reliance. Methods: A survey was conducted with 223 dermatologists, which included lesion image classification tasks and validated questionnaires assessing subjective trust, propensity to trust technology, affinity for technology interaction, control beliefs, need for cognition, as well as queries on medical experience and decision confidence. Results: A 2-tailed t test revealed that participants’ accuracy improved significantly with AI support (t222=−3.3; P\&lt;.001; Cohen d=4.5), but only by an average of 1\% (1/100). Reliance on AI was stronger for correct advice than for incorrect advice (t222=4.2; P\&lt;.001; Cohen d=0.1). Notably, participants demonstrated a mean relative AI reliance of 10.04\% (139/1384) and a relative self-reliance of 85.6\% (487/569), indicating a high level of self-reliance but a low level of AI reliance. Propensity to trust technology influenced AI reliance, mediated by trust (indirect effect=0.024, 95\% CI 0.008-0.042; P\&lt;.001), and medical experience negatively predicted AI reliance (indirect effect=–0.001, 95\% CI –0.002 to −0.001; P\&lt;.001). Conclusions: The findings highlight the need to design AI support systems in a way that assists less experienced users with a high propensity to trust technology to identify potential AI errors, while encouraging experienced physicians to actively engage with system recommendations and potentially reassess initial decisions.},
  langid = {english},
  annotation = {https://osf.io/87tpu/files/osfstorage\#},
  file = {/Users/thomasgorman/Zotero/storage/7C8KLSSX/Küper et al. - 2025 - Psychological Factors Influencing Appropriate Reliance on AI-enabled Clinical Decision Support Syste.pdf;/Users/thomasgorman/Zotero/storage/88783Q3S/e58660.html}
}

@article{laupichlerDevelopmentScaleAssessment2023,
  title = {Development of the “{{Scale}} for the Assessment of Non-Experts’ {{AI}} Literacy” – {{An}} Exploratory Factor Analysis},
  author = {Laupichler, Matthias Carl and Aster, Alexandra and Haverkamp, Nicolas and Raupach, Tobias},
  date = {2023-12-01},
  journaltitle = {Computers in Human Behavior Reports},
  shortjournal = {Computers in Human Behavior Reports},
  volume = {12},
  pages = {100338},
  issn = {2451-9588},
  doi = {10.1016/j.chbr.2023.100338},
  url = {https://www.sciencedirect.com/science/article/pii/S2451958823000714},
  urldate = {2025-05-02},
  abstract = {Artificial Intelligence competencies will become increasingly important in the near future. Therefore, it is essential that the AI literacy of individuals can be assessed in a valid and reliable way. This study presents the development of the “Scale for the assessment of non-experts' AI literacy” (SNAIL). An existing AI literacy item set was distributed as an online questionnaire to a heterogeneous group of non-experts (i.e., individuals without a formal AI or computer science education). Based on the data collected, an exploratory factor analysis was conducted to investigate the underlying latent factor structure. The results indicated that a three-factor model had the best model fit. The individual factors reflected AI competencies in the areas of “Technical Understanding”, “Critical Appraisal”, and “Practical Application”. In addition, eight items from the original questionnaire were deleted based on high intercorrelations and low communalities to reduce the length of the questionnaire. The final SNAIL-questionnaire consists of 31 items that can be used to assess the AI literacy of individual non-experts or specific groups and is also designed to enable the evaluation of AI literacy courses’ teaching effectiveness.},
  keywords = {AI competencies,AI literacy,AI literacy questionnaire,AI literacy scale,Assessment,Exploratory factor analysis},
  annotation = {https://ars-els-cdn-com.proxyiub.uits.iu.edu/content/image/1-s2.0-S2451958823000714-mmc1.pdf},
  file = {/Users/thomasgorman/Zotero/storage/V6BSVFXF/Laupichler et al. - 2023 - Development of the “Scale for the assessment of non-experts’ AI literacy” – An exploratory factor an.pdf;/Users/thomasgorman/Zotero/storage/LVBV4DE8/S2451958823000714.html}
}

@online{leeSuperintelligenceSuperstitionExploring2024,
  title = {Super-Intelligence or {{Superstition}}? {{Exploring Psychological Factors Influencing Belief}} in {{AI Predictions}} about {{Personal Behavior}}},
  shorttitle = {Super-Intelligence or {{Superstition}}?},
  author = {Lee, Eunhae and Pataranutaporn, Pat and Amores, Judith and Maes, Pattie},
  date = {2024-12-19},
  eprint = {2408.06602},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2408.06602},
  url = {http://arxiv.org/abs/2408.06602},
  urldate = {2025-04-30},
  abstract = {Could belief in AI predictions be just another form of superstition? This study investigates psychological factors that influence belief in AI predictions about personal behavior, comparing it to belief in astrology- and personality-based predictions. Through an experiment with 238 participants, we examined how cognitive style, paranormal beliefs, AI attitudes, personality traits, and other factors affect perceived validity, reliability, usefulness, and personalization of predictions from different sources. Our findings reveal that belief in AI predictions is positively correlated with belief in predictions based on astrology and personality psychology. Notably, paranormal beliefs and positive attitudes about AI significantly increased perceived validity, reliability, usefulness, and personalization of AI predictions. Conscientiousness was negatively correlated with belief in predictions across all sources, and interest in the prediction topic increased believability across predictions. Surprisingly, we found no evidence that cognitive style has an impact on belief in fictitious AI-generated predictions. These results highlight the "rational superstition" phenomenon in AI, where belief is driven more by mental heuristics and intuition than critical evaluation. This research advances our understanding of the psychology of human-AI interaction, offering insights into designing and promoting AI systems that foster appropriate trust and skepticism, critical for responsible integration in an increasingly AI-driven world.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  annotation = {https://github.com/mitmedialab/ai-superstition},
  file = {/Users/thomasgorman/Zotero/storage/WA8AQUEJ/Lee et al. - 2024 - Super-intelligence or Superstition Exploring Psychological Factors Influencing Belief in AI Predict.pdf;/Users/thomasgorman/Zotero/storage/F8JBGGLU/2408.html}
}

@article{lesicComparingConsumerPerceptions2019,
  title = {Comparing Consumer Perceptions of Appliances’ Electricity Use to Appliances’ Actual Direct-Metered Consumption},
  author = {Lesic, Vedran and Glasgo, Brock and Krishnamurti, Tamar and Bruine De Bruin, Wändi and Davis, Matthew and Azevedo, Inês Lima},
  date = {2019-12-01},
  journaltitle = {Environmental Research Communications},
  shortjournal = {Environ. Res. Commun.},
  volume = {1},
  number = {11},
  pages = {111002},
  issn = {2515-7620},
  doi = {10.1088/2515-7620/ab4a99},
  url = {https://iopscience.iop.org/article/10.1088/2515-7620/ab4a99},
  urldate = {2025-05-05},
  abstract = {Many strategies for reducing residential energy consumption—including product labelling programs, subsidies for the purchase of efficient devices, behavioral programs that encourage efficient energy use, and others—rely on building owners and end users to make informed investment and operational decisions. These strategies may be ineffective if consumers are unaware of how much electricity is used by different devices in their homes and buildings. This study therefore compares consumers’ perceptions of their appliances’ electricity use to these appliances’ actual direct-metered electricity consumption. Using an online survey, 118 homeowners from Austin, Texas were asked to estimate the energy consumption of six household devices which were monitored in the participants’ homes. Homeowners were randomly assigned to assess their appliance-specific electricity use in terms of energy units (kWh/month) or energy cost units (\$/month) for an average summer month. Consistent with previous studies, participants overestimated the energy consumed by their low energy consuming devices and slightly underestimated that of their most energy-consuming device. Results also showed that responses of the experimental groups estimating their consumption in energy units and energy cost units were similar, the accuracy of the two groups’ perceptions was similar, and levels of confidence in the two groups were similar. These results suggest that targeted information campaigns focused on air conditioning energy consumption and device power reduction opportunities could improve consumer decision-making to save energy and reduce demand.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/THGZZSBT/Lesic et al. - 2019 - Comparing consumer perceptions of appliances’ electricity use to appliances’ actual direct-metered c.pdf}
}

@article{liAIMotivationScale2025,
  title = {The {{AI Motivation Scale}} ({{AIMS}}): A Self-Determination Theory Perspective},
  shorttitle = {The {{AI Motivation Scale}} ({{AIMS}})},
  author = {Li, Jiajing and King, Ronnel B. and Chai, Ching Sing and Zhai, Xuesong and Lee, Vivian W. Y.},
  date = {2025-03-11},
  journaltitle = {Journal of Research on Technology in Education},
  publisher = {Routledge},
  issn = {1539-1523},
  url = {https://www.tandfonline.com/doi/abs/10.1080/15391523.2025.2478424},
  urldate = {2025-05-02},
  abstract = {Artificial Intelligence (AI) has a profound impact on university teaching and learning. However, there is a lack of instruments for measuring university students’ motivation to use AI in their learning. In this study, we developed and validated a questionnaire to measure students’ motivation to learn with AI. In Study 1, we developed the AI Motivation Scale (AIMS). Rooted in self-determination theory, the scale measures university students’ motivation to learn with AI across five dimensions: intrinsic motivation, identified regulation, introjected regulation, external regulation, and amotivation. Both within-network and between-network validation analyses indicated that the AIMS is psychometrically sound. In Study 2, we used the AIMS to explore whether students’ motivation to learn with AI is influenced by their university environment and promotes their engagement in learning with AI. The results showed that motivation to learn with AI mediated the positive relationship between supportive environments and engagement in learning with AI. The study shows that AIMS is a psychometrically sound instrument that can be used to assess university students’ motivation to learn with AI. It also sheds light on the pivotal role of motivation to learn with AI in the higher education context.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/MMUJE5TS/Li et al. - 2025 - The AI Motivation Scale (AIMS) a self-determination theory perspective.pdf;/Users/thomasgorman/Zotero/storage/XGNN399L/15391523.2025.html}
}

@online{liImSpartacusNo2024,
  title = {I'm {{Spartacus}}, {{No}}, {{I}}'m {{Spartacus}}: {{Measuring}} and {{Understanding LLM Identity Confusion}}},
  shorttitle = {I'm {{Spartacus}}, {{No}}, {{I}}'m {{Spartacus}}},
  author = {Li, Kun and Zhuang, Shichao and Zhang, Yue and Xu, Minghui and Wang, Ruoxi and Xu, Kaidi and Fu, Xinwen and Cheng, Xiuzhen},
  date = {2024-11-16},
  eprint = {2411.10683},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.10683},
  url = {http://arxiv.org/abs/2411.10683},
  urldate = {2025-05-03},
  abstract = {Large Language Models (LLMs) excel in diverse tasks such as text generation, data analysis, and software development, making them indispensable across domains like education, business, and creative industries. However, the rapid proliferation of LLMs (with over 560 companies developing or deploying them as of 2024) has raised concerns about their originality and trustworthiness. A notable issue, termed identity confusion, has emerged, where LLMs misrepresent their origins or identities. This study systematically examines identity confusion through three research questions: (1) How prevalent is identity confusion among LLMs? (2) Does it arise from model reuse, plagiarism, or hallucination? (3) What are the security and trust-related impacts of identity confusion? To address these, we developed an automated tool combining documentation analysis, self-identity recognition testing, and output similarity comparisons--established methods for LLM fingerprinting--and conducted a structured survey via Credamo to assess its impact on user trust. Our analysis of 27 LLMs revealed that 25.93\% exhibit identity confusion. Output similarity analysis confirmed that these issues stem from hallucinations rather than replication or reuse. Survey results further highlighted that identity confusion significantly erodes trust, particularly in critical tasks like education and professional use, with declines exceeding those caused by logical errors or inconsistencies. Users attributed these failures to design flaws, incorrect training data, and perceived plagiarism, underscoring the systemic risks posed by identity confusion to LLM reliability and trustworthiness.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security},
  file = {/Users/thomasgorman/Zotero/storage/PXM3QHHC/Li et al. - 2024 - I'm Spartacus, No, I'm Spartacus Measuring and Understanding LLM Identity Confusion.pdf;/Users/thomasgorman/Zotero/storage/BC9LQUYQ/2411.html}
}

@online{liOverconfidentUnconfidentAI2024,
  title = {Overconfident and {{Unconfident AI Hinder Human-AI Collaboration}}},
  author = {Li, Jingshu and Yang, Yitian and Zhang, Renwen and Lee, Yi-chieh},
  date = {2024-04-17},
  eprint = {2402.07632},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.07632},
  url = {http://arxiv.org/abs/2402.07632},
  urldate = {2025-05-09},
  abstract = {AI transparency is a central pillar of responsible AI deployment and effective human-AI collaboration. A critical approach is communicating uncertainty, such as displaying AI's confidence level, or its correctness likelihood (CL), to users. However, these confidence levels are often uncalibrated, either overestimating or underestimating actual CL, posing risks and harms to human-AI collaboration. This study examines the effects of uncalibrated AI confidence on users' trust in AI, AI advice adoption, and collaboration outcomes. We further examined the impact of increased transparency, achieved through trust calibration support, on these outcomes. Our results reveal that uncalibrated AI confidence leads to both the misuse of overconfident AI and disuse of unconfident AI, thereby hindering outcomes of human-AI collaboration. Deficiency of trust calibration support exacerbates this issue by making it harder to detect uncalibrated confidence, promoting misuse and disuse of AI. Conversely, trust calibration support aids in recognizing uncalibration and reducing misuse, but it also fosters distrust and causes disuse of AI. Our findings highlight the importance of AI confidence calibration for enhancing human-AI collaboration and suggest directions for AI design and regulation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/BFRHLL9W/Li et al. - 2024 - Overconfident and Unconfident AI Hinder Human-AI Collaboration.pdf;/Users/thomasgorman/Zotero/storage/TEMV2FV3/2402.html}
}

@inproceedings{longWhatAILiteracy2020,
  title = {What Is {{AI Literacy}}? {{Competencies}} and {{Design Considerations}}},
  shorttitle = {What Is {{AI Literacy}}?},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Long, Duri and Magerko, Brian},
  date = {2020-04-21},
  pages = {1--16},
  publisher = {ACM},
  location = {Honolulu HI USA},
  doi = {10.1145/3313831.3376727},
  url = {https://dl.acm.org/doi/10.1145/3313831.3376727},
  urldate = {2025-05-01},
  abstract = {Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learner-centered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper's contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.},
  eventtitle = {{{CHI}} '20: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  file = {/Users/thomasgorman/Zotero/storage/RNET5I88/Long and Magerko - 2020 - What is AI Literacy Competencies and Design Considerations.pdf}
}

@article{lundbergEasyNotEffective2019,
  title = {Easy but Not Effective: {{Why}} “Turning off the Lights” Remains a Salient Energy Conserving Behaviour in the {{United States}}},
  shorttitle = {Easy but Not Effective},
  author = {Lundberg, Daniel C. and Tang, Janine A. and Attari, Shahzeen Z.},
  date = {2019-12},
  journaltitle = {Energy Research \& Social Science},
  shortjournal = {Energy Research \& Social Science},
  volume = {58},
  pages = {101257},
  issn = {22146296},
  doi = {10.1016/j.erss.2019.101257},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2214629618312672},
  urldate = {2025-05-05},
  abstract = {When participants are asked how best to save energy in the home, the most frequent response since the 1980s has been “turning off the lights”. Here, we use an online survey (N = 1418) to investigate why turning off the lights persists as a modal response despite decades of energy education promoting far more effective behaviors. We confirm that turning off the lights is still the modal response when participants are asked for the single most effective action they currently do to save energy (36.3\% of participants). We find that being taught to turn off the light is an important reason for why turning off the lights has remained so popular. When participants are asked to make a recommendation to a friend between turning off the lights (curtailment action) or replacing incandescent bulbs with CFL or LED bulbs (efficiency action), we observe a remarkable shift towards efficiency (77\%) rather than curtailment (23\%). We find that participants explain their choice of turning off lights or replacing bulbs with different heuristics. Participants who choose turning off the lights state that energy savings occur when an appliance is completely turned off. Alternatively, those who pick replacing inefficient light bulbs state that far less energy can be used for a given task.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/MFNRY8KU/Lundberg et al. - 2019 - Easy but not effective Why “turning off the lights” remains a salient energy conserving behaviour i.pdf}
}

@article{maertensMisinformationSusceptibilityTest2023,
  title = {The {{Misinformation Susceptibility Test}} ({{MIST}}): {{A}} Psychometrically Validated Measure of News Veracity Discernment},
  shorttitle = {The {{Misinformation Susceptibility Test}} ({{MIST}})},
  author = {Maertens, Rakoen and Götz, Friedrich M. and Golino, Hudson F. and Roozenbeek, Jon and Schneider, Claudia R. and Kyrychenko, Yara and Kerr, John R. and Stieger, Stefan and McClanahan, William P. and Drabot, Karly and He, James and Van Der Linden, Sander},
  date = {2023-06-29},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {56},
  number = {3},
  pages = {1863--1899},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02124-2},
  url = {https://link.springer.com/10.3758/s13428-023-02124-2},
  urldate = {2025-03-12},
  abstract = {Interest in the psychology of misinformation has exploded in recent years. Despite ample research, to date there is no validated framework to measure misinformation susceptibility. Therefore, we introduce Verification done, a nuanced interpretation schema and assessment tool that simultaneously considers Veracity discernment, and its distinct, measurable abilities (real/fake news detection), and biases (distrust/naïvité—negative/positive judgment bias). We then conduct three studies with seven independent samples (Ntotal = 8504) to show how to develop, validate, and apply the Misinformation Susceptibility Test (MIST). In Study 1 (N=409) we use a neural network language model to generate items, and use three psychometric methods—factor analysis, item response theory, and exploratory graph analysis—to create the MIST-20 (20 items; completion time {$<$} 2 minutes), the MIST-16 (16 items; {$<$} 2 minutes), and the MIST-8 (8 items; {$<$} 1 minute). In Study 2 (N = 7674) we confirm the internal and predictive validity of the MIST in five national quota samples (US, UK), across 2 years, from three different sampling platforms—Respondi, CloudResearch, and Prolific. We also explore the MIST’s nomological net and generate age-, region-, and country-specific norm tables. In Study 3 (N=421) we demonstrate how the MIST—in conjunction with Verification done—can provide novel insights on existing psychological interventions, thereby advancing theory development. Finally, we outline the versatile implementations of the MIST as a screening tool, covariate, and intervention evaluation framework. As all methods are transparently reported and detailed, this work will allow other researchers to create similar scales or adapt them for any population of interest.},
  langid = {english},
  annotation = {https://osf.io/r7phc/files},
  file = {/Users/thomasgorman/Zotero/storage/76SVIZXQ/Maertens et al. - 2023 - The Misinformation Susceptibility Test (MIST) A psychometrically validated measure of news veracity.pdf}
}

@article{marghetisSimpleInterventionsCan2019,
  title = {Simple Interventions Can Correct Misperceptions of Home Energy Use},
  author = {Marghetis, Tyler and Attari, Shahzeen Z. and Landy, David},
  date = {2019-10},
  journaltitle = {Nature Energy},
  shortjournal = {Nat Energy},
  volume = {4},
  number = {10},
  pages = {874--881},
  issn = {2058-7546},
  doi = {10.1038/s41560-019-0467-2},
  url = {https://www.nature.com/articles/s41560-019-0467-2},
  urldate = {2024-12-07},
  abstract = {Public estimates of energy use suffer from severe biases. Failure to correct these may hinder efforts to conserve energy and undermine support for evidence-based policies. Here we present a randomized online experiment that showed that home energy perceptions can be improved. We tested two simple, potentially scalable interventions: providing numerical information (in watt-hours) about extremes of energy use and providing an explicit heuristic that addressed a common misperception. Both succeeded in improving numerical estimates of energy use, but in different ways. Numerical information about extremes primarily improved the use of the watt-hours response scale, while the heuristic improved underlying understanding of relative energy use. As a result, only the heuristic significantly benefitted judgements about energy-conserving behaviours. Because understanding of energy use also predicted self-reported energy-conservation behaviour, belief in climate change, and support for climate policies, targeting energy misperceptions may have the potential to shape individual behaviour and national policy support.},
  langid = {english},
  keywords = {Energy and behaviour,Energy conservation,Psychology and behaviour},
  annotation = {https://osf.io/2qbxt/\\
\\
https://www.szattari.com/publications},
  file = {/Users/thomasgorman/Zotero/storage/EL2PIXPK/Marghetis et al. - 2019 - Simple interventions can correct misperceptions of home energy use.pdf}
}

@incollection{marikyanTechnologyAcceptanceModel2023,
  title = {Technology Acceptance Model},
  booktitle = {{{TheoryHub Book}}},
  author = {Marikyan and Papagiannidis},
  date = {2023},
  url = {https://open.ncl.ac.uk/theories/1/technology-acceptance-model/},
  urldate = {2025-05-02},
  abstract = {TheoryHub reviews a wide range of theories, acting as a starting point for theory exploration in different research and teaching and learning contexts.},
  langid = {english},
  annotation = {https://open.ncl.ac.uk/theories/1/technology-acceptance-model/},
  file = {/Users/thomasgorman/Zotero/storage/2LNMJE77/eBusiness@Newcastle - 2023 - Technology acceptance model.pdf}
}

@online{markusObjectiveMeasurementAI2025,
  title = {Objective {{Measurement}} of {{AI Literacy}}: {{Development}} and {{Validation}} of the {{AI Competency Objective Scale}} ({{AICOS}})},
  shorttitle = {Objective {{Measurement}} of {{AI Literacy}}},
  author = {Markus, André and Carolus, Astrid and Wienrich, Carolin},
  date = {2025-03-17},
  eprint = {2503.12921},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2503.12921},
  url = {http://arxiv.org/abs/2503.12921},
  urldate = {2025-05-02},
  abstract = {As Artificial Intelligence (AI) becomes more pervasive in various aspects of life, AI literacy is becoming a fundamental competency that enables individuals to move safely and competently in an AI-pervaded world. There is a growing need to measure this competency, e.g., to develop targeted educational interventions. Although several measurement tools already exist, many have limitations regarding subjective data collection methods, target group differentiation, validity, and integration of current developments such as Generative AI Literacy. This study develops and validates the AI Competency Objective Scale (AICOS) for measuring AI literacy objectively. The presented scale addresses weaknesses and offers a robust measurement approach that considers established competency and measurement models, captures central sub-competencies of AI literacy, and integrates the dimension of Generative AI Literacy. The AICOS provides a sound and comprehensive measure of AI literacy, and initial analyses show potential for a modular structure. Furthermore, a first edition of a short version of the AICOS is developed. Due to its methodological foundation, extensive validation, and integration of recent developments, the test represents a valuable resource for scientific research and practice in educational institutions and professional contexts. The AICOS significantly contributes to the development of standardized measurement instruments and enables the targeted assessment and development of AI skills in different target groups.},
  pubstate = {prepublished},
  keywords = {Computer Science - Human-Computer Interaction},
  annotation = {https://osf.io/ehk8u/files/osfstorage?view\_only=df9a6ea06d1446659437a73946f68e5c},
  file = {/Users/thomasgorman/Zotero/storage/L8RMU2TN/Markus et al. - 2025 - Objective Measurement of AI Literacy Development and Validation of the AI Competency Objective Scal.pdf;/Users/thomasgorman/Zotero/storage/86Z8P2SI/2503.html}
}

@article{mederDevelopmentalTrajectoriesUnderstanding,
  title = {Developmental {{Trajectories}} in the {{Understanding}} of {{Everyday Uncertainty Terms}}},
  author = {Meder, Björn and Mayrhofer, Ralf and Ruggeri, Azzurra},
  journaltitle = {Topics in Cognitive Science},
  volume = {n/a},
  number = {n/a},
  issn = {1756-8765},
  doi = {10.1111/tops.12564},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/tops.12564},
  urldate = {2022-04-10},
  abstract = {Dealing with uncertainty and different degrees of frequency and probability is critical in many everyday activities. However, relevant information does not always come in the form of numerical estimates or direct experiences, but is instead obtained through qualitative, rather vague verbal terms (e.g., “the virus often causes coughing” or “the train is likely to be delayed”). Investigating how people interpret and utilize different natural language expressions of frequency and probability is therefore crucial to understand reasoning and behavior in real-world situations. While there is considerable work exploring how adults understand everyday uncertainty phrases, very little is known about how children interpret them and how their understanding develops with age. We take a developmental and computational perspective to address this issue and examine how 4- to 14-year-old children and adults interpret different terms. Each participant provided numerical estimates for 14 expressions, comprising both frequency and probability phrases. In total we obtained 2856 quantitative judgments, including 2240 judgments from children. Our findings demonstrate that adult-like intuitions about the interpretation of everyday uncertainty terms emerge fairly early in development, with the quantitative estimates of children converging to those of adults from around 9 years on. We also demonstrate how the vagueness of verbal terms can be represented through probability distributions, which provides additional leverage for tracking developmental shifts through cognitive modeling techniques. Taken together, our findings provide key insights into the developmental trajectories underlying the understanding of everyday uncertainty terms, and open up novel methodological pathways to formally model the vagueness of probability and frequency phrases, which are abundant in our everyday life and activities.},
  langid = {english},
  keywords = {Computational modeling,Development,Everyday activities,Everyday uncertainty terms,Frequency phrases,Probability phrases,Verbal uncertainty terms},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Meder et al_Developmental Trajectories in the Understanding of Everyday Uncertainty Terms.pdf;/Users/thomasgorman/Zotero/storage/GGVQFMYP/tops.html}
}

@article{mei-shiuDevelopmentValidationEnergyIssue2018,
  title = {Development and {{Validation}} of the {{Energy-Issue Attitude Questionnaire}}: {{Relations}} with {{Energy Knowledge}}, {{Affect}}, and {{Behavior}}},
  shorttitle = {Development and {{Validation}} of the {{Energy-Issue Attitude Questionnaire}}},
  author = {Mei-Shiu, Chiu and Jan, DeWaters and {Clarkson University, Potsdam, NY, USA}},
  date = {2018-02-01},
  journaltitle = {Journal of Advances in Education Research},
  shortjournal = {JAER},
  volume = {3},
  number = {1},
  publisher = {Isaac Scientific Publishing Co., Ltd.},
  issn = {2519-7002, 2519-7010},
  doi = {10.22606/jaer.2018.31003},
  url = {http://www.isaacpub.org/images/PaperPDF/JAER_100041_2018020109400203155.pdf},
  urldate = {2025-05-01},
  abstract = {This study aims to develop the Energy-Issue Attitude Questionnaire (EIAQ). The EIAQ focuses on student responses to energy issues in society and includes ten constructs, organized pairwise with tension: energy-saving vs. carbon-reducing knowledge, having vs. being lifestyles, questioning vs. conforming to authorities, technology vs. nature approaches, and future vs. present goals. The EIAQ was validated with a criterion questionnaire on energy literacy, including knowledge, affect and behavior. Research participants were 4,689 Taiwanese secondary students. The results show that the EIAQ has desirable construct validity and reliability. Significant differences occur between the two attitudes in each pair. Energy attitudes have medium correlations with energy affect and behavior but low correlations with energy knowledge. The results of structural equation modeling show that energy behavior is directly predicted by 'being' lifestyles and conformity to authorities, and indirectly predicted by energy-saving knowledge, mediated by energy affect.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/EXPLI3RB/Mei-Shiu et al. - 2018 - Development and Validation of the Energy-Issue Attitude Questionnaire Relations with Energy Knowled.pdf}
}

@article{menonChattingChatGPTAnalyzing2023,
  title = {“{{Chatting}} with {{ChatGPT}}”: {{Analyzing}} the Factors Influencing Users' Intention to {{Use}} the {{Open AI}}'s {{ChatGPT}} Using the {{UTAUT}} Model},
  shorttitle = {“{{Chatting}} with {{ChatGPT}}”},
  author = {Menon, Devadas and Shilpa, K},
  date = {2023-11-01},
  journaltitle = {Heliyon},
  shortjournal = {Heliyon},
  volume = {9},
  number = {11},
  pages = {e20962},
  issn = {2405-8440},
  doi = {10.1016/j.heliyon.2023.e20962},
  url = {https://www.sciencedirect.com/science/article/pii/S2405844023081707},
  urldate = {2025-05-02},
  abstract = {Open AI's ChatGPT has emerged as a popular AI language model that can engage in natural language conversations with users. Based on a qualitative research approach using semistructured interviews with 32 ChatGPT users from India, this study examined the factors influencing users' acceptance and use of ChatGPT using the unified theory of acceptance and usage of technology (UTAUT) model. The study results demonstrated that the four factors of UTAUT, along with two extended constructs, i.e. perceived interactivity and privacy concerns, can explain users' interaction and engagement with ChatGPT. The study also found that age and experience can moderate the impact of various factors on the use of ChatGPT. The theoretical and practical implications of the study were also discussed.},
  keywords = {Acceptance and use,Chatbots,ChatGPT,OpenAI,UTAUT},
  file = {/Users/thomasgorman/Zotero/storage/QW8IKTND/Menon and Shilpa - 2023 - “Chatting with ChatGPT” Analyzing the factors influencing users' intention to Use the Open AI's Cha.pdf;/Users/thomasgorman/Zotero/storage/34RQ4BAH/S2405844023081707.html}
}

@article{miniardSharedVisionDecarbonized2020,
  title = {Shared Vision for a Decarbonized Future Energy System in the {{United States}}},
  author = {Miniard, Deidra and Kantenbacher, Joseph and Attari, Shahzeen Z.},
  date = {2020-03-31},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {117},
  number = {13},
  pages = {7108--7114},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1920558117},
  url = {https://pnas.org/doi/full/10.1073/pnas.1920558117},
  urldate = {2025-05-05},
  abstract = {How do people envision the future energy system in the United States with respect to using fossil fuels, renewable energy, and nuclear energy? Are there shared policy pathways of achieving a decarbonized energy system? Here, we present results of an online survey (               n               = 2,429) designed to understand public perceptions of the current and future energy mixes in the United States (i.e., energy sources used for electric power, transportation, industrial, commercial, and residential sectors). We investigate support for decarbonization policies and antidecarbonization policies and the relative importance of climate change as an issue. Surprisingly, we find bipartisan support for a decarbonized energy future. Although there is a shared vision for decarbonization, there are strong partisan differences regarding the policy pathways for getting there. On average, our participants think that climate change is not the most important problem facing the United States today, but they do view climate change as an important issue for the world today and for the United States and the world in the future.},
  langid = {english},
  annotation = {https://www.szattari.com/publications\\
\\
https://static1.squarespace.com/static/54e39dcfe4b033c7e0e77c20/t/5e6ff908c80356332ba7042a/1584396554209/EnergyMix\_SI.pdf},
  file = {/Users/thomasgorman/Zotero/storage/NFQSVXT3/Miniard et al. - 2020 - Shared vision for a decarbonized future energy system in the United States.pdf}
}

@article{montagCanWeAssess2025,
  title = {Can {{We Assess Attitudes Toward AI}} with {{Single Items}}? {{Associations}} with {{Existing Attitudes Toward AI Measures}} and {{Trust}} in {{ChatGPT}}},
  shorttitle = {Can {{We Assess Attitudes Toward AI}} with {{Single Items}}?},
  author = {Montag, Christian and Ali, Raian},
  date = {2025-02-11},
  journaltitle = {Journal of Technology in Behavioral Science},
  shortjournal = {J. technol. behav. sci.},
  issn = {2366-5963},
  doi = {10.1007/s41347-025-00481-7},
  url = {https://link.springer.com/10.1007/s41347-025-00481-7},
  urldate = {2025-05-01},
  abstract = {A growing number of researchers investigate individual differences in attitudes toward Artificial Intelligence (AI), which is not surprising given that the AI revolution is impacting societies around the globe. Different frameworks have been proposed to study both positive and negative attitudes toward AI. To our knowledge, the present work is the first to simultaneously investigate the ATAI (Attitudes Toward Artificial Intelligence Scale) and the GAAIS (General Attitudes Towards Artificial Intelligence Scale). Further, two single items assessing positive and negative attitudes toward AI were added to the study to see if they would grasp substantial parts of the variance of the already established ATAI and GAAIS inventories. Correlations were of moderate to large effect size when comparing associations between the single-item measures and both ATAI and GAAI scales (German speaking sample 1 = 151 participants; German speaking sample 2 = 386). Finally, also associations with trusting the generative AI ChatGPT were included as external validation measurement in both investigated samples. Results revealed that all attitudes toward AI measures were associated with trusting ChatGPT. Moreover, a stepwise regression model demonstrated that the acceptance scale of the ATAI was the best predictor for trust in ChatGPT in sample 1, with more predictors in sample 2. The present work shows substantial overlap between the available attitudes towards AI measures, and this could be replicated in two samples. These insights can help future researchers and AI designers to choose the appropriate survey tool when considering to assess attitudes toward AI.},
  langid = {english},
  annotation = {https://osf.io/pmdhn/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/3N5ZK5KB/Montag and Ali - 2025 - Can We Assess Attitudes Toward AI with Single Items Associations with Existing Attitudes Toward AI.pdf}
}

@article{morales-garciaAdaptationPsychometricProperties2024,
  title = {Adaptation and Psychometric Properties of a Brief Version of the General Self-Efficacy Scale for Use with Artificial Intelligence ({{GSE-6AI}}) among University Students},
  author = {Morales-García, Wilter C. and Sairitupa-Sanchez, Liset Z. and Morales-García, Sandra B. and Morales-García, Mardel},
  date = {2024-03-08},
  journaltitle = {Frontiers in Education},
  shortjournal = {Front. Educ.},
  volume = {9},
  publisher = {Frontiers},
  issn = {2504-284X},
  doi = {10.3389/feduc.2024.1293437},
  url = {https://www.frontiersin.orghttps://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1293437/full},
  urldate = {2025-05-02},
  abstract = {Background Individual beliefs about one’s ability to carry out tasks and face challenges play a pivotal role in academic and professional formation. In the contemporary technological landscape, Artificial Intelligence (AI) is effecting profound changes across multiple sectors. Adaptation to this technology varies greatly among individuals. The integration of AI in the educational setting has necessitated a tool that measures self-efficacy concerning the adoption and use of this technology. Objective To adapt and validate a short version of the General Self-Efficacy Scale (GSE-6) for self-efficacy in the use of Artificial Intelligence (GSE-6AI) in a university student population. Methods An instrumental study was conducted with the participation of 469 medical students aged between 18 and 29 (M = 19.71; SD = 2.47). The GSE-6 was adapted to the AI context, following strict translation and cultural adaptation procedures. Its factorial structure was evaluated through confirmatory factorial analysis (CFA). Additionally, the factorial invariance of the scale based on gender was studied. Results The GSE-6AI exhibited a unidimensional structure with excellent fit indices. All item factorial loads surpassed the recommended threshold, and both Cronbach’s Alpha (α) and McDonald’s Omega (ω) achieved a value of 0.91. Regarding factorial invariance by gender, the scale proved to maintain its structure and meaning in both men and women. Conclusion The adapted GSE-6AI version is a valid and reliable tool for measuring self-efficacy in the use of Artificial Intelligence among university students. Its unidimensional structure and gender-related factorial invariance make it a robust and versatile tool for future research and practical applications in educational and technological contexts.},
  langid = {english},
  keywords = {adaptation,Artificial intelligence (AI),GSE-6AI,invariance,self-efficacy,Technological},
  file = {/Users/thomasgorman/Zotero/storage/7LGUXQNG/Morales-García et al. - 2024 - Adaptation and psychometric properties of a brief version of the general self-efficacy scale for use.pdf}
}

@online{morrillShortformAILiteracy2023,
  title = {A Short-Form {{AI}} Literacy Intervention Can Reduce over-Reliance on {{AI}}},
  author = {Morrill, Jake and Noetel, Michael},
  date = {2023-12-12},
  eprinttype = {OSF},
  doi = {10.31234/osf.io/hv9qc},
  url = {https://osf.io/hv9qc_v1},
  urldate = {2025-04-28},
  abstract = {Artificial intelligence (AI) is becoming very capable, and can match or even exceed human performance in various tasks. Human-AI teams are necessary to maintain oversight over AI, however, humans tend to over-rely on AI, especially when their trust is high. New research has emerged showing that interventions that improve AI literacy—knowledge and understanding of AI—appear to reduce over-reliance on AI, however it is unclear what is mediating this effect. Further, AI literacy in the general public is poor, and short-form interventions that aim to teach about important AI concepts are sparse. We aimed to test whether a short, 5-minute text-based AI literacy intervention would improve AI literacy and reduce over-reliance on AI. We recruited 153 undergraduate psychology students from the University of Queensland and randomly assigned them to receive either the AI literacy intervention or a similarly-lengthed control material unrelated to AI, and subsequently tested their reliance on AI in a human-AI team context. We found that, compared with the control condition, those who received the literacy intervention had significantly improved AI literacy and significantly reduced over-reliance, though there was no indirect effect of the intervention on over-reliance through AI literacy or trust. Our findings highlight the potential for short-form AI literacy interventions in not only improving AI literacy but reducing over-reliance on AI in a human-AI team context, however, more research is necessary to bring clarity to what may be mediating this effect.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {AI,AI Literacy,Artificial Intelligence,Reliance,Trust,Trust Calibration},
  file = {/Users/thomasgorman/Zotero/storage/YE9WA6UP/Morrill and Noetel - 2023 - A short-form AI literacy intervention can reduce over-reliance on AI.pdf}
}

@article{ngDesignValidationAI2024,
  title = {Design and Validation of the {{AI}} Literacy Questionnaire: {{The}} Affective, Behavioural, Cognitive and Ethical Approach},
  shorttitle = {Design and Validation of the {{AI}} Literacy Questionnaire},
  author = {Ng, Davy Tsz Kit and Wu, Wenjie and Leung, Jac Ka Lok and Chiu, Thomas Kin Fung and Chu, Samuel Kai Wah},
  date = {2024},
  journaltitle = {British Journal of Educational Technology},
  volume = {55},
  number = {3},
  pages = {1082--1104},
  issn = {1467-8535},
  doi = {10.1111/bjet.13411},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/bjet.13411},
  urldate = {2025-05-02},
  abstract = {Artificial intelligence (AI) literacy is at the top of the agenda for education today in developing learners' AI knowledge, skills, attitudes and values in the 21st century. However, there are few validated research instruments for educators to examine how secondary students develop and perceive their learning outcomes. After reviewing the literature on AI literacy questionnaires, we categorized the identified competencies in four dimensions: (1) affective learning (intrinsic motivation and self-efficacy/confidence), (2) behavioural learning (behavioural commitment and collaboration), (3) cognitive learning (know and understand; apply, evaluate and create) and (4) ethical learning. Then, a 32-item self-reported questionnaire on AI literacy (AILQ) was developed and validated to measure students' literacy development in the four dimensions. The design and validation of AILQ were examined through theoretical review, expert judgement, interview, pilot study and first- and second-order confirmatory factor analysis. This article reports the findings of a pilot study using a preliminary version of the AILQ among 363 secondary school students in Hong Kong to analyse the psychometric properties of the instrument. Results indicated a four-factor structure of the AILQ and revealed good reliability and validity. The AILQ is recommended as a reliable measurement scale for assessing how secondary students foster their AI literacy and inform better instructional design based on the proposed affective, behavioural, cognitive and ethical (ABCE) learning framework. Practitioner notes What is already known about this topic AI literacy has drawn increasing attention in recent years and has been identified as an important digital literacy. Schools and universities around the world started to incorporate AI into their curriculum to foster young learners' AI literacy. Some studies have worked to design suitable measurement tools, especially questionnaires, to examine students' learning outcomes in AI learning programmes. What this paper adds Develops an AI literacy questionnaire (AILQ) to evaluate students' literacy development in terms of affective, behavioural, cognitive and ethical (ABCE) dimensions. Proposes a parsimonious model based on the ABCE framework and addresses a skill set of AI literacy. Implications for practice and/or policy Researchers are able to use the AILQ as a guide to measure students' AI literacy. Practitioners are able to use the AILQ to assess students' AI literacy development.},
  langid = {english},
  keywords = {AI education,AI literacy,AI literacy questionnaire (AILQ),artificial intelligence,questionnaire validation},
  annotation = {https://bera-journals.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1111\%2Fbjet.13411\&file=bjet13411-sup-0001-AppendixS1.docx},
  file = {/Users/thomasgorman/Zotero/storage/BSIK3K2M/Ng et al. - 2024 - Design and validation of the AI literacy questionnaire The affective, behavioural, cognitive and et.pdf;/Users/thomasgorman/Zotero/storage/LUZY3DG5/bjet.html}
}

@online{niMeasurementLLMsPhilosophies2025,
  title = {Measurement of {{LLM}}'s {{Philosophies}} of {{Human Nature}}},
  author = {Ni, Minheng and Wu, Ennan and Gong, Zidong and Yang, Zhengyuan and Li, Linjie and Lin, Chung-Ching and Lin, Kevin and Wang, Lijuan and Zuo, Wangmeng},
  date = {2025-04-03},
  eprint = {2504.02304},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2504.02304},
  url = {http://arxiv.org/abs/2504.02304},
  urldate = {2025-05-05},
  abstract = {The widespread application of artificial intelligence (AI) in various tasks, along with frequent reports of conflicts or violations involving AI, has sparked societal concerns about interactions with AI systems. Based on Wrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically validated over decades to effectively assess individuals' attitudes toward human nature, we design the standardized psychological scale specifically targeting large language models (LLM), named the Machine-based Philosophies of Human Nature Scale (M-PHNS). By evaluating LLMs' attitudes toward human nature across six dimensions, we reveal that current LLMs exhibit a systemic lack of trust in humans, and there is a significant negative correlation between the model's intelligence level and its trust in humans. Furthermore, we propose a mental loop learning framework, which enables LLM to continuously optimize its value system during virtual interactions by constructing moral scenarios, thereby improving its attitude toward human nature. Experiments demonstrate that mental loop learning significantly enhances their trust in humans compared to persona or instruction prompts. This finding highlights the potential of human-based psychological assessments for LLM, which can not only diagnose cognitive biases but also provide a potential solution for ethical learning in artificial intelligence. We release the M-PHNS evaluation code and data at https://github.com/kodenii/M-PHNS.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/kodenii/M-PHNS},
  file = {/Users/thomasgorman/Zotero/storage/PGWR669A/Ni et al. - 2025 - Measurement of LLM's Philosophies of Human Nature.pdf;/Users/thomasgorman/Zotero/storage/FA7LJ3QF/2504.html}
}

@article{ovsyannikovaThirdpartyEvaluatorsPerceive2025,
  title = {Third-Party Evaluators Perceive {{AI}} as More Compassionate than Expert Humans},
  author = {Ovsyannikova, Dariya and De Mello, Victoria Oldemburgo and Inzlicht, Michael},
  date = {2025-01-10},
  journaltitle = {Communications Psychology},
  shortjournal = {Commun Psychol},
  volume = {3},
  number = {1},
  issn = {2731-9121},
  doi = {10.1038/s44271-024-00182-6},
  url = {https://www.nature.com/articles/s44271-024-00182-6},
  urldate = {2025-04-30},
  abstract = {AbstractEmpathy connects us but strains under demanding settings. This study explored how third parties evaluated AI-generated empathetic responses versus human responses in terms of compassion, responsiveness, and overall preference across four preregistered experiments. Participants (N\,=\,556) read empathy prompts describing valenced personal experiences and compared the AI responses to select non-expert or expert humans. Results revealed that AI responses were preferred and rated as more compassionate compared to select human responders (Study 1). This pattern of results remained when author identity was made transparent (Study 2), when AI was compared to expert crisis responders (Study 3), and when author identity was disclosed to all participants (Study 4). Third parties perceived AI as being more responsive—conveying understanding, validation, and care—which partially explained AI’s higher compassion ratings in Study 4. These findings suggest that AI has robust utility in contexts requiring empathetic interaction, with the potential to address the increasing need for empathy in supportive communication contexts.},
  langid = {english},
  annotation = {https://osf.io/wjx48/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/S4HCRTP4/Ovsyannikova et al. - 2025 - Third-party evaluators perceive AI as more compassionate than expert humans.pdf}
}

@article{pataranutapornInfluencingHumanAI2023,
  title = {Influencing Human–{{AI}} Interaction by Priming Beliefs about {{AI}} Can Increase Perceived Trustworthiness, Empathy and Effectiveness},
  author = {Pataranutaporn, Pat and Liu, Ruby and Finn, Ed and Maes, Pattie},
  date = {2023-10-02},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {5},
  number = {10},
  pages = {1076--1086},
  issn = {2522-5839},
  doi = {10.1038/s42256-023-00720-7},
  url = {https://www.nature.com/articles/s42256-023-00720-7},
  urldate = {2024-11-15},
  abstract = {As conversational agents powered by large language models become more human-like, users are starting to view them as companions rather than mere assistants. Our study explores how changes to a person’s mental model of an AI system affects their interaction with the system. Participants interacted with the same conversational AI, but were influenced by different priming statements regarding the AI’s inner motives: caring, manipulative or no motives. Here we show that those who perceived a caring motive for the AI also perceived it as more trustworthy, empathetic and better-performing, and that the effects of priming and initial mental models were stronger for a more sophisticated AI model. Our work also indicates a feedback loop in which the user and AI reinforce the user’s mental model over a short time; further work should investigate long-term effects. The research highlights the importance of how AI systems are introduced can notably affect the interaction and how the AI is experienced.},
  langid = {english},
  annotation = {https://github.com/mitmedialab/nmi-ai-2023\\
\\
https://zenodo.org/records/8136979},
  file = {/Users/thomasgorman/Zotero/storage/CYI4H6N5/Pataranutaporn et al. - 2023 - Influencing human–AI interaction by priming beliefs about AI can increase perceived trustworthiness,.pdf}
}

@inproceedings{perrigTrustIssuesTrust2023,
  title = {Trust {{Issues}} with {{Trust Scales}}: {{Examining}} the {{Psychometric Quality}} of {{Trust Measures}} in the {{Context}} of {{AI}}},
  shorttitle = {Trust {{Issues}} with {{Trust Scales}}},
  booktitle = {Extended {{Abstracts}} of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Perrig, Sebastian A. C. and Scharowski, Nicolas and Brühlmann, Florian},
  date = {2023-04-19},
  pages = {1--7},
  publisher = {ACM},
  location = {Hamburg Germany},
  doi = {10.1145/3544549.3585808},
  url = {https://dl.acm.org/doi/10.1145/3544549.3585808},
  urldate = {2025-05-01},
  eventtitle = {{{CHI}} '23: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  file = {/Users/thomasgorman/Zotero/storage/9CPQD79F/Perrig et al. - 2023 - Trust Issues with Trust Scales Examining the Psychometric Quality of Trust Measures in the Context.pdf}
}

@online{puppartShorttermAILiteracy2025,
  title = {Short-Term {{AI}} Literacy Intervention Does Not Reduce over-Reliance on Incorrect {{ChatGPT}} Recommendations},
  author = {Puppart, Brett and Aru, Jaan},
  date = {2025-03-13},
  eprint = {2503.10556},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.10556},
  url = {http://arxiv.org/abs/2503.10556},
  urldate = {2025-04-28},
  abstract = {In this study, we examined whether a short-form AI literacy intervention could reduce the adoption of incorrect recommendations from large language models. High school seniors were randomly assigned to either a control or an intervention group, which received an educational text explaining ChatGPT's working mechanism, limitations, and proper use. Participants solved math puzzles with the help of ChatGPT's recommendations, which were incorrect in half of the cases. Results showed that students adopted incorrect suggestions 52.1\% of the time, indicating widespread over-reliance. The educational intervention did not significantly reduce over-reliance. Instead, it led to an increase in ignoring ChatGPT's correct recommendations. We conclude that the usage of ChatGPT is associated with over-reliance and it is not trivial to increase AI literacy to counter over-reliance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computers and Society,Quantitative Biology - Neurons and Cognition},
  file = {/Users/thomasgorman/Zotero/storage/R4QHZNXK/Puppart and Aru - 2025 - Short-term AI literacy intervention does not reduce over-reliance on incorrect ChatGPT recommendatio.pdf;/Users/thomasgorman/Zotero/storage/XN87SGPV/2503.html}
}

@article{ravseljHigherEducationStudents2025,
  title = {Higher Education Students’ Perceptions of {{ChatGPT}}: {{A}} Global Study of Early Reactions},
  shorttitle = {Higher Education Students’ Perceptions of {{ChatGPT}}},
  author = {Ravšelj, Dejan and Keržič, Damijana and Tomaževič, Nina and Umek, Lan and Brezovar, Nejc and A. Iahad, Noorminshah and Abdulla, Ali Abdulla and Akopyan, Anait and Aldana Segura, Magdalena Waleska and AlHumaid, Jehan and Allam, Mohamed Farouk and Alló, Maria and Andoh, Raphael Papa Kweku and Andronic, Octavian and Arthur, Yarhands Dissou and Aydın, Fatih and Badran, Amira and Balbontín-Alvarado, Roxana and Ben Saad, Helmi and Bencsik, Andrea and Benning, Isaac and Besimi, Adrian and Bezerra, Denilson da Silva and Buizza, Chiara and Burro, Roberto and Bwalya, Anthony and Cachero, Cristina and Castillo-Briceno, Patricia and Castro, Harold and Chai, Ching Sing and Charalambous, Constadina and Chiu, Thomas K. F. and Clipa, Otilia and Colombari, Ruggero and Corral Escobedo, Luis José H. and Costa, Elísio and Crețulescu, Radu George and Crispino, Marta and Cucari, Nicola and Dalton, Fergus and Demir Kaya, Meva and Dumić-Čule, Ivo and Dwidienawati, Diena and Ebardo, Ryan and Egbenya, Daniel Lawer and Faris, MoezAlIslam Ezzat and Fečko, Miroslav and Ferrinho, Paulo and Florea, Adrian and Fong, Chun Yuen and Francis, Zoë and Ghilardi, Alberto and González-Fernández, Belinka and Hau, Daniela and Hossain, Md. Shamim and Hug, Theo and Inasius, Fany and Ismail, Maryam Jaffar and Jahić, Hatidža and Jessa, Morrison Omokiniovo and Kapanadze, Marika and Kar, Sujita Kumar and Kateeb, Elham Talib and Kaya, Feridun and Khadri, Hanaa Ouda and Kikuchi, Masao and Kobets, Vitaliy Mykolayovych and Kostova, Katerina Metodieva and Krasmane, Evita and Lau, Jesus and Law, Wai Him Crystal and Lazăr, Florin and Lazović-Pita, Lejla and Lee, Vivian Wing Yan and Li, Jingtai and López-Aguilar, Diego Vinicio and Luca, Adrian and Luciano, Ruth Garcia and Machin-Mastromatteo, Juan D. and Madi, Marwa and Manguele, Alexandre Lourenço and Manrique, Rubén Francisco and Mapulanga, Thumah and Marimon, Frederic and Marinova, Galia Ilieva and Mas-Machuca, Marta and Mejía-Rodríguez, Oliva and Meletiou-Mavrotheris, Maria and Méndez-Prado, Silvia Mariela and Meza-Cano, José Manuel and Mirķe, Evija and Mishra, Alpana and Mital, Ondrej and Mollica, Cristina and Morariu, Daniel Ionel and Mospan, Natalia and Mukuka, Angel and Navarro Jiménez, Silvana Guadalupe and Nikaj, Irena and Nisheva, Maria Mihaylova and Nisiforou, Efi and Njiku, Joseph and Nomnian, Singhanat and Nuredini-Mehmedi, Lulzime and Nyamekye, Ernest and Obadić, Alka and Okela, Abdelmohsen Hamed and Olenik-Shemesh, Dorit and Ostoj, Izabela and Peralta-Rizzo, Kevin Javier and Peštek, Almir and Pilav-Velić, Amila and Pires, Dilma Rosanda Miranda and Rabin, Eyal and Raccanello, Daniela and Ramie, Agustine and family=Rashid, given=Md. Mamun, prefix=ur, useprefix=false and Reuter, Robert A. P. and Reyes, Valentina and Rodrigues, Ana Sofia and Rodway, Paul and Ručinská, Silvia and Sadzaglishvili, Shorena and Salem, Ashraf Atta M. S. and Savić, Gordana and Schepman, Astrid and Shahpo, Samia Mokhtar and Snouber, Abdelmajid and Soler, Emma and Sonyel, Bengi and Stefanova, Eliza and Stone, Anna and Strzelecki, Artur and Tanaka, Tetsuji and Tapia Cortes, Carolina and Teira-Fachado, Andrea and Tilga, Henri and Titko, Jelena and Tolmach, Maryna and Turmudi, Dedi and Varela-Candamio, Laura and Vekiri, Ioanna and Vicentini, Giada and Woyo, Erisher and Yorulmaz, Özlem and Yunus, Said A. S. and Zamfir, Ana-Maria and Zhou, Munyaradzi and Aristovnik, Aleksander},
  date = {2025-02-05},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS One},
  volume = {20},
  number = {2},
  pages = {e0315011},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0315011},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11798494/},
  urldate = {2025-05-02},
  abstract = {The paper presents the most comprehensive and large-scale global study to date on how higher education students perceived the use of ChatGPT in early 2024. With a sample of 23,218 students from 109 countries and territories, the study reveals that students primarily used ChatGPT for brainstorming, summarizing texts, and finding research articles, with a few using it for professional and creative writing. They found it useful for simplifying complex information and summarizing content, but less reliable for providing information and supporting classroom learning, though some considered its information clearer than that from peers and teachers. Moreover, students agreed on the need for AI regulations at all levels due to concerns about ChatGPT promoting cheating, plagiarism, and social isolation. However, they believed ChatGPT could potentially enhance their access to knowledge and improve their learning experience, study efficiency, and chances of achieving good grades. While ChatGPT was perceived as effective in potentially improving AI literacy, digital communication, and content creation skills, it was less useful for interpersonal communication, decision-making, numeracy, native language proficiency, and the development of critical thinking skills. Students also felt that ChatGPT would boost demand for AI-related skills and facilitate remote work without significantly impacting unemployment. Emotionally, students mostly felt positive using ChatGPT, with curiosity and calmness being the most common emotions. Further examinations reveal variations in students’ perceptions across different socio-demographic and geographic factors, with key factors influencing students’ use of ChatGPT also being identified. Higher education institutions’ managers and teachers may benefit from these findings while formulating the curricula and instructions/regulations for ChatGPT use, as well as when designing the teaching methods and assessment tools. Moreover, policymakers may also consider the findings when formulating strategies for secondary and higher education system development, especially in light of changing labor market needs and related digital skills development.},
  annotation = {https://data.mendeley.com/datasets/ymg9nsn6kn/2},
  file = {/Users/thomasgorman/Zotero/storage/CKJUXMHM/Ravšelj et al. - 2025 - Higher education students’ perceptions of ChatGPT A global study of early reactions.pdf}
}

@article{razinConvergingMeasuresEmergent2024,
  title = {Converging {{Measures}} and an {{Emergent Model}}: {{A Meta-Analysis}} of {{Human-Machine Trust Questionnaires}}},
  shorttitle = {Converging {{Measures}} and an {{Emergent Model}}},
  author = {Razin, Yosef S. and Feigh, Karen M.},
  date = {2024-11-01},
  journaltitle = {J. Hum.-Robot Interact.},
  volume = {13},
  number = {4},
  pages = {58:1--58:41},
  doi = {10.1145/3677614},
  url = {https://dl.acm.org/doi/10.1145/3677614},
  urldate = {2025-05-02},
  abstract = {Trust is crucial for technological acceptance, continued usage, and teamwork. However, human-robot trust, and human-machine trust more generally, suffer from terminological disagreement and construct proliferation. By comparing, mapping, and analyzing well-constructed trust survey instruments, this work uncovers a consensus structure of trust in human–machine interaction. To do so, we identify the most frequently cited and best-validated human-machine and human-robot trust questionnaires as well as the best-established factors that form the dimensions and antecedents of such trust. To reduce both confusion and construct proliferation, we provide a detailed mapping of terminology between questionnaires. Furthermore, we perform a meta-analysis of the regression models which emerged from the experiments that employed multi-factorial survey instruments. Based on this meta-analysis, we provide the most complete, experimentally validated model of human-machine and human-robot trust to date. This convergent model establishes an integrated framework for future research. It determines the current boundaries of trust measurement and where further investigation and validation are necessary. We close by discussing how to choose an appropriate trust survey instrument and how to design for trust. By identifying the internal workings of trust, a more complete basis for measuring trust is developed that is widely applicable.},
  file = {/Users/thomasgorman/Zotero/storage/YGIQGGFS/Razin and Feigh - 2024 - Converging Measures and an Emergent Model A Meta-Analysis of Human-Machine Trust Questionnaires.pdf}
}

@inproceedings{rheuTrapAILiteracy2025,
  title = {The {{Trap}} of {{AI Literacy}}: {{The Paradoxical Relationships Between College Students}}’ {{Use}} of {{LLMs}}, {{AI Literacy}}, and {{Fact-checking Behavior}}},
  shorttitle = {The {{Trap}} of {{AI Literacy}}},
  booktitle = {Proceedings of the {{Extended Abstracts}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Rheu, Minjin (MJ) and Cho, Janghee},
  date = {2025-04-25},
  series = {{{CHI EA}} '25},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3706599.3719681},
  url = {https://dl.acm.org/doi/10.1145/3706599.3719681},
  urldate = {2025-04-29},
  abstract = {This study examines factors influencing users’ critical engagement with large language models (LLMs), focusing on fact-checking behavior. While LLMs transform how individuals acquire knowledge, their rapid adoption raises concerns about uncritical acceptance due to limitations like hallucinations. A survey of college students and young professionals revealed nuanced effects of LLM literacy. Understanding LLM processes, such as input, processing, and output, encourages fact-checking. However, self-efficacy and knowledge of LLM features paradoxically reduce verification by fostering reliance on machine heuristics and elevating the perceived credibility of outputs. These findings highlight the complex role of AI literacy in promoting critical engagement and the importance of education to deepen users’ technological understanding.},
  isbn = {979-8-4007-1395-8},
  file = {/Users/thomasgorman/Zotero/storage/HQKXC9UF/Rheu and Cho - 2025 - The Trap of AI Literacy The Paradoxical Relationships Between College Students’ Use of LLMs, AI Lit.pdf}
}

@article{salahChattingChatGPTDecoding2024,
  title = {Chatting with {{ChatGPT}}: Decoding the Mind of {{Chatbot}} Users and Unveiling the Intricate Connections between User Perception, Trust and Stereotype Perception on Self-Esteem and Psychological Well-Being},
  shorttitle = {Chatting with {{ChatGPT}}},
  author = {Salah, Mohammed and Alhalbusi, Hussam and Ismail, Maria Mohd and Abdelfattah, Fadi},
  date = {2024-03-01},
  journaltitle = {Current Psychology},
  shortjournal = {Curr Psychol},
  volume = {43},
  number = {9},
  pages = {7843--7858},
  issn = {1936-4733},
  doi = {10.1007/s12144-023-04989-0},
  url = {https://link.springer.com/article/10.1007/s12144-023-04989-0},
  urldate = {2025-03-12},
  abstract = {Artificial Intelligence (AI) technology has revolutionized how we interact with information and entertainment, with ChatGPT, a language model developed by OpenAI, being among its prominent applications. However, knowledge regarding the psychological impact of interacting with ChatGPT is limited. This study investigated the relationships between trust in ChatGPT; ChatGPT’s user perceptions; perceived stereotyping by ChatGPT; and two psychological outcomes, namely, psychological well-being and self-esteem. This study hypothesized that the former three variables exhibit a positive direct relationship with self-esteem. Additionally, the study proposed that job anxiety moderates the associations among trust in ChatGPT, user perceptions of ChatGPT, and psychological well-being. Using a survey design, data were collected from 732 participants and analyzed using SEM and SmartPLS analysis. Notably, perceived stereotyping by ChatGPT significantly predicted self-esteem, while user perceptions of ChatGPT and trust in ChatGPT exhibited a positive direct relationship with self-esteem. Additionally, job anxiety moderated the relationship between ChatGPT’s user perceptions and psychological well-being. These results provide important insights into the psychological effects of interacting with AI technology and highlight job anxiety’s role in moderating these effects. This study’s findings have implications for developing and using AI technology in various fields, including mental health and human-robot interactions.},
  langid = {english},
  annotation = {https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-024-00471-4/tables/6},
  file = {/Users/thomasgorman/Zotero/storage/RMK89WZB/Salah et al. - 2024 - Chatting with ChatGPT decoding the mind of Chatbot users and unveiling the intricate connections be.pdf}
}

@article{scantamburloArtificialIntelligenceEurope2025,
  title = {Artificial {{Intelligence Across Europe}}: {{A Study}} on {{Awareness}}, {{Attitude}} and {{Trust}}},
  shorttitle = {Artificial {{Intelligence Across Europe}}},
  author = {Scantamburlo, Teresa and Cortés, Atia and Foffano, Francesca and Barrué, Cristian and Distefano, Veronica and Pham, Long and Fabris, Alessandro},
  date = {2025-02},
  journaltitle = {IEEE Transactions on Artificial Intelligence},
  volume = {6},
  number = {2},
  pages = {477--490},
  issn = {2691-4581},
  doi = {10.1109/TAI.2024.3461633},
  url = {https://ieeexplore.ieee.org/document/10681325/},
  urldate = {2025-04-30},
  abstract = {This article presents the results of an extensive study investigating the opinions on artificial intelligence (AI) of a sample of 4006 European citizens from eight distinct countries (France, Germany, Italy, Netherlands, Poland, Romania, Spain, and Sweden). The aim of the study is to gain a better understanding of people's views and perceptions within the European context, which is already marked by important policy actions and regulatory processes. To survey the perceptions of the citizens of Europe, we design and validate a new questionnaire (PAICE) structured around three dimensions: people's awareness, attitude, and trust. We observe that while awareness is characterized by a low level of self-assessed competency, the attitude toward AI is very positive for more than half of the population. Reflecting on the collected results, we highlight implicit contradictions and identify trends that may interfere with the creation of an ecosystem of trust and the development of inclusive AI policies. The introduction of rules that ensure legal and ethical standards, along with the activity of high-level educational entities, and the promotion of AI literacy are identified as key factors in supporting a trustworthy AI ecosystem. We make some recommendations for AI governance focused on the European context and conclude with suggestions for future work.},
  keywords = {AI,AI Policy,Artificial intelligence,Ecosystems,Ethics,Europe,Market research,Privacy,Public Perception,Surveys},
  annotation = {https://github.com/EU-Survey/Material},
  file = {/Users/thomasgorman/Zotero/storage/AKM3623B/Scantamburlo et al. - 2025 - Artificial Intelligence Across Europe A Study on Awareness, Attitude and Trust.pdf}
}

@online{scharowskiTrustDistrustTrust2025,
  title = {To {{Trust}} or {{Distrust Trust Measures}}: {{Validating Questionnaires}} for {{Trust}} in {{AI}}},
  shorttitle = {To {{Trust}} or {{Distrust Trust Measures}}},
  author = {Scharowski, Nicolas and Perrig, Sebastian A. C. and Aeschbach, Lena Fanya and family=Felten, given=Nick, prefix=von, useprefix=false and Opwis, Klaus and Wintersberger, Philipp and Brühlmann, Florian},
  date = {2025-01-15},
  eprint = {2403.00582},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2403.00582},
  url = {http://arxiv.org/abs/2403.00582},
  urldate = {2025-04-28},
  abstract = {Despite the importance of trust in human-AI interactions, researchers must adopt questionnaires from other disciplines that lack validation in the AI context. Motivated by the need for reliable and valid measures, we investigated the psychometric quality of two trust questionnaires, the Trust between People and Automation scale (TPA) by Jian et al. (2000) and the Trust Scale for the AI Context (TAI) by Hoffman et al. (2023). In a pre-registered online experiment (N = 1485), participants observed interactions with trustworthy and untrustworthy AI (autonomous vehicle and chatbot). Results support the psychometric quality of the TAI while revealing opportunities to improve the TPA, which we outline in our recommendations for using the two questionnaires. Furthermore, our findings provide additional empirical evidence of trust and distrust as two distinct constructs that may coexist independently. Building on our findings, we highlight the opportunities and added value of measuring both trust and distrust in human-AI research and advocate for further work on both constructs.},
  pubstate = {prepublished},
  keywords = {Computer Science - Human-Computer Interaction},
  annotation = {https://osf.io/7cdne/files/osfstorage?view\_only=ad812bc898154990959a50aaea43ca61},
  file = {/Users/thomasgorman/Zotero/storage/YYR5W4MR/Scharowski et al. - 2025 - To Trust or Distrust Trust Measures Validating Questionnaires for Trust in AI.pdf;/Users/thomasgorman/Zotero/storage/824DVBMF/2403.html}
}

@article{schepmanGeneralAttitudesArtificial2023,
  title = {The {{General Attitudes}} towards {{Artificial Intelligence Scale}} ({{GAAIS}}): {{Confirmatory Validation}} and {{Associations}} with {{Personality}}, {{Corporate Distrust}}, and {{General Trust}}},
  shorttitle = {The {{General Attitudes}} towards {{Artificial Intelligence Scale}} ({{GAAIS}})},
  author = {Schepman, Astrid and family=Rodway, given=Paul, prefix=and, useprefix=true},
  date = {2023-08-09},
  journaltitle = {International Journal of Human–Computer Interaction},
  volume = {39},
  number = {13},
  pages = {2724--2741},
  publisher = {Taylor \& Francis},
  issn = {1044-7318},
  doi = {10.1080/10447318.2022.2085400},
  url = {https://doi.org/10.1080/10447318.2022.2085400},
  urldate = {2025-05-03},
  abstract = {Acceptance of Artificial Intelligence (AI) may be predicted by individual psychological correlates, examined here. Study 1 reports confirmatory validation of the General Attitudes towards Artificial Intelligence Scale (GAAIS) following initial validation elsewhere. Confirmatory Factor Analysis confirmed the two-factor structure (Positive, Negative) and showed good convergent and divergent validity with a related scale. Study 2 tested whether psychological factors (Big Five personality traits, corporate distrust, and general trust) predicted attitudes towards AI. Introverts had more positive attitudes towards AI overall, likely because of algorithm appreciation. Conscientiousness and agreeableness were associated with forgiving attitudes towards negative aspects of AI. Higher corporate distrust led to negative attitudes towards AI overall, while higher general trust led to positive views of the benefits of AI. The dissociation between general trust and corporate distrust may reflect the public’s attributions of the benefits and drawbacks of AI. Results are discussed in relation to theory and prior findings.},
  file = {/Users/thomasgorman/Zotero/storage/ULUSAKYZ/Schepman and and Rodway - 2023 - The General Attitudes towards Artificial Intelligence Scale (GAAIS) Confirmatory Validation and Ass.pdf}
}

@inproceedings{schille-hudsonBigHotBright2019,
  title = {Big, Hot, or Bright? {{Integrating}} Cues to Perceive Home Energy Use},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Schille-Hudson, Eleanor B and Margehtis, Tyler and Miniard, Deidra and Landy, David and Attari, Shahzeen Z.},
  date = {2019},
  volume = {41},
  url = {https://escholarship.org/uc/item/83z4w09n},
  abstract = {Despite constantly using energy and having extensive interactions with household appliances, people consistently mis-estimate the amount of energy that is used by home appliances. This poses major problems for conservation efforts, while also presenting an interesting case study in human perception. Since many forms of energy used are not directly perceptible, and since the amount of energy that is being used by an appliance is often difficult to infer from appearances alone, people often rely on cues. Some of these cues are more reliable than others and previous literature has investigated which of these cues people rely on. However, past literature has always studied these proximal cues in isolation—despite the fact that, during real-world perception, people are always integrating a variety of cues. Here, we investigate how people rely on a variety of cues, and how individual differences in the reliance on those cues predicts the ability to estimate home energy use.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Schille-Hudson et al. - 2019 - Big, hot, or bright Integrating cues to perceive home energy use.pdf}
}

@article{schleyCognitiveAccessibilityJudgments2015,
  title = {Cognitive Accessibility in Judgments of Household Energy Consumption},
  author = {Schley, Dan R. and DeKay, Michael L.},
  date = {2015-09},
  journaltitle = {Journal of Environmental Psychology},
  shortjournal = {Journal of Environmental Psychology},
  volume = {43},
  pages = {30--41},
  issn = {02724944},
  doi = {10.1016/j.jenvp.2015.05.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0272494415300049},
  urldate = {2024-12-07},
  abstract = {Individuals appear to use the frequency that they interact with and think about various energyconsuming devices (the cognitive accessibility of those devices) when estimating relative energy consumption. This conclusion is based on 3 studies in which 608 participants estimated the percentages of total individual and household energy consumed annually in the U.S. by several categories of devices (e.g., lighting, cooking, water heating, air conditioning, computers, private motor vehicles) and 1 study in which 125 participants made similar estimates for their own consumption. Additionally, seasonal and geographical variations in local temperature predicted national annual consumption estimates for home heating and air conditioning, with these relationships being mediated by cognitive accessibility. Changes in available information, including more accessible cross-category and cross-fuel comparisons, greater media attention to high-consumption categories and high-impact solutions, and more disaggregated feedback regarding household energy use, could potentially improve consumers' understanding of relative consumption and hence their energy-conservation decisions.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/2BFGJABS/Schley and DeKay - 2015 - Cognitive accessibility in judgments of household energy consumption.pdf}
}

@article{scholzMeasuringPropensityTrust2025,
  title = {Measuring the {{Propensity}} to {{Trust}} in {{Automated Technology}}: {{Examining Similarities}} to {{Dispositional Trust}} in {{Other Humans}} and {{Validation}} of the {{PTT-A Scale}}},
  shorttitle = {Measuring the {{Propensity}} to {{Trust}} in {{Automated Technology}}},
  author = {Scholz, David D. and , Johannes, Kraus and family=Miller, given=Linda, prefix=and, useprefix=true},
  date = {2025-01-17},
  journaltitle = {International Journal of Human–Computer Interaction},
  volume = {41},
  number = {2},
  pages = {970--993},
  issn = {1044-7318},
  doi = {10.1080/10447318.2024.2307691},
  url = {https://doi.org/10.1080/10447318.2024.2307691},
  urldate = {2025-05-02},
  abstract = {In this work, an integrative theoretical structure for the propensity to trust (PTT) is derived from literature. In an online study (N = 669), the validity of the structure was assessed and compared in two domains: propensity to trust in humans (PTT-H) and propensity to trust in automated technology (PTT-A). Based on this, an economic scale to measure PTT-A was derived and its psychometric quality was explored based on the first and an additional second study. The observed correlational pattern to basic personality traits supports the convergent validity of PTT-A. Moreover, discriminative predictive validity of PTT-A over PTT-H was supported by its higher relationships to technology-related outcomes. Additionally, incremental validity of PTT-A over basic personality traits was supported. Finally, the internal validity of the scale was replicated in an independent sample and re-test reliability was established. The findings support the added value of integrating PTT-A in research on the interaction with automated technology.},
  keywords = {dispositional trust,interpersonal trust,Propensity to trust,trust in AI,trust in artificial intelligence,trust in automation,trust in technology},
  annotation = {https://osf.io/bkw23/files/osfstorage\\
\\
https://osf.io/ntypg/\\
\\
https://osf.io/bkw23/files/osfstorage\\
\\
https://osf.io/wmukf/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/WHZ627G3/Scholz et al. - 2025 - Measuring the Propensity to Trust in Automated Technology Examining Similarities to Dispositional T.pdf}
}

@inproceedings{shangTrustingYourAI2025,
  title = {Trusting {{Your AI Agent Emotionally}} and {{Cognitively}}: {{Development}} and {{Validation}} of a {{Semantic Differential Scale}} for {{AI Trust}}},
  shorttitle = {Trusting {{Your AI Agent Emotionally}} and {{Cognitively}}},
  booktitle = {Proceedings of the 2024 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Shang, Ruoxi and Hsieh, Gary},
  date = {2025-02-07},
  series = {{{AIES}} '24},
  pages = {1343--1356},
  publisher = {AAAI Press},
  location = {San Jose, California, USA},
  abstract = {Trust is not just a cognitive issue but also an emotional one, yet the research in human-AI interactions has primarily focused on the cognitive route of trust development. Recent work has highlighted the importance of studying affective trust towards AI, especially in the context of emerging human-like LLM-powered conversational agents. However, there is a lack of validated and generalizable measures for the two-dimensional construct of trust in AI agents. To address this gap, we developed and validated a set of 27-item semantic differential scales for affective and cognitive trust through a scenario-based survey study. We then further validated and applied the scale through an experiment study. Our empirical findings showed how the emotional and cognitive aspects of trust interact with each other and collectively shape a person's overall trust in AI agents. Our study methodology and findings also provide insights into the capability of the state-of-art LLMs to foster trust through different routes.},
  file = {/Users/thomasgorman/Zotero/storage/SSJ758G6/Shang et al. - 2025 - Trusting Your AI Agent Emotionally and Cognitively Development and Validation of a Semantic Differe.pdf}
}

@article{sindermannAssessingAttitudeArtificial2021,
  title = {Assessing the {{Attitude Towards Artificial Intelligence}}: {{Introduction}} of a {{Short Measure}} in {{German}}, {{Chinese}}, and {{English Language}}},
  shorttitle = {Assessing the {{Attitude Towards Artificial Intelligence}}},
  author = {Sindermann, Cornelia and Sha, Peng and Zhou, Min and Wernicke, Jennifer and Schmitt, Helena S. and Li, Mei and Sariyska, Rayna and Stavrou, Maria and Becker, Benjamin and Montag, Christian},
  date = {2021-03-01},
  journaltitle = {KI - Künstliche Intelligenz},
  shortjournal = {Künstl Intell},
  volume = {35},
  number = {1},
  pages = {109--118},
  publisher = {Springer Berlin Heidelberg},
  issn = {1610-1987},
  doi = {10.1007/s13218-020-00689-0},
  url = {https://link.springer.com/article/10.1007/s13218-020-00689-0},
  urldate = {2025-05-01},
  abstract = {In the context of (digital) human–machine interaction, people are increasingly dealing with artificial intelligence in everyday life. Through this, we observe humans who embrace technological advances with a positive attitude. Others, however, are particularly sceptical and claim to foresee substantial problems arising from such uses of technology. The aim of the present study was to introduce a short measure to assess the Attitude Towards Artificial Intelligence (ATAI scale) in the German, Chinese, and English languages. Participants from Germany (N\,=\,461; 345 females), China (N\,=\,413; 145 females), and the UK (N\,=\,84; 65 females) completed the ATAI scale, for which the factorial structure was tested and compared between the samples. Participants from Germany and China were additionally asked about their willingness to interact with/use self-driving cars, Siri, Alexa, the social robot Pepper, and the humanoid robot Erica, which are representatives of popular artificial intelligence products. The results showed that the five-item ATAI scale comprises two negatively associated factors assessing (1) acceptance and (2) fear of artificial intelligence. The factor structure was found to be similar across the German, Chinese, and UK samples. Additionally, the ATAI scale was validated, as the items on the willingness to use specific artificial intelligence products were positively associated with the ATAI Acceptance scale and negatively with the ATAI Fear scale, in both the German and Chinese samples. In conclusion we introduce a short, reliable, and valid measure on the attitude towards artificial intelligence in German, Chinese, and English language.},
  issue = {1},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/JVC4MCCX/Sindermann et al. - 2021 - Assessing the Attitude Towards Artificial Intelligence Introduction of a Short Measure in German, C.pdf}
}

@article{smithNavigatingAIConvergence2025,
  title = {Navigating {{AI Convergence}} in {{Human}}–{{Artificial Intelligence Teams}}: {{A Signaling Theory Approach}}},
  shorttitle = {Navigating {{AI Convergence}} in {{Human}}–{{Artificial Intelligence Teams}}},
  author = {Smith, Andria and family=Wagoner, given=Hunter Phoenix, prefix=van, useprefix=true and Keplinger, Ksenia and Celebi, Can},
  date = {2025},
  journaltitle = {Journal of Organizational Behavior},
  issn = {1099-1379},
  doi = {10.1002/job.2856},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/job.2856},
  urldate = {2025-01-23},
  abstract = {Teams that combine human intelligence with artificial intelligence (AI) have become indispensable for solving complex tasks in various decision-making contexts in modern organizations. However, the factors that contribute to AI convergence, where human team members align their decisions with those of their AI counterparts, still remain unclear. This study integrates signaling theory with self-determination theory to investigate how specific signals—such as signal fit, optional AI advice, and signal set congruence—affect employees' AI convergence in human–AI teams. Based on four experimental studies conducted in facial recognition and hiring contexts with approximately 1100 participants, the findings highlight the significant positive impact of congruent signals from both human and AI team members on AI convergence. Moreover, providing an option for employees to solicit AI advice also enhances AI convergence; when AI signals are chosen by employees rather than forced upon them, participants are more likely to accept AI advice. This research advances knowledge on human–AI teaming by (1) expanding signaling theory into the human–AI team context; (2) developing a deeper understanding of AI convergence and its drivers in human–AI teams; (3) providing actionable insights for designing teams and tasks to optimize decision-making in high-stakes, uncertain environments; and (4) introducing facial recognition as an innovative context for human–AI teaming.},
  langid = {english},
  keywords = {AI convergence,artificial intelligence,human–AI teaming,optional advice,signaling theory},
  annotation = {https://osf.io/zjwta/?view\_only=74a15f71b21c4b0abb69f3e5de543537\\
\\
.},
  file = {/Users/thomasgorman/Zotero/storage/IME48J8B/Smith et al. - 2025 - Navigating AI Convergence in Human–Artificial Intelligence Teams A Signaling Theory Approach.pdf}
}

@article{songMultiAgentsAreSocial2024,
  title = {Multi-{{Agents}} Are {{Social Groups}}: {{Investigating Social Influence}} of {{Multiple Agents}} in {{Human-Agent Interactions}}},
  shorttitle = {Multi-{{Agents}} Are {{Social Groups}}},
  author = {Song, Tianqi and Tan, Yugin and Zhu, Zicheng and Feng, Yibin and Lee, Yi-Chieh},
  date = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2411.04578},
  url = {https://arxiv.org/abs/2411.04578},
  urldate = {2025-02-25},
  abstract = {Multi-agent systems - systems with multiple independent AI agents working together to achieve a common goal - are becoming increasingly prevalent in daily life. Drawing inspiration from the phenomenon of human group social influence, we investigate whether a group of AI agents can create social pressure on users to agree with them, potentially changing their stance on a topic. We conducted a study in which participants discussed social issues with either a single or multiple AI agents, and where the agents either agreed or disagreed with the user's stance on the topic. We found that conversing with multiple agents (holding conversation content constant) increased the social pressure felt by participants, and caused a greater shift in opinion towards the agents' stances on each topic. Our study shows the potential advantages of multi-agent systems over single-agent platforms in causing opinion change. We discuss design implications for possible multi-agent systems that promote social good, as well as the potential for malicious actors to use these systems to manipulate public opinion.},
  version = {1},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Human-Computer Interaction (cs.HC)},
  file = {/Users/thomasgorman/Zotero/storage/B62UC52D/Song et al. - 2024 - Multi-Agents are Social Groups Investigating Social Influence of Multiple Agents in Human-Agent Int.pdf}
}

@online{soto-sanfielScaleArtificialIntelligence2024,
  title = {The {{Scale}} of {{Artificial Intelligence Literacy}} for All ({{SAIL4ALL}}): {{A Tool}} for {{Assessing Knowledge}} on {{Artificial Intelligence}} in {{All Adult Populations}} and {{Settings}}},
  shorttitle = {The {{Scale}} of {{Artificial Intelligence Literacy}} for All ({{SAIL4ALL}})},
  author = {Soto-Sanfiel, María T. and Angulo-Brunet, Ariadna and Lutz, Christoph},
  date = {2024-04-30},
  eprinttype = {OSF},
  doi = {10.31235/osf.io/bvyku},
  url = {https://osf.io/bvyku_v1},
  urldate = {2025-05-02},
  abstract = {This study provides evidence of the psychometric quality of a new artificial intelligence (AI) literacy scale for comprehensive assessment of the concept across adult populations, regardless of the setting in which it is applied: the SAIL4ALL. It contains 56 items distributed across four themes [(1) What is AI? (a: Recognizing AI, Understanding Intelligence and Interdisciplinarity; b: General vs. Narrow); (2) What can AI do?; (3) How does AI work?; and (4) How should AI be used?], which can be used in combination or independently. Moreover, the scale is validated in two different response formats (true/false and 5-point Likert scale), each of which is applied depending on the context. The version with a true/false format is ideal for when AI literacy levels need to be assessed quickly because of its simplicity and ease of interpretation. Conversely, the 5-point Likert scale yields more nuanced responses based on the degree of confidence, providing richer insights into the respondents’ perceptions of their AI literacy. SAIL4ALL has demonstrated positive evidence of psychometric quality, and serves as a valuable tool for determining both actual and perceived knowledge of AI, thus guiding educational, organizational, and institutional AI literacy initiatives.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {AI ethics,AI literacy,artificial intelligence,education,literacy,machine learning,scale development},
  annotation = {https://osf.io/preprints/socarxiv/bvyku\_v1},
  file = {/Users/thomasgorman/Zotero/storage/Q5CN35PA/Soto-Sanfiel et al. - 2024 - The Scale of Artificial Intelligence Literacy for all (SAIL4ALL) A Tool for Assessing Knowledge on.pdf}
}

@incollection{starkePsychologicallyInformedDesign2024,
  title = {Psychologically {{Informed Design}} of {{Energy Recommender Systems}}: {{Are Nudges Still Effective}} in {{Tailored Choice Environments}}?},
  shorttitle = {Psychologically {{Informed Design}} of {{Energy Recommender Systems}}},
  booktitle = {A {{Human-Centered Perspective}} of {{Intelligent Personalized Environments}} and {{Systems}}},
  author = {Starke, Alain D. and Willemsen, Martijn C.},
  editor = {Ferwerda, Bruce and Graus, Mark and Germanakos, Panagiotis and Tkalčič, Marko},
  date = {2024},
  pages = {221--259},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-55109-3_9},
  url = {https://link.springer.com/10.1007/978-3-031-55109-3_9},
  urldate = {2025-05-05},
  isbn = {978-3-031-55108-6 978-3-031-55109-3},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/RN6774H7/Starke and Willemsen - 2024 - Psychologically Informed Design of Energy Recommender Systems Are Nudges Still Effective in Tailore.pdf}
}

@thesis{starkeSupportingEnergyefficientChoices2019,
  type = {Phd Thesis 1 (Research TU/e / Graduation TU/e)},
  title = {Supporting Energy-Efficient Choices Using {{Rasch-based}} Recommender Interfaces},
  author = {Starke, A.D.},
  date = {2019-02-26},
  institution = {Technische Universiteit Eindhoven},
  location = {Eindhoven},
  url = {https://pure.tue.nl/ws/portalfiles/portal/119737279/20190226_CO_Starke.pdf},
  abstract = {‘Netflix for energy’: tailored home energy-saving recommendations},
  annotation = {https://osf.io/jz4wt/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/X7DUT52K/Starke - 2019 - Supporting energy-efficient choices using Rasch-based recommender interfaces.pdf}
}

@inproceedings{starkeUsingExplanationsEnergySaving2021,
  title = {Using {{Explanations}} as {{Energy-Saving Frames}}: {{A User-Centric Recommender Study}}},
  shorttitle = {Using {{Explanations}} as {{Energy-Saving Frames}}},
  booktitle = {Adjunct {{Proceedings}} of the 29th {{ACM Conference}} on {{User Modeling}}, {{Adaptation}} and {{Personalization}}},
  author = {Starke, Alain D. and Willemsen, Martijn C. and Snijders, Chris},
  date = {2021-06-21},
  pages = {229--237},
  publisher = {ACM},
  location = {Utrecht Netherlands},
  doi = {10.1145/3450614.3464477},
  url = {https://dl.acm.org/doi/10.1145/3450614.3464477},
  urldate = {2025-05-05},
  eventtitle = {{{UMAP}} '21: 29th {{ACM Conference}} on {{User Modeling}}, {{Adaptation}} and {{Personalization}}},
  isbn = {978-1-4503-8367-7},
  langid = {english},
  annotation = {https://osf.io/jz4wt/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/SLP752W7/Starke et al. - 2021 - Using Explanations as Energy-Saving Frames A User-Centric Recommender Study.pdf}
}

@article{steinAttitudesAIMeasurement2024,
  title = {Attitudes towards {{AI}}: Measurement and Associations with Personality},
  shorttitle = {Attitudes towards {{AI}}},
  author = {Stein, Jan-Philipp and Messingschlager, Tanja and Gnambs, Timo and Hutmacher, Fabian and Appel, Markus},
  date = {2024-02-05},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {14},
  number = {1},
  pages = {2909},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-53335-2},
  url = {https://www.nature.com/articles/s41598-024-53335-2},
  urldate = {2024-11-15},
  abstract = {Artificial intelligence (AI) has become an integral part of many contemporary technologies, such as social media platforms, smart devices, and global logistics systems. At the same time, research on the public acceptance of AI shows that many people feel quite apprehensive about the potential of such technologies—an observation that has been connected to both demographic and sociocultural user variables (e.g., age, previous media exposure). Yet, due to divergent and often ad-hoc measurements of AI-related attitudes, the current body of evidence remains inconclusive. Likewise, it is still unclear if attitudes towards AI are also affected by users’ personality traits. In response to these research gaps, we offer a two-fold contribution. First, we present a novel, psychologically informed questionnaire (ATTARI-12) that captures attitudes towards AI as a single construct, independent of specific contexts or applications. Having observed good reliability and validity for our new measure across two studies (N1\,=\,490; N2\,=\,150), we examine several personality traits—the Big Five, the Dark Triad, and conspiracy mentality—as potential predictors of AI-related attitudes in a third study (N3\,=\,298). We find that agreeableness and younger age predict a more positive view towards artificially intelligent technology, whereas the susceptibility to conspiracy beliefs connects to a more negative attitude. Our findings are discussed considering potential limitations and future directions for research and practice.},
  langid = {english},
  keywords = {Human behaviour,Information technology,Psychology},
  annotation = {https://osf.io/3j67a/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/KJGQ95YM/Stein et al. - 2024 - Attitudes towards AI measurement and associations with personality.pdf}
}

@online{steyversMetacognitionUncertaintyCommunication2025,
  title = {Metacognition and {{Uncertainty Communication}} in {{Humans}} and {{Large Language Models}}},
  author = {Steyvers, Mark and Peters, Megan A. K.},
  date = {2025-04-18},
  eprint = {2504.14045},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2504.14045},
  url = {http://arxiv.org/abs/2504.14045},
  urldate = {2025-04-26},
  abstract = {Metacognition, the capacity to monitor and evaluate one's own knowledge and performance, is foundational to human decision-making, learning, and communication. As large language models (LLMs) become increasingly embedded in high-stakes decision contexts, it is critical to assess whether, how, and to what extent they exhibit metacognitive abilities. Here, we provide an overview of current knowledge of LLMs' metacognitive capacities, how they might be studied, and how they relate to our knowledge of metacognition in humans. We show that while humans and LLMs can sometimes appear quite aligned in their metacognitive capacities and behaviors, it is clear many differences remain. Attending to these differences is crucial not only for enhancing human-AI collaboration, but also for promoting the development of more capable and trustworthy artificial systems. Finally, we discuss how endowing future LLMs with more sensitive and more calibrated metacognition may also help them develop new capacities such as more efficient learning, self-direction, and curiosity.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/RX4FJCEK/Steyvers and Peters - 2025 - Metacognition and Uncertainty Communication in Humans and Large Language Models.pdf;/Users/thomasgorman/Zotero/storage/K6N4M2HW/2504.html}
}

@article{steyversWhatLargeLanguage2025,
  title = {What Large Language Models Know and What People Think They Know},
  author = {Steyvers, Mark and Tejeda, Heliodoro and Kumar, Aakriti and Belem, Catarina and Karny, Sheer and Hu, Xinyue and Mayer, Lukas and Smyth, Padhraic},
  date = {2025},
  journaltitle = {Nature Machine Intelligence},
  pages = {1--11},
  doi = {10.1038/s42256-024-00976-7},
  abstract = {As AI systems, particularly large language models (LLMs), become increasingly integrated into decision-making processes, the ability to trust their outputs is crucial. To earn human trust, LLMs must be well-calibrated such that they can accurately assess and communicate the likelihood of their predictions being correct. While recent work has focused on LLMs’ internal confidence, less is understood about how effectively they convey uncertainty to users. This study explores the “calibration gap” which refers to the difference between human confidence in LLM-generated answers and the models’ actual confidence, and the “discrimination gap” which reflects how well humans and models can distinguish between correct and incorrect answers. Our experiments with multiple-choice and short-answer questions reveal that users tend to overestimate the accuracy of LLM responses when provided with default explanations. Additionally, longer explanations increased user confidence, even when the extra length did not improve answer accuracy. By adjusting LLM explanations to better reflect the models’ internal confidence, both the calibration and discrimination gaps narrowed, significantly improving user perception of LLM accuracy. These findings underscore the importance of accurate uncertainty communication and highlight the effect of explanation length in influencing user trust in AI-assisted decision-making environments.},
  langid = {english},
  keywords = {matlab code},
  annotation = {https://osf.io/y7pr6/\\
\\
https://madlabatuci.github.io/working-with-chatGPT/\\
\\
https://osf.io/y7pr6/files/osfstorage\\
\\
https://static-content.springer.com/esm/art\%3A10.1038\%2Fs42256-024-00976-7/MediaObjects/42256\_2024\_976\_MOESM1\_ESM.pdf},
  file = {/Users/thomasgorman/Zotero/storage/DQSYRXZW/Steyvers et al. - 2025 - What large language models know and what people think they know.pdf}
}

@thesis{sunDelegationVirtualAgents2023,
  title = {Delegation to {{Virtual Agents}} in {{Critical Scenarios}}: {{Influencing Factors}} and {{Immersive Settings}}},
  shorttitle = {Delegation to {{Virtual Agents}} in {{Critical Scenarios}}},
  author = {Sun, Ningyuan},
  date = {2023-11-14},
  institution = {University of Luxembourg},
  url = {https://orbilu.uni.lu/handle/10993/58847},
  urldate = {2025-05-03},
  abstract = {Favored by the rapid advance of technologies such as artificial intelligence and computer graphics, virtual agents have been increasingly accessible, capable, and autonomous over the past decades. As a result of their growing technological prowess, interaction with virtual agents has been gradually evolving from a traditional user-tool relationship to one resembling interpersonal delegation, where users empower virtual agents to autonomously carry out specific tasks on their behalf. Forming a delegatory relationship with virtual agents can facilitate the user-agent interaction in numerous aspects, particularly regarding convenience and efficiency. Yet, it also comes with problems and challenges that may harm users drastically in critical scenarios and thus deserves extensive research. This thesis presents a thorough discussion of delegation to virtual agents based on a series of studies my colleagues and I conducted over the past four years. Several factors --including agent representation, theory of mind, rapport, and technological immersion-- are examined individually via empirical approaches to reveal their impacts on delegation to virtual agents. A conceptual model featuring three interrelated dimensions is proposed, constituting a theoretical framework to integrate the empirical findings. An overall evaluation of these works indicates that users' decisions on delegating critical tasks to virtual agents are mainly based on rational thinking. Performance-related factors have a significant impact on delegation, whereas affective cues --such as rapport, agent representation, and theory of mind-- are influential only to a limited extent. Furthermore, the usage of immersive media devices (e.g., head-mounted displays) has a marginal effect on users' delegatory decisions. Thus, it is advisable for developers to focus on performance-related aspects when designing virtual agents for critical tasks.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/VU2W5R3X/Sun - 2023 - Delegation to Virtual Agents in Critical Scenarios Influencing Factors and Immersive Settings.pdf}
}

@article{tomsettRapidTrustCalibration2020,
  title = {Rapid {{Trust Calibration}} through {{Interpretable}} and {{Uncertainty-Aware AI}}},
  author = {Tomsett, Richard and Preece, Alun and Braines, Dave and Cerutti, Federico and Chakraborty, Supriyo and Srivastava, Mani and Pearson, Gavin and Kaplan, Lance},
  date = {2020-07-10},
  journaltitle = {Patterns},
  shortjournal = {Patterns (N Y)},
  volume = {1},
  number = {4},
  eprint = {33205113},
  eprinttype = {pubmed},
  pages = {100049},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2020.100049},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7660448/},
  urldate = {2025-05-08},
  abstract = {Artificial intelligence (AI) systems hold great promise as decision-support tools, but we must be able to identify and understand their inevitable mistakes if they are to fulfill this potential. This is particularly true in domains where the decisions are high-stakes, such as law, medicine, and the military. In this Perspective, we describe the particular challenges for AI decision support posed in military coalition operations. These include having to deal with limited, low-quality data, which inevitably compromises AI performance. We suggest that these problems can be mitigated by taking steps that allow rapid trust calibration so that decision makers understand the AI system's limitations and likely failures and can calibrate their trust in its outputs appropriately. We propose that AI services can achieve this by being both interpretable and uncertainty-aware. Creating such AI systems poses various technical and human factors challenges. We review these challenges and recommend directions for future research., This article is about artificial intelligence (AI) used to inform high-stakes decisions, such as those arising in legal, healthcare, or military contexts. Users must have an understanding of the capabilities and limitations of an AI system when making high-stakes decisions. Usually this requires the user to interact with the system and learn over time how it behaves in different circumstances. We propose that long-term interaction would not be necessary for an AI system with the properties of interpretability and uncertainty awareness. Interpretability makes clear what the system “knows” while uncertainty awareness reveals what the system does not “know.” This allows the user to rapidly calibrate their trust in the system's outputs, spotting flaws in its reasoning or seeing when it is unsure. We illustrate these concepts in the context of a military coalition operation, where decision makers may be using AI systems with which they are unfamiliar and which are operating in rapidly changing environments. We review current research in these areas, considering both technical and human factors challenges, and propose a framework for future work based on Lasswell's communication model., We introduce the concept of rapid trust calibration for AI decision support, and propose how this can be achieved by building AI systems that are both interpretable and uncertainty-aware. We provide a literature review of these research areas and describe a military scenario illustrating the relevant concepts. We propose a framework inspired by Lasswell's communication model to structure future work in this area.},
  pmcid = {PMC7660448}
}

@article{tullyLowerArtificialIntelligence2025,
  title = {Lower {{Artificial Intelligence Literacy Predicts Greater AI Receptivity}}},
  author = {Tully, Stephanie and Longoni, Chiara and Appel, Gil},
  date = {2025},
  journaltitle = {Journal of Marketing},
  doi = {10.1177/00222429251314491},
  url = {https://osf.io/t9u8g_v1},
  urldate = {2025-04-30},
  abstract = {As artificial intelligence (AI) transforms society, understanding factors that influence AI receptivity is increasingly important. The current research investigates which types of consumers have greater AI receptivity. Contrary to expectations revealed in four surveys, cross country data and six additional studies find that people with lower AI literacy are typically more receptive to AI. This lower literacy-greater receptivity link is not explained by differences in perceptions of AI’s capability, ethicality, or feared impact on humanity. Instead, this link occurs because people with lower AI literacy are more likely to perceive AI as magical and experience feelings of awe in the face of AI’s execution of tasks that seem to require uniquely human attributes. In line with this theorizing, the lower literacy-higher receptivity link is mediated by perceptions of AI as magical and is moderated among tasks not assumed to require distinctly human attributes. These findings suggest that companies may benefit from shifting their marketing efforts and product development towards consumers with lower AI literacy. Additionally, efforts to demystify AI may inadvertently reduce its appeal, indicating that maintaining an aura of magic around AI could be beneficial for adoption.},
  langid = {american},
  keywords = {AI adoption,AI knowledge,algorithms,artificial intelligence,human-computer interaction,literacy,technology},
  file = {/Users/thomasgorman/Zotero/storage/7XEX6F8K/Tully et al. - 2023 - Lower Artificial Intelligence Literacy Predicts Greater AI Receptivity.pdf}
}

@article{vandenbroekHeuristicsEnergyJudgement2019,
  title = {Heuristics in Energy Judgement Tasks},
  author = {Van Den Broek, Karlijn L. and Walker, Ian},
  date = {2019-04},
  journaltitle = {Journal of Environmental Psychology},
  shortjournal = {Journal of Environmental Psychology},
  volume = {62},
  pages = {95--104},
  issn = {02724944},
  doi = {10.1016/j.jenvp.2019.02.008},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0272494418303839},
  urldate = {2024-12-11},
  abstract = {To save energy effectively, householders need to be aware of the energy consumption in their homes, in particular the energy use of their household appliances. People's perception of the energy use of their appliances has been found to be influenced by the use of heuristics (simple rules for making quick decisions), yet these heuristics have received little research attention. Three studies investigated the use of these energy judgement heuristics using mixed methods. Findings show that 1) participants used as many as twenty-four different heuristics in an energy judgement task – an order of magnitude more than identified in existing literature; 2) participants are aware they use the heuristics, but awareness varies per heuristic; 3) the use of these heuristics can be changed and this in turn can improve energy literacy. These studies demonstrate for the first time that the energy judgement process is much more complex than previously thought and provides a promising starting point for future research to uncover opportunities to improve energy literacy.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/UFLRKFVD/Van Den Broek and Walker - 2019 - Heuristics in energy judgement tasks.pdf}
}

@incollection{vargheseEnergyLiteracyScale2023,
  title = {Energy {{Literacy Scale}} ({{ELS}}): {{Validated Survey Instrument}} to {{Measure Energy Knowledge}}, {{Attitude}}, and {{Behaviour}}},
  shorttitle = {Energy {{Literacy Scale}} ({{ELS}})},
  booktitle = {The 9th {{International Conference}} on {{Energy}} and {{Environment Research}}},
  author = {Varghese, Annie Feba and Chandrasenan, Divya},
  date = {2023},
  pages = {793--800},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-43559-1_75},
  url = {https://link.springer.com/10.1007/978-3-031-43559-1_75},
  urldate = {2025-05-02},
  abstract = {The Energy Literacy Scale (ELS) was created to assess students’ energy-related knowledge and awareness of the implications of energy production and consumption, everyday energy use, and the adoption of energy-saving behaviors. The energy literacy scale was drafted and pilot tested among elementary school students across Kerala, India. Initial exploration of the measure yielded promising results: Cronbach's reliability coefficients for cognitive, emotional, and behavioral subscales varied from 0.68 to 0.78, while average discrimination indices ranged from 0.28 to 0.43. Factor Analysis was used to select appropriate questions for the Energy Literacy Scale with due importance being given to each of the Knowledge, Attitudinal and Behavioral domains. The field-tested ELS includes three knowledge factors namely (1) Energy sources, Efficiency and Conservation, (2) Energy Use and Implications and (3) Basic Energy Concepts. Three behaviour and two attitude dimension sub-scales are included in the accepted instrument. The ELS is particularly useful for determining the baseline energy literacy skills of potential responders and evaluating the broader effects of educational initiatives.},
  isbn = {978-3-031-43558-4 978-3-031-43559-1},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/V2ZGESI4/Varghese and Chandrasenan - 2023 - Energy Literacy Scale (ELS) Validated Survey Instrument to Measure Energy Knowledge, Attitude, and.pdf}
}

@article{weberObjectiveMeasurementAI2023,
  title = {Toward an {{Objective Measurement}} of {{AI Literacy}}},
  author = {Weber, Patrick and Pinski, Marc and Baum, Lorenz},
  date = {2023},
  url = {https://aisel.aisnet.org/pacis2023/60/},
  abstract = {Humans multitudinously interact with Artificial Intelligence (AI) as it permeates every aspect of contemporary professional and private life. The socio-technical competencies of humans, i.e., their AI literacy, shape human-AI interactions. While academia does explore AI literacy measurement, current literature exclusively approaches the topic from a subjective perspective. This study draws on a well-established scale development procedure employing ten expert interviews, two card-sorting rounds, and a betweensubject comparison study with 88 participants in two groups to define, conceptualize, and empirically validate an objective measurement instrument for AI literacy. With 16 items, our developed instrument discriminates between an AI-literate test and a control group. Furthermore, the structure of our instrument allows us to distinctly assess AI literacy aspects. We contribute to IS education research by providing a new instrument and conceptualizing AI literacy, incorporating critical themes from the literature. Practitioners may employ our instrument to assess AI literacy in their organizations.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/PY8GMC47/Weber et al. - 2023 - Toward an Objective Measurement of AI Literacy.pdf}
}

@online{wischnewskiDevelopmentValidationTrust2025,
  title = {Development and Validation of the {{Trust}} in {{AI Scale}} ({{TAIS}})},
  author = {Wischnewski, Magdalena and Doebler, Philipp and Krämer, Nicole},
  date = {2025-02-26},
  eprinttype = {OSF},
  doi = {10.31234/osf.io/eqa9y_v1},
  url = {https://osf.io/eqa9y_v1},
  urldate = {2025-05-03},
  abstract = {In everyday life, users increasingly interact and communicate with AI systems. Despite the importance of trust in AI as an influencing factor for this interaction, there is a shortage of validated scales to reliably measure users’ trust. In this paper, we present a theory-driven development and validation of the Trust in AI scale (TAIS) that consists of the subdimensions ability, integrity, transparency, unbiasedness, vigilance, and global trust. To validate the scale, we conducted two studies. In study 1 (N = 883 participants), we derived 57 items from theory and existing scales for which an exploratory factor analysis resulted in a 30 item scale. In study 2 (N = 1204 participants), we tested the psychometric quality of the scale through confirmatory factor analysis for ordinal data. Employing a bifactor model with global trust as the higher-order factor, our results confirm the six-factor structure. Correlational results of context variables and related scales support the convergent validity of the scale. Results show that existing scales rather correlate with the global trust factor but less with specific factors (especially vigilance) - indicating that the TAIS scale helps to uncover new facets of trust and thereby goes beyond what existing, less validated scales can provide.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {artificial intelligence,bifactor model,calibrated trust,scale development},
  annotation = {https://osf.io/9zp6y/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/BBE87LYI/Wischnewski et al. - 2025 - Development and validation of the Trust in AI Scale (TAIS).pdf}
}

@article{wojtonInitialValidationTrust2020,
  title = {Initial Validation of the Trust of Automated Systems Test ({{TOAST}})},
  author = {Wojton, Heather M. and Porter, Daniel and T Lane, Stephanie and Bieber, Chad and Madhavan, Poornima},
  date = {2020-11-01},
  journaltitle = {The Journal of Social Psychology},
  shortjournal = {J Soc Psychol},
  volume = {160},
  number = {6},
  pages = {735--750},
  issn = {1940-1183},
  doi = {10.1080/00224545.2020.1749020},
  abstract = {Trust is a key determinant of whether people rely on automated systems in the military and the public. However, there is currently no standard for measuring trust in automated systems. In the present studies, we propose a scale to measure trust in automated systems that is grounded in current research and theory on trust formation, which we refer to as the Trust in Automated Systems Test (TOAST). We evaluated both the reliability of the scale structure and criterion validity using independent, military-affiliated and civilian samples. In both studies we found that the TOAST exhibited a two-factor structure, measuring system understanding and performance (respectively), and that factor scores significantly predicted scores on theoretically related constructs demonstrating clear criterion validity. We discuss the implications of our findings for advancing the empirical literature and in improving interface design.},
  langid = {english},
  keywords = {Adult,Automation,Confirmatory Factor Analysis,Female,Human-Machine Trust,Humans,Male,Man-Machine Systems,Military Personnel,Reproducibility of Results,Surveys and Questionnaires,Trust,Trust in Automation,Trust Scale},
  annotation = {https://testscience.org/wp-content/uploads/formidable/20/TOAST.pdf\\
\\
https://osf.io/swkny/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/DDT344YB/Wojton et al. - 2020 - Initial validation of the trust of automated systems test (TOAST).pdf}
}

@article{xuConfrontingVerbalizedUncertainty2025,
  title = {Confronting Verbalized Uncertainty: {{Understanding}} How {{LLM}}’s Verbalized Uncertainty Influences Users in {{AI-assisted}} Decision-Making},
  shorttitle = {Confronting Verbalized Uncertainty},
  author = {Xu, Zhengtao and Song, Tianqi and Lee, Yi-Chieh},
  date = {2025-03-01},
  journaltitle = {International Journal of Human-Computer Studies},
  shortjournal = {International Journal of Human-Computer Studies},
  volume = {197},
  pages = {103455},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2025.103455},
  url = {https://www.sciencedirect.com/science/article/pii/S1071581925000126},
  urldate = {2025-02-25},
  abstract = {Due to the human-like nature, large language models (LLMs) often express uncertainty in their outputs. This expression, known as ”verbalized uncertainty”, can appear in phrases such as ”I’m sure that [...]” or ”It could be [...]”. However, few studies have explored how this expression impacts human users’ feelings towards AI, including their trust, satisfaction and task performance. Our research aims to fill this gap by exploring how different levels of verbalized uncertainty from the LLM’s outputs affect users’ perceptions and behaviors in AI-assisted decision-making scenarios. To this end, we conducted a between-condition study (N = 156), dividing participants into six groups based on two accuracy conditions and three conditions of verbalized uncertainty. We also used the widely played word guessing game Codenames to simulate the role of LLMs in assisting human decision-making. Our results show that medium verbalized uncertainty in the LLM’s expressions consistently leads to higher user trust, satisfaction, and task performance compared to high and low verbalized uncertainty. Our results also show that participants experience verbalized uncertainty differently based on the accuracy of the LLM. This study offers important implications for the future design of LLMs, suggesting adaptive strategies to express verbalized uncertainty based on the LLM’s accuracy.},
  keywords = {AI-assisted decision-making,Large language model,Performance,Satisfaction,Trust,Verbalized uncertainty},
  file = {/Users/thomasgorman/Zotero/storage/UN8UX4GI/Xu et al. - 2025 - Confronting verbalized uncertainty Understanding how LLM’s verbalized uncertainty influences users.pdf;/Users/thomasgorman/Zotero/storage/92GIBE8W/S1071581925000126.html}
}

@online{xuSaySelfTeachingLLMs2024,
  title = {{{SaySelf}}: {{Teaching LLMs}} to {{Express Confidence}} with {{Self-Reflective Rationales}}},
  shorttitle = {{{SaySelf}}},
  author = {Xu, Tianyang and Wu, Shujin and Diao, Shizhe and Liu, Xiaoze and Wang, Xingyao and Chen, Yangyi and Gao, Jing},
  date = {2024-10-04},
  eprint = {2405.20974},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2405.20974},
  url = {http://arxiv.org/abs/2405.20974},
  urldate = {2025-03-24},
  abstract = {Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://github.com/tianyang-x/SaySelf\\
\\
https://www.youtube.com/watch?v=9dBYBnE8rZk\&ab\_channel=PaperWithVideo},
  file = {/Users/thomasgorman/Zotero/storage/G9ACZB6D/Xu et al. - 2024 - SaySelf Teaching LLMs to Express Confidence with Self-Reflective Rationales.pdf;/Users/thomasgorman/Zotero/storage/P7DN9WLF/2405.html}
}

@article{zhaiEffectsOverrelianceAI2024,
  title = {The Effects of Over-Reliance on {{AI}} Dialogue Systems on Students' Cognitive Abilities: A Systematic Review},
  shorttitle = {The Effects of Over-Reliance on {{AI}} Dialogue Systems on Students' Cognitive Abilities},
  author = {Zhai, Chunpeng and Wibowo, Santoso and Li, Lily D.},
  date = {2024-06-18},
  journaltitle = {Smart Learning Environments},
  shortjournal = {Smart Learning Environments},
  volume = {11},
  number = {1},
  pages = {28},
  issn = {2196-7091},
  doi = {10.1186/s40561-024-00316-7},
  url = {https://doi.org/10.1186/s40561-024-00316-7},
  urldate = {2025-05-02},
  abstract = {The growing integration of artificial intelligence (AI) dialogue systems within educational and research settings highlights the importance of learning aids. Despite examination of the ethical concerns associated with these technologies, there is a noticeable gap in investigations on how these ethical issues of AI contribute to students’ over-reliance on AI dialogue systems, and how such over-reliance affects students’ cognitive abilities. Overreliance on AI occurs when users accept AI-generated recommendations without question, leading to errors in task performance in the context of decision-making. This typically arises when individuals struggle to assess the reliability of AI or how much trust to place in its suggestions. This systematic review investigates how students’ over-reliance on AI dialogue systems, particularly those embedded with generative models for academic research and learning, affects their critical cognitive capabilities including decision-making, critical thinking, and analytical reasoning. By using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, our systematic review evaluated a body of literature addressing the contributing factors and effects of such over-reliance within educational and research contexts. The comprehensive literature review spanned 14 articles retrieved from four distinguished databases: ProQuest, IEEE Xplore, ScienceDirect, and Web of Science. Our findings indicate that over-reliance stemming from ethical issues of AI impacts cognitive abilities, as individuals increasingly favor fast and optimal solutions over slow ones constrained by practicality. This tendency explains why users prefer efficient cognitive shortcuts, or heuristics, even amidst the ethical issues presented by AI technologies.},
  keywords = {Analytical thinking,Cognitive abilities,Critical thinking,Decision-making,Ethical issues of AI,Generative AI},
  file = {/Users/thomasgorman/Zotero/storage/C6I7AWZ3/Zhai et al. - 2024 - The effects of over-reliance on AI dialogue systems on students' cognitive abilities a systematic r.pdf;/Users/thomasgorman/Zotero/storage/2VTDXKQW/s40561-024-00316-7.html}
}

@article{zhangArtificialIntelligenceAmerican2019,
  title = {Artificial {{Intelligence}}: {{American Attitudes}} and {{Trends}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Zhang, Baobao and Dafoe, Allan},
  date = {2019},
  journaltitle = {SSRN Electronic Journal},
  shortjournal = {SSRN Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3312874},
  url = {https://www.ssrn.com/abstract=3312874},
  urldate = {2025-05-02},
  abstract = {This report presents a broad look at the American public’s attitudes toward artificial intelligence (AI) and AI governance, based on findings from a nationally representative survey of 2,000 American adults. As the study of the public opinion toward AI is relatively new, we aimed for breadth over depth, with our questions touching on: workplace automation; attitudes regarding international cooperation; the public’s trust in various actors to develop and regulate AI; views about the importance and likely impact of different AI governance challenges; and historical and cross-national trends in public opinion regarding AI. Our results provide preliminary insights into the character of US public opinion regarding AI.},
  langid = {english},
  annotation = {https://isps.yale.edu/sites/default/files/files/Zhang\_us\_public\_opinion\_report\_jan\_2019.pdf},
  file = {/Users/thomasgorman/Zotero/storage/FE6REEW5/Zhang and Dafoe - 2019 - Artificial Intelligence American Attitudes and Trends.pdf}
}
