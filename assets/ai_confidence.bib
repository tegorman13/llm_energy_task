@article{adamsTrustAutomatedSystems2003,
  title = {Trust in Automated Systems},
  author = {Adams, Barb and Bruyn, L.E. and Houde, S. and Angelopoulos, P. and {Iwasa-Madge}, K. and McCann, C.},
  year = {2003},
  journal = {Ministry of National Defence},
  pages = {3--7},
  abstract = {This report reviews research literature pertaining to trust in automated systems. Based on the review, we argue that trust in automation has many similarities with trust in the interpersonal domain, but also several unique dynamics and influences. Existing research has focused primarily on trust in automation that has an executive or control function, and to a lesser extent, has considered trust in automation that is designed to present information to operators (e.g. decision aids). We maintain that although there are many similarities between trust in automation and interpersonal trust, the dynamics of trust in automation also have some distinct qualities. Several models related to trust in automation have already been developed; in this report, a comprehensive -- although still preliminary -model of trust in military automation is proposed. Several sets of factors are likely to impact on the development of trust in automation, including properties of the automation, properties of the operator, and properties of the context in which interaction with automation occurs. The consequences of trust in automation have yet to be fully explored. Based on this review, measures and methods to study trust in automation are considered, and a program of research to study trust in automated systems is described.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/QNTXHDX2/Adams - TRUST IN AUTOMATED SYSTEMS.pdf}
}

@misc{ahdritzDistinguishingKnowableUnknowable2024,
  title = {Distinguishing the {{Knowable}} from the {{Unknowable}} with {{Language Models}}},
  author = {Ahdritz, Gustaf and Qin, Tian and Vyas, Nikhil and Barak, Boaz and Edelman, Benjamin L.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.03563},
  eprint = {2402.03563},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.03563},
  urldate = {2025-05-12},
  abstract = {We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more informative indicators of model confidence in diverse practical settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Zotero/storage/9NX6K2YH/Ahdritz et al. - 2024 - Distinguishing the Knowable from the Unknowable with Language Models.pdf;/Users/thomasgorman/Zotero/storage/QQHXI43A/2402.html}
}

@article{anayatAugmentAutomateImpact2025,
  title = {To Augment or to Automate: Impact of Anthropomorphism on Users' Choice of Decision Delegation to {{AI-powered}} Agents},
  shorttitle = {To Augment or to Automate},
  author = {Anayat, Shaista and Kaushik, Arun},
  year = {2025},
  month = apr,
  journal = {Behaviour \& Information Technology},
  pages = {1--19},
  issn = {0144-929X, 1362-3001},
  doi = {10.1080/0144929X.2025.2497441},
  urldate = {2025-05-07},
  abstract = {Understanding how anthropomorphic design influences user decision-making is critical as anthropomorphic technologies become more integrated into daily life and work environments. The existing literature on AI-anthropomorphism mainly focuses on physical or voice-based anthropomorphism and its impact on adoption. However, it ignores mind-based (cognitive and affective) anthropomorphism and its impact on users' decision delegation (automated or augmented) to AI-systems. This study examines how mind-based anthropomorphism influences users' decisions to delegate tasks to AI-powered agents, using a survey of 243 actual AI agent users and PLS-SEM for data analysis. Results revealed that users' choice for augmented decision delegation to AI-powered agents is significantly increased by cognitive anthropomorphism. Also, users' choice for automated decision delegation to AI-powered agents is positively influenced by affective anthropomorphism. Trust emerged as a crucial mediator between anthropomorphism and automated decision delegation, encouraging users to hand over the decision-making completely to AI-powered agents. Results also revealed decreased reliance on trust when users prefer to retain a certain level of control in the decision-making process. The study significantly contributes to the literature on human-AI interaction by enhancing the understanding of users' psychology towards anthropomorphism and their choice for decision delegation to AI-powered agents.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/BP7D35I7/Anayat and Kaushik - 2025 - To augment or to automate impact of anthropomorphism on users’ choice of decision delegation to AI-.pdf}
}

@article{atanasovImprovingLowProbabilityJudgments2024,
  title = {Improving {{Low-Probability Judgments}}},
  author = {Atanasov, Pavel and Consigny, Coralie and Karger, Ezra and Schoenegger, Philipp and Budescu, David and Tetlock, Philip E},
  year = {2024},
  abstract = {High-stakes debates often pivot on clashing estimates of outcomes that one side sees as so improbable as not to deserve policy prioritization. These debates are especially intractable when they focus on rare events ranging from disasters (e.g., existential risks from Artificial Intelligence, nuclear war, or bioengineered pandemics) to surprising successes (e.g., once inconceivable scientific discoveries). The research literature offers grounds for suspecting that the micro-probability judgments flowing into such debates are both unreliable and biased. This article covers experimental manipulations that achieve improvements in accuracy for low-probability judgments by shifting from the standard linear elicitation scale and Brier scoring rule to nonlinear (logarithmic) elicitation scales and logarithmic scoring rules. These methodological changes produced accuracy improvements of approximately d = 0.2 to 0.5 for individual accuracy scores. Improvements in aggregate accuracy varied more widely by aggregation function (mean vs. median) and accuracy scoring rule, between parity (d = 0) and a large advantage for non-linear over linear scales (d = 0.68). Judgments obtained via the linear scale and text box elicitations systematically overestimated the true values. New scales allowed forecasters to provide precise judgments at the low end of the probability scale and logarithmic scoring rules penalize large errors harshly, incentivising judges to avoid 0\%and provide precise non-zero probabilities. An indirect elicitation protocol we developed, successive menus, yielded mixed results, such as improving aggregate accuracy and individual calibration at the cost of increasing outlier judgments and reducing retention. Base rate anchors provided context but no measurable accuracy benefits. These results point to next steps for improving probability judgments of rare events. The most promising next steps include a) using subject-specific Base-Rate Anchors, b) developing training programs specific to low-probability events, c) developing more robust and usable indirect elicitation protocols, and d) assessing all of these methods in longitudinal forecasting tournament featuring many forecasting questions focused on rare events.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/EHQBDFBE/Atanasov et al. - Improving Low-Probability Judgments.pdf}
}

@misc{atanasovImprovingLowProbabilityJudgments2025,
  title = {Improving {{Low-Probability Judgments}}},
  author = {Atanasov, Pavel D. and Consigny, Coralie and Karger, Ezra and Schoenegger, Philipp and Budescu, David V. and Tetlock, Philip},
  year = {2025},
  publisher = {SSRN},
  doi = {10.2139/ssrn.5025990},
  urldate = {2025-05-12},
  abstract = {High-stakes debates often pivot on clashing estimates of outcomes that one side sees as so improbable as not to deserve policy prioritization. These debates are especially intractable when they focus on rare events ranging from disasters (e.g., existential risks from Artificial Intelligence, nuclear war, or bioengineered pandemics) to surprising successes (e.g., once inconceivable scientific discoveries). The research literature offers grounds for suspecting that the micro-probability judgments flowing into such debates are both unreliable and biased. This article covers experimental manipulations that achieve improvements in accuracy for low-probability judgments by shifting from the standard linear elicitation scale and Brier scoring rule to nonlinear (logarithmic) elicitation scales and logarithmic scoring rules. These methodological changes produced accuracy improvements of approximately d = 0.2 to 0.5 for individual accuracy scores. Improvements in aggregate accuracy varied more widely by aggregation function (mean vs. median) and accuracy scoring rule, between parity (d = 0) and a large advantage for non-linear over linear scales (d = 0.68). Judgments obtained via the linear scale and text box elicitations systematically overestimated the true values. New scales allowed forecasters to provide precise judgments at the low end of the probability scale and logarithmic scoring rules penalize large errors harshly, incentivising judges to avoid 0\%and provide precise non-zero probabilities. An indirect elicitation protocol we developed, successive menus, yielded mixed results, such as improving aggregate accuracy and individual calibration at the cost of increasing outlier judgments and reducing retention. Base rate anchors provided context but no measurable accuracy benefits. These results point to next steps for improving probability judgments of rare events. The most promising next steps include a) using subject-specific Base-Rate Anchors, b) developing training programs specific to low-probability events, c) developing more robust and usable indirect elicitation protocols, and d) assessing all of these methods in longitudinal forecasting tournament featuring many forecasting questions focused on rare events.},
  archiveprefix = {SSRN},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/DX3UFESZ/Atanasov et al. - 2025 - Improving Low-Probability Judgments.pdf}
}

@article{attariEnergyConservationGoals2016,
  title = {Energy Conservation Goals: {{What}} People Adopt, What They Recommend, and Why},
  shorttitle = {Energy Conservation Goals},
  author = {Attari, Shahzeen Z. and Krantz, David H. and Weber, Elke U.},
  year = {2016},
  month = jul,
  journal = {Judgment and Decision Making},
  volume = {11},
  number = {4},
  pages = {342--351},
  issn = {1930-2975},
  doi = {10.1017/S1930297500003776},
  urldate = {2025-05-05},
  abstract = {Failures to reduce greenhouse gas emissions by adopting policies, technologies, and lifestyle changes have led the world to the brink of crisis, or likely beyond. Here we use Internet surveys to attempt to understand these failures by studying factors that affect the adoption of personal energy conservation behaviors and also endorsement of energy conservation goals proposed for others. We demonstrate an asymmetry between goals for self and others (``I'll do the easy thing, you do the hard thing''), but we show that this asymmetry is partly produced by actor/observer differences: people know what they do already (and generally do not propose those actions as personal goals) and also know their own situational constraints that are barriers to action. We also show, however, that endorsement of conservation goals decreases steeply as a function of perceived difficulty; this suggests a role for motivated cognition as a barrier to conservation: difficult things are perceived as less applicable to one's situation.},
  copyright = {http://creativecommons.org/licenses/by-nc-nd/3.0/},
  langid = {english},
  annotation = {https://www.szattari.com/publications},
  file = {/Users/thomasgorman/Zotero/storage/9K3EKL3E/Attari et al. - 2016 - Energy conservation goals What people adopt, what they recommend, and why.pdf}
}

@article{attariHumanBehaviorEnergy,
  title = {Human Behavior and Energy Consumption},
  author = {Attari, Shahzeen Z},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/LNKPNGZN/Attari - Human behavior and energy consumption.pdf}
}

@article{attariPerceptionsWaterUse2014,
  title = {Perceptions of Water Use},
  author = {Attari, Shahzeen Z.},
  year = {2014},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {111},
  number = {14},
  pages = {5129--5134},
  doi = {10.1073/pnas.1316402111},
  urldate = {2025-05-05},
  abstract = {In a national online survey, 1,020 participants reported their perceptions of water use for household activities. When asked for the most effective strategy they could implement to conserve water in their lives, or what other Americans could do, most participants mentioned curtailment (e.g., taking shorter showers, turning off the water while brushing teeth) rather than efficiency improvements (e.g., replacing toilets, retrofitting washers). This contrasts with expert recommendations. Additionally, some participants are more likely to list curtailment actions for themselves, but list efficiency actions for other Americans. For a sample of 17 activities, participants underestimated water use by a factor of 2 on average, with large underestimates for high water-use activities. An additional ranking task showed poor discrimination of low vs. high embodied water content in food products. High numeracy scores, older age, and male sex were associated with more accurate perceptions of water use. Overall, perception of water use is more accurate than the perception of energy consumption and savings previously reported. Well-designed efforts to improve public understanding of household water use could pay large dividends for behavioral adaptation to temporary or long-term decreases in availability of fresh water.},
  annotation = {https://www.szattari.com/publications\\
\\
https://www.pnas.org/doi/suppl/10.1073/pnas.1316402111/suppl\_file/pnas.201316402si.pdf\\
\\
\\
https://www.pnas.org/doi/10.1073/pnas.1316402111\#supplementary-materials},
  file = {/Users/thomasgorman/Zotero/storage/P5G22WWH/Attari - 2014 - Perceptions of water use.pdf}
}

@article{attariPublicPerceptionsEnergy2010,
  title = {Public Perceptions of Energy Consumption and Savings},
  author = {Attari, Shahzeen Z. and DeKay, Michael L. and Davidson, Cliff I. and Bruine De Bruin, W{\"a}ndi},
  year = {2010},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {107},
  number = {37},
  pages = {16054--16059},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1001509107},
  urldate = {2024-12-13},
  abstract = {In a national online survey, 505 participants reported their perceptions of energy consumption and savings for a variety of household, transportation, and recycling activities. When asked for the most effective strategy they could implement to conserve energy, most participants mentioned curtailment (e.g., turning off lights, driving less) rather than efficiency improvements (e.g., installing more efficient light bulbs and appliances), in contrast to experts' recommendations. For a sample of 15 activities, participants underestimated energy use and savings by a factor of 2.8 on average, with small overestimates for low-energy activities and large underestimates for high-energy activities. Additional estimation and ranking tasks also yielded relatively flat functions for perceived energy use and savings. Across several tasks, participants with higher numeracy scores and stronger proenvironmental attitudes had more accurate perceptions. The serious deficiencies highlighted by these results suggest that well-designed efforts to improve the public's understanding of energy use and savings could pay large dividends.},
  langid = {english},
  annotation = {survey items: https://www.pnas.org/action/downloadSupplement?doi=10.1073\%2Fpnas.1001509107\&file=sapp.pdf\\
\\
supplemental: https://www.pnas.org/action/downloadSupplement?doi=10.1073\%2Fpnas.1001509107\&file=pnas.201001509SI.pdf},
  file = {/Users/thomasgorman/Zotero/storage/6UKLYDAK/Attari et al. - 2010 - Public perceptions of energy consumption and savings.pdf}
}

@article{attariReasonsCooperationDefection2014,
  title = {Reasons for Cooperation and Defection in Real-World Social Dilemmas},
  author = {Attari, Shahzeen Z. and Krantz, David H. and Weber, Elke U.},
  year = {2014},
  month = jul,
  journal = {Judgment and Decision Making},
  volume = {9},
  number = {4},
  pages = {316--334},
  issn = {1930-2975},
  doi = {10.1017/S1930297500006197},
  urldate = {2025-05-05},
  abstract = {Interventions to increase cooperation in social dilemmas depend on understanding decision makers' motivations for cooperation or defection. We examined these in five real-world social dilemmas: situations where private interests are at odds with collective ones. An online survey (N = 929) asked respondents whether or not they cooperated in each social dilemma and then elicited both open-ended reports of reasons for their choices and endorsements of a provided list of reasons. The dilemmas chosen were ones that permit individual action rather than voting or advocacy: (1) conserving energy, (2) donating blood, (3) getting a flu vaccination, (4) donating to National Public Radio (NPR), and (5) buying green electricity. Self-reported cooperation is weakly but positively correlated across these dilemmas. Cooperation in each dilemma correlates fairly strongly with self-reported altruism and with punitive attitudes toward defectors. Some strong domain-specific behaviors and beliefs also correlate with cooperation. The strongest example is frequency of listening to NPR, which predicts donation. Socio-demographic variables relate only weakly to cooperation. Respondents who self-report cooperation usually cite social reasons (including reciprocity) for their choice. Defectors often give self-interest reasons but there are also some domain-specific reasons---some report that they are not eligible to donate blood; some cannot buy green electricity because they do not pay their own electric bills. Cooperators generally report that several of the provided reasons match their actual reasons fairly well, but most defectors endorse none or at most one of the provided reasons for defection. In particular, defectors often view cooperation as costly but do not endorse free riding as a reason for defection. We tentatively conclude that cooperation in these settings is based mostly on pro-social norms and defection on a mixture of self-interest and the possibly motivated perception that situational circumstances prevent cooperation in the given situation.},
  langid = {english},
  keywords = {cooperation,self-interest,social dilemmas,social norms},
  annotation = {https://www.szattari.com/publications},
  file = {/Users/thomasgorman/Zotero/storage/STK898ID/Attari et al. - 2014 - Reasons for cooperation and defection in real-world social dilemmas.pdf}
}

@article{attariReplyFrederickAnchoring2011,
  title = {Reply to {{Frederick}} et al.: {{Anchoring}} Effects on Energy Perceptions},
  shorttitle = {Reply to {{Frederick}} et Al.},
  author = {Attari, Shahzeen Z. and DeKay, Michael L. and Davidson, Cliff I. and De Bruin, W{\"a}ndi Bruine},
  year = {2011},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {8},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1019040108},
  urldate = {2025-05-05},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/5CVRJ5AZ/Attari et al. - 2011 - Reply to Frederick et al. Anchoring effects on energy perceptions.pdf}
}

@article{ayanwaleTeachersReadinessIntention2022,
  title = {Teachers' Readiness and Intention to Teach Artificial Intelligence in Schools},
  author = {Ayanwale, Musa Adekunle and Sanusi, Ismaila Temitayo and Adelana, Owolabi Paul and Aruleba, Kehinde D. and Oyelere, Solomon Sunday},
  year = {2022},
  month = jan,
  journal = {Computers and Education: Artificial Intelligence},
  volume = {3},
  pages = {100099},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2022.100099},
  urldate = {2025-05-02},
  abstract = {The emergence of artificial intelligence (AI) as a subject to be incorporated into K-12 educational levels places new demand on relevant stakeholders, especially teachers that drive the teaching and learning process. It is therefore important to understand how ready teachers are to teach the emerging subject as the success of AI education would probably be closely dependent on the readiness of teachers. As a result, this study presents an insight into factors influencing the behavioural intention and readiness of Nigerian in-service teachers to teach artificial intelligence. A total of 368 teachers, from elementary to high school participated in the study. We utilised quantitative methodology using variance-based structural equation modelling to understand the relationship among the eight variables (AI anxiety, perceived usefulness, AI for social good, Attitude towards using AI, perceived confidence in teaching AI, relevance of AI, AI readiness, and behavioural intention) considered in the study. The result indicated that confidence in teaching AI predicts intention to teach AI while AI relevance strongly predicts readiness to teach AI. While other factors influence the teaching of AI, anxiety and social good could not predict teachers' intention and readiness to implement AI in classrooms respectively. We discussed the implication of our findings in relation to AI implementation in schools and highlight future directions.},
  keywords = {Behavioural intention,K-12,Nigerian schools,Teacher readiness,Teaching artificial intelligence},
  file = {/Users/thomasgorman/Zotero/storage/2YSYWJQ3/Ayanwale et al. - 2022 - Teachers’ readiness and intention to teach artificial intelligence in schools.pdf;/Users/thomasgorman/Zotero/storage/GCQLT4UH/S2666920X22000546.html}
}

@article{babikerAttitudeAIPotential2024,
  title = {Attitude {{Towards AI}}: {{Potential Influence}} of {{Conspiracy Belief}}, {{XAI Experience}} and {{Locus}} of {{Control}}},
  shorttitle = {Attitude {{Towards AI}}},
  author = {Babiker, Areej and Alshakhsi, Sameha and {Al-Thani}, Dena and Montag, Christian and Ali, Raian},
  year = {2024},
  month = oct,
  journal = {International Journal of Human--Computer Interaction},
  pages = {1--13},
  publisher = {Informa UK Limited},
  issn = {1044-7318, 1532-7590},
  doi = {10.1080/10447318.2024.2401249},
  urldate = {2025-05-01},
  abstract = {The proliferation of Artificial Intelligence (AI) technologies, exemplified by Large Language Models (LLM), has ushered in a transformative era across various fields. As the AI revolution will impact societies in complex and uncertain ways, it is likely that persons tending towards belief of conspiracy theories also tend to form more negative and less positive attitudes towards AI. Such persons might believe that some evil force will use AI to destroy human mankind. Drawing on the Interplay of Modality, Person, Area, Country/Culture, and Transparency categories (IMPACT) framework, this study aims to investigate the interplay of locus of control (LOC), belief in conspiracy theories, and the perception of the importance and availability of eXplainable AI (XAI) on attitudes towards AI (measured via AI acceptance and fear). The study used an online survey with 281 participants from the UK and 281 from the Arab world. Statistical analysis revealed that in the UK but not in the Arab sample, female participants reported higher fear of AI and lower acceptance of AI compared to males. The regression results consistently confirmed the role of internal LOC, perceived XAI importance, and perceived availability of XAI in fostering AI acceptance, as well as the role of belief in conspiracy theories, external LOC, and perceiving availability of XAI as being low in increasing fear of AI. The perceived availability of XAI emerges as a crucial influencing factor; addressing it appropriately could enhance societal awareness and acceptance of AI while reducing fear. Personal factors and XAI influence attitudes towards AI in both Arab and UK cultures, enhancing result robustness and revealing nuanced differences.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/LSANB5CZ/Babiker et al. - 2024 - Attitude Towards AI Potential Influence of Conspiracy Belief, XAI Experience and Locus of Control.pdf}
}

@article{barsantiAverageConsumerMapping2024,
  title = {Beyond the Average Consumer: {{Mapping}} the Potential of Demand-Side Management among Patterns of Appliance Usage},
  shorttitle = {Beyond the Average Consumer},
  author = {Barsanti, Matteo and Yilmaz, Selin and Binder, Claudia R.},
  year = {2024},
  month = may,
  journal = {Energy Research \& Social Science},
  volume = {111},
  pages = {103463},
  issn = {2214-6296},
  doi = {10.1016/j.erss.2024.103463},
  urldate = {2025-05-12},
  abstract = {To support the decarbonisation of the power sector and offset the volatility of a system with high levels of renewables, there is growing interest in residential Demand-Side Management (DSM) solutions. Traditional DSM strategies require consumers to actively adjust the timing, mode, and frequency of their appliance usage to curtail or shift in time energy consumption. Therefore, overlooking the dynamic intricacies of these adjustments and assuming uniform consumption patterns across households can lead to inaccurate and untargeted recommendations in DSM programme design. This study aims to contribute to DSM research by introducing a novel methodology for analysing energy demand and flexibility. Our primary goal is to uncover patterns in volume, timing, and mechanisms of demand management across the population. Drawing insights from engineering and social science studies, we conducted a comprehensive quantitative survey (N~=~1188) focusing on laundry and dishwashing habits in German households. Employing statistical methods, such as hierarchical clustering, multinomial logistic regression, and analysis of variance, we identify distinct patterns, explore their determinants, and assess variations in load-shifting potential and perceived inconvenience. Our findings reveal three key insights: 1) significant and meaningful patterns can be identified among the large diversity of dishwashing and laundry habits, 2) pattern membership is influenced by multiple and complex factors that resist a narrow categorisation and 3) households with more energy-intensive patterns tend to perceive load-shifting as more inconvenient, revealing a misalignment between flexibility potential and readiness. Importantly, our approach enables the identification of appliance usage patterns easily applicable in energy demand models. Furthermore, by integrating insights from various disciplines, this pattern-oriented methodology can inform more targeted and effective DSM interventions, thereby supporting the transition towards a highly electrified renewables-based energy system.},
  keywords = {Appliance usage patterns,Clustering,Demand-side management,Energy demand,Load-shifting},
  file = {/Users/thomasgorman/Zotero/storage/MHD4R8C6/Barsanti et al. - 2024 - Beyond the average consumer Mapping the potential of demand-side management among patterns of appli.pdf;/Users/thomasgorman/Zotero/storage/F6MDYQP3/S2214629624000549.html}
}

@misc{belemPerceptionsLinguisticUncertainty2024,
  title = {Perceptions of {{Linguistic Uncertainty}} by {{Language Models}} and {{Humans}}},
  author = {Belem, Catarina G. and Kelly, Markelle and Steyvers, Mark and Singh, Sameer and Smyth, Padhraic},
  year = {2024},
  month = jul,
  number = {arXiv:2407.15814},
  eprint = {2407.15814},
  publisher = {arXiv},
  urldate = {2024-07-26},
  abstract = {Uncertainty expressions such as ``probably'' or ``highly unlikely'' are pervasive in human language. While prior work has established that there is population-level agreement in terms of how humans interpret these expressions, there has been little inquiry into the abilities of language models to interpret such expressions. In this paper, we investigate how language models map linguistic expressions of uncertainty to numerical responses. Our approach assesses whether language models can employ theory of mind in this setting: understanding the uncertainty of another agent about a particular statement, independently of the model's own certainty about that statement. We evaluate both humans and 10 popular language models on a task created to assess these abilities. Unexpectedly, we find that 8 out of 10 models are able to map uncertainty expressions to probabilistic responses in a human-like manner. However, we observe systematically different behavior depending on whether a statement is actually true or false. This sensitivity indicates that language models are substantially more susceptible to bias based on their prior knowledge (as compared to humans). These findings raise important questions and have broad implications for human-AI alignment and AI-AI communication.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://github.com/UCIDataLab/llm-uncertainty-perceptions},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Belem et al_2024_Perceptions of Linguistic Uncertainty by Language Models and Humans.pdf}
}

@article{bewersdorffAIAdvocatesCautious2025,
  title = {{{AI}} Advocates and Cautious Critics: {{How AI}} Attitudes, {{AI}} Interest, Use of {{AI}}, and {{AI}} Literacy Build University Students' {{AI}} Self-Efficacy},
  shorttitle = {{{AI}} Advocates and Cautious Critics},
  author = {Bewersdorff, Arne and Hornberger, Marie and Nerdel, Claudia and Schiff, Daniel S.},
  year = {2025},
  month = jun,
  journal = {Computers and Education: Artificial Intelligence},
  volume = {8},
  pages = {100340},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2024.100340},
  urldate = {2025-05-02},
  abstract = {This study investigates how cognitive, affective, and behavioral variables related to artificial intelligence (AI) build AI self-efficacy among university students. Based on these variables, we identify three meaningful student groups, which can guide educational initiatives. We recruited 1465 undergraduate and graduate students from the United States, the United Kingdom, and Germany and measured their AI self-efficacy, AI literacy, interest in AI, attitudes towards AI, and AI use. Using a path model, we examine the correlations and paths among these variables. Results reveal that AI usage and positive AI attitudes significantly predict interest in AI, which in turn and together with AI literacy, enhance AI self-efficacy. Moreover, using Gaussian Mixture Models, we identify three groups of students: 'AI Advocates,' 'Cautious Critics,' and 'Pragmatic Observers,' each exhibiting unique patterns of AI-related cognitive, affective, and behavioral traits. Our findings demonstrate the necessity of educational strategies that not only focus on AI literacy but also aim to foster students' AI attitudes, usage, and interest to effectively promote AI self-efficacy. Furthermore, we argue that educators who aim to design inclusive AI educational programs should take into account the distinct needs of different student groups identified in this study.},
  keywords = {AI education,AI literacy,AI self-efficacy,Artificial intelligence,Gaussian mixture model,Higher education,Structural equation model},
  file = {/Users/thomasgorman/Zotero/storage/MSKS6VGF/Bewersdorff et al. - 2025 - AI advocates and cautious critics How AI attitudes, AI interest, use of AI, and AI literacy build u.pdf;/Users/thomasgorman/Zotero/storage/L7QZGD9I/S2666920X24001437.html}
}

@article{brandsmaOneAllImpact2019,
  title = {One for All? -- {{The}} Impact of Different Types of Energy Feedback and Goal Setting on Individuals' Motivation to Conserve Electricity},
  shorttitle = {One for All?},
  author = {Brandsma, Jeroen S. and Blasch, Julia E.},
  year = {2019},
  month = dec,
  journal = {Energy Policy},
  volume = {135},
  pages = {110992},
  issn = {0301-4215},
  doi = {10.1016/j.enpol.2019.110992},
  urldate = {2025-03-16},
  abstract = {We investigate how different types of energy feedback, combined with goal setting, impact on consumers' motivation to conserve electricity. Using an online survey, we test the influence of energy feedback in physical units (kWh), monetary values (EUR) and environmental values (avoided CO2 emissions). We asked participants to set themselves either a high, low or no energy conservation goal. In addition, we assess the respondents' value types - hedonic, egoistic, altruistic and biospheric -- to test predictions derived from goal framing theory. In general, individuals scoring high on biospheric values were more motivated to conserve electricity and their motivation did not increase in response to setting an energy conservation goal. Individuals with egoistic values seem less willing to reduce their electricity consumption, unless in the monetary feedback or high goal conditions. A high conservation goal was only found to be effective in combination with monetary feedback: it increased the motivation to save electricity by 6.7 percentage points in comparison to the low goal condition and 6.6 percentage points in comparison to the control condition.},
  keywords = {Energy conservation behaviour,Energy feedback,Goal framing,Goal setting,Value orientation},
  annotation = {https://data.mendeley.com/datasets/zgy44yc22r/1},
  file = {/Users/thomasgorman/Zotero/storage/B8DRDJWQ/Brandsma and Blasch - 2019 - One for all – The impact of different types of energy feedback and goal setting on individuals’ mot.pdf;/Users/thomasgorman/Zotero/storage/9XHXBST5/S0301421519305798.html}
}

@incollection{budescuProcessingLinguisticProbabilities1995,
  title = {Processing {{Linguistic Probabilities}}: {{General Principles}} and {{Empirical Evidence}}},
  shorttitle = {Processing {{Linguistic Probabilities}}},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Budescu, David V. and Wallsten, Thomas S.},
  year = {1995},
  volume = {32},
  pages = {275--318},
  publisher = {Elsevier},
  doi = {10.1016/S0079-7421(08)60313-8},
  urldate = {2025-05-12},
  isbn = {978-0-12-543332-7},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/GR9Q4NH6/Budescu and Wallsten - 1995 - Processing Linguistic Probabilities General Principles and Empirical Evidence.pdf}
}

@article{camilleriConsumersUnderestimateEmissions2019,
  title = {Consumers Underestimate the Emissions Associated with Food but Are Aided by Labels},
  author = {Camilleri, Adrian R. and Larrick, Richard P. and Hossain, Shajuti and {Patino-Echeverri}, Dalia},
  year = {2019},
  month = jan,
  journal = {Nature Climate Change},
  volume = {9},
  number = {1},
  pages = {53--58},
  issn = {1758-6798},
  doi = {10.1038/s41558-018-0354-z},
  urldate = {2025-05-07},
  abstract = {Food production is a major cause of energy use and GHG emissions, and therefore diet change is an important behavioural strategy for reducing associated environmental impacts. However, a severe obstacle to diet change may be consumers' underestimation of the environmental impacts of different types of food. Here we show that energy consumption and GHG emission estimates are significantly underestimated for foods, suggesting a possible blind spot suitable for intervention. In a second study, we find that providing consumers with information regarding the GHG emissions associated with the life cycle of food, presented in terms of a familiar reference unit (light-bulb minutes), shifts their actual purchase choices away from higher-emission options. Thus, although consumers' poor understanding of the food system is a barrier to reducing energy use and GHG emissions, it also represents a promising area for simple interventions such as a well-designed carbon label.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Attribution,Carbon and energy,Decision making,Psychology},
  annotation = {https://osf.io/smj67/files/osfstorage\\
\\
\\
https://static-content.springer.com/esm/art\%3A10.1038\%2Fs41558-018-0354-z/MediaObjects/41558\_2018\_354\_MOESM1\_ESM.pdf},
  file = {/Users/thomasgorman/Zotero/storage/3GLM9TYY/Camilleri et al. - 2019 - Consumers underestimate the emissions associated with food but are aided by labels.pdf}
}

@article{canfieldPerceptionsElectricityuseCommunications2017,
  title = {Perceptions of Electricity-Use Communications: Effects of Information, Format, and Individual Differences},
  shorttitle = {Perceptions of Electricity-Use Communications},
  author = {Canfield, Casey and Bruine De Bruin, W{\"a}ndi and {Wong-Parodi}, Gabrielle},
  year = {2017},
  month = sep,
  journal = {Journal of Risk Research},
  volume = {20},
  number = {9},
  pages = {1132--1153},
  issn = {1366-9877, 1466-4461},
  doi = {10.1080/13669877.2015.1121909},
  urldate = {2024-10-02},
  abstract = {Electricity bills could be an effective strategy for improving communications about consumers' electricity use and promoting electricity savings. However, quantitative communications about electricity use may be difficult to understand, especially for consumers with low energy literacy. Here, we build on the health communication and graph comprehension literature to inform electricity bill design, with the goal of improving understanding, preferences for the presented communication, and intentions to save electricity. In a survey-based experiment, each participant saw a hypothetical electricity bill for a family with relatively high electricity use, covering information about (a) historical use, (b) comparisons to neighbors, and (c) historical use with appliance breakdown. Participants saw all information types in one of three formats including (a) tables, (b) bar graphs, and (c) icon graphs. We report on three main findings. First, consumers understood each type of electricity-use information the most when it was presented in a table, perhaps because tables facilitate simple point reading. Second, preferences and intentions to save electricity were the strongest for the historical use information, independent of format. Third, individuals with lower energy literacy understood all information less. We discuss implications for designing utility bills that are understandable, perceived as useful, and motivate consumers to save energy.},
  langid = {english},
  annotation = {docx supp file:\\
\\
https://www.tandfonline.com/doi/suppl/10.1080/13669877.2015.1121909?scroll=top},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Canfield et al_2017_Perceptions of electricity-use communications.pdf}
}

@article{carolusMAILSMetaAI2023,
  title = {{{MAILS}} - {{Meta AI}} Literacy Scale: {{Development}} and Testing of an {{AI}} Literacy Questionnaire Based on Well-Founded Competency Models and Psychological Change- and Meta-Competencies},
  shorttitle = {{{MAILS}} - {{Meta AI}} Literacy Scale},
  author = {Carolus, Astrid and Koch, Martin J. and Straka, Samantha and Latoschik, Marc Erich and Wienrich, Carolin},
  year = {2023},
  month = aug,
  journal = {Computers in Human Behavior: Artificial Humans},
  volume = {1},
  number = {2},
  pages = {100014},
  issn = {2949-8821},
  doi = {10.1016/j.chbah.2023.100014},
  urldate = {2025-04-28},
  abstract = {Valid measurement of AI literacy is important for the selection of personnel, identification of shortages in skill and knowledge, and evaluation of AI literacy interventions. A questionnaire is missing that is deeply grounded in the existing literature on AI literacy, is modularly applicable depending on the goals, and includes further psychological competencies in addition to the typical facets of AIL. This paper presents the development and validation of a questionnaire considering the desiderata described above. We derived items to represent different facets of AI literacy and psychological competencies, such as problem-solving, learning, and emotion regulation in regard to AI. We collected data from 300 German-speaking adults to confirm the factorial structure. The result is the Meta AI Literacy Scale (MAILS) for AI literacy with the facets Use \& apply AI, Understand AI, Detect AI, and AI Ethics and the ability to Create AI as a separate construct, and AI Self-efficacy in learning and problem-solving and AI Self-management (i.e., AI persuasion literacy and emotion regulation). This study contributes to the research on AI literacy by providing a measurement instrument relying on profound competency models. Psychological competencies are included particularly important in the context of pervasive change through AI systems.},
  keywords = {AI literacy,Competence modeling,Psychological competencies,Questionnaire development,Self-efficacy},
  annotation = {https://hci.uni-wuerzburg.de/research/MAILS/\\
\\
https://downloads.hci.informatik.uni-wuerzburg.de/MAILS-qustionnaire-english.pdf\\
\\
https://downloads.hci.informatik.uni-wuerzburg.de/MAILS-short-questionnaire-english.pdf},
  file = {/Users/thomasgorman/Zotero/storage/7A66XBBY/Carolus et al. - 2023 - MAILS - Meta AI literacy scale Development and testing of an AI literacy questionnaire based on wel.pdf;/Users/thomasgorman/Zotero/storage/MDEFHI9M/S2949882123000142.html}
}

@misc{cashQuantifyingUncertAIntyTesting2024,
  title = {Quantifying {{UncertAInty}}: {{Testing}} the {{Accuracy}} of {{LLMs}}' {{Confidence Judgments}}},
  shorttitle = {Quantifying {{UncertAInty}}},
  author = {Cash, Trent N. and Oppenheimer, Daniel M. and Christie, Sara},
  year = {2024},
  month = jul,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/47df5},
  urldate = {2025-05-07},
  abstract = {The rise of Large Language Model (LLM) chatbots, such as ChatGPT and Gemini, has revolutionized how we access information. These LLMs can answer a wide array of questions, including those for which the answer is uncertain, such as predictions about future events. For example, a user may ask an LLM to predict whether a stock will go up in the next week or if they will earn an A on their final exam. When humans make these predictions, they often accompany their responses with metacognitive confidence judgments indicating their belief in the accuracy of their prediction. LLMs are certainly capable of -- and willing to -- provide confidence judgments, but it is currently unclear how meaningful or accurate these confidence judgments are. To fill this gap in the literature, the present studies investigate the capability of LLMs to quantify uncertainty through confidence judgments. We evaluate the absolute and relative accuracy of confidence judgments from two LLMs (ChatGPT and Gemini) compared to human participants across three prediction domains: NFL game winners (Study 1a; n = 502), Oscar award winners (Study 1b; n = 109), and future Pictionary performance (Study 2; n = 164). Our findings reveal that LLMs' confidence judgments closely align with those of humans in terms of accuracy, biases, and errors. However, unlike humans, LLMs struggle to adjust their confidence judgments based on past performance, highlighting a key area for improvement in their design.},
  copyright = {https://creativecommons.org/publicdomain/zero/1.0/legalcode},
  langid = {english},
  annotation = {https://osf.io/b6qhx/files/osfstorage?view\_only=a5451f8df2f941e58ab11280f34c3c80\#},
  file = {/Users/thomasgorman/Zotero/storage/DMF4KXJE/Cash et al. - 2024 - Quantifying UncertAInty Testing the Accuracy of LLMs’ Confidence Judgments.pdf}
}

@misc{casolinEvaluatingInfluencesExplanation2024,
  title = {Evaluating the {{Influences}} of {{Explanation Style}} on {{Human-AI Reliance}}},
  author = {Casolin, Emma and Salim, Flora D. and Newell, Ben},
  year = {2024},
  month = oct,
  number = {arXiv:2410.20067},
  eprint = {2410.20067},
  publisher = {arXiv},
  urldate = {2024-11-01},
  abstract = {Explainable AI (XAI) aims to support appropriate human-AI reliance by increasing the interpretability of complex model decisions. Despite the proliferation of proposed methods, there is mixed evidence surrounding the effects of different styles of XAI explanations on human-AI reliance. Interpreting these conflicting findings requires an understanding of the individual and combined qualities of different explanation styles that influence appropriate and inappropriate human-AI reliance, and the role of interpretability in this interaction. In this study, we investigate the influences of feature-based, example-based, and combined feature- and example-based XAI methods on human-AI reliance through a two-part experimental study with 274 participants comparing these explanation style conditions. Our findings suggest differences between feature-based and example-based explanation styles beyond interpretability that affect human-AI reliance patterns across differences in individual performance and task complexity. Our work highlights the importance of adapting explanations to their specific users and context over maximising broad interpretability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/B3PSHMRP/Casolin et al. - 2024 - Evaluating the Influences of Explanation Style on Human-AI Reliance.pdf;/Users/thomasgorman/Zotero/storage/9IPQFWPM/2410.html}
}

@misc{chenEvaluatingHumanTrust2025,
  title = {Evaluating {{Human Trust}} in {{LLM-Based Planners}}: {{A Preliminary Study}}},
  shorttitle = {Evaluating {{Human Trust}} in {{LLM-Based Planners}}},
  author = {Chen, Shenghui and Yang, Yunhao and Boggess, Kayla and Heo, Seongkook and Feng, Lu and Topcu, Ufuk},
  year = {2025},
  month = feb,
  number = {arXiv:2502.20284},
  eprint = {2502.20284},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.20284},
  urldate = {2025-05-04},
  abstract = {Large Language Models (LLMs) are increasingly used for planning tasks, offering unique capabilities not found in classical planners such as generating explanations and iterative refinement. However, trust--a critical factor in the adoption of planning systems--remains underexplored in the context of LLM-based planning tasks. This study bridges this gap by comparing human trust in LLM-based planners with classical planners through a user study in a Planning Domain Definition Language (PDDL) domain. Combining subjective measures, such as trust questionnaires, with objective metrics like evaluation accuracy, our findings reveal that correctness is the primary driver of trust and performance. Explanations provided by the LLM improved evaluation accuracy but had limited impact on trust, while plan refinement showed potential for increasing trust without significantly enhancing evaluation accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/4ZX8BZJW/Chen et al. - 2025 - Evaluating Human Trust in LLM-Based Planners A Preliminary Study.pdf;/Users/thomasgorman/Zotero/storage/UWH7P8F5/2502.html}
}

@misc{chenMissingPiecesHow2025,
  title = {Missing {{Pieces}}: {{How Do Designs}} That {{Expose Uncertainty Longitudinally Impact Trust}} in {{AI Decision Aids}}? {{An In Situ Study}} of {{Gig Drivers}}},
  shorttitle = {Missing {{Pieces}}},
  author = {Chen, Rex and Wang, Ruiyi and Sadeh, Norman and Fang, Fei},
  year = {2025},
  month = apr,
  number = {arXiv:2404.06432},
  eprint = {2404.06432},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.06432},
  urldate = {2025-05-08},
  abstract = {Decision aids based on artificial intelligence (AI) induce a wide range of outcomes when they are deployed in uncertain environments. In this paper, we investigate how users' trust in recommendations from an AI decision aid is impacted over time by designs that expose uncertainty in predicted outcomes. Unlike previous work, we focus on gig driving - a real-world, repeated decision-making context. We report on a longitudinal mixed-methods study (\$n=51\$) where we measured gig drivers' trust as they interacted with an AI-based schedule recommendation tool. Our results show that participants' trust in the tool was shaped by both their first impressions of its accuracy and their longitudinal interactions with it; and that task-aligned framings of uncertainty improved trust by allowing participants to incorporate uncertainty into their decision-making processes. Additionally, we observed that trust depended on their characteristics as drivers, underscoring the need for more in situ studies of AI decision aids.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/SBRGNSS3/Chen et al. - 2025 - Missing Pieces How Do Designs that Expose Uncertainty Longitudinally Impact Trust in AI Decision Ai.pdf;/Users/thomasgorman/Zotero/storage/LGX2GZ32/2404.html}
}

@inproceedings{chenPortrayingLargeLanguage2025,
  title = {Portraying {{Large Language Models}} as {{Machines}}, {{Tools}}, or {{Companions Affects What Mental Capacities Humans Attribute}} to {{Them}}},
  booktitle = {Proceedings of the {{Extended Abstracts}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Chen, Allison and Kim, Sunnie S. Y. and Dharmasiri, Amaya and Russakovsky, Olga and Fan, Judith E.},
  year = {2025},
  month = apr,
  pages = {1--14},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706599.3719710},
  urldate = {2025-04-30},
  abstract = {As large language models (LLMs) become increasingly popular and prevalent in media and daily conversations, individuals encounter different portrayals of LLMs from various sources. It is important to understand how these portrayals can shape their beliefs about LLMs as this can have downstream impacts on adoption and usage behaviors. In this work, we investigate what mental capacities individuals attribute to LLMs after being exposed to short videos adopting one of three portrayals: mechanistic (LLMs as machines), functional (LLMs as tools), and intentional (LLMs as companions). We find that the intentional portrayal increases the attribution of mental capacities to LLMs, and that individuals tend to attribute mind-related capacities the most, followed by heart- then bodyrelated capacities. We discuss the implications of these findings, provide recommendations on how to portray LLMs, and outline directions for future research.},
  copyright = {http://www.acm.org/publications/policies/copyright\_policy\#Background},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/VVVF3TQX/Chen et al. - 2025 - Portraying Large Language Models as Machines, Tools, or Companions Affects What Mental Capacities Hu.pdf}
}

@article{chenWhatCanWe2015,
  title = {What Can We Learn from High-Frequency Appliance-Level Energy Metering? {{Results}} from a Field Experiment},
  shorttitle = {What Can We Learn from High-Frequency Appliance-Level Energy Metering?},
  author = {Chen, Victor L. and Delmas, Magali A. and Kaiser, William J. and Locke, Stephen L.},
  year = {2015},
  month = feb,
  journal = {Energy Policy},
  volume = {77},
  pages = {164--175},
  issn = {0301-4215},
  doi = {10.1016/j.enpol.2014.11.021},
  urldate = {2025-05-12},
  abstract = {This study uses high-frequency appliance-level electricity consumption data for 124 apartments over 24 months to provide a better understanding of appliance-level electricity consumption behavior. We conduct our analysis in a standardized set of apartments with similar appliances, which allows us to identify behavioral differences in electricity use. The Results show that households' estimations of appliance-level consumption are inaccurate and that they overestimate lighting use by 75\% and underestimate plug-load use by 29\%. We find that similar households using the same major appliances exhibit substantial variation in appliance-level electricity consumption. For example, households in the 75th percentile of HVAC usage use over four times as much electricity as a user in the 25th percentile. Additionally, we show that behavior accounts for 25--58\% of this variation. Lastly, we find that replacing the existing refrigerator with a more energy-efficient model leads to overall energy savings of approximately 11\%. This is equivalent to results from behavioral interventions targeting all appliances but might not be as cost effective. Our findings have important implications for behavior-based energy conservation policies.},
  keywords = {Appliance electricity usage,Consumer behavior,Energy monitoring,Field experiments,Information feedback,Smart metering},
  file = {/Users/thomasgorman/Zotero/storage/BJRD79L8/Chen et al. - 2015 - What can we learn from high-frequency appliance-level energy metering Results from a field experime.pdf;/Users/thomasgorman/Zotero/storage/MF8MGENF/S0301421514006296.html}
}

@incollection{chisikImageElectricityUnderstanding2011,
  title = {An {{Image}} of {{Electricity}}: {{Towards}} an {{Understanding}} of {{How People Perceive Electricity}}},
  shorttitle = {An {{Image}} of {{Electricity}}},
  booktitle = {Human-{{Computer Interaction}} -- {{INTERACT}} 2011},
  author = {Chisik, Yoram},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Campos, Pedro and Graham, Nicholas and Jorge, Joaquim and Nunes, Nuno and Palanque, Philippe and Winckler, Marco},
  year = {2011},
  volume = {6949},
  pages = {100--117},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-23768-3_9},
  urldate = {2025-05-05},
  abstract = {Although an enormous amount of research effort has been devoted to understanding people's energy consumption habits, visualizing their consumption and finding ways of motivating them towards more sustainable behaviours we are still in the dark with regards to people's basic perception of electricity, their concept of what electricity is and their notion of the consumption rates of various electrical devices. In this study we have employed a sketching methodology to elicit people's basic mental image of what electricity is, how they conceive of the electrical infrastructure in their home and which devices they think represent the largest drain on their wallets. Preliminary analysis of the results show that people do not have a clear mental model of electricity and tend to associate the size of the device and the duration of use with higher rates of consumption regardless of the type of device, the type of use it is put to and its actual consumption level.},
  isbn = {978-3-642-23767-6 978-3-642-23768-3},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/NEQC9UR2/Chisik - 2011 - An Image of Electricity Towards an Understanding of How People Perceive Electricity.pdf}
}

@article{choudhuryImpactPerformanceExpectancy2024,
  title = {The {{Impact}} of {{Performance Expectancy}}, {{Workload}}, {{Risk}}, and {{Satisfaction}} on {{Trust}} in {{ChatGPT}}: {{Cross-Sectional Survey Analysis}}},
  shorttitle = {The {{Impact}} of {{Performance Expectancy}}, {{Workload}}, {{Risk}}, and {{Satisfaction}} on {{Trust}} in {{ChatGPT}}},
  author = {Choudhury, Avishek and Shamszare, Hamid},
  year = {2024},
  month = may,
  journal = {JMIR human factors},
  volume = {11},
  pages = {e55399},
  issn = {2292-9495},
  doi = {10.2196/55399},
  abstract = {BACKGROUND: ChatGPT (OpenAI) is a powerful tool for a wide range of tasks, from entertainment and creativity to health care queries. There are potential risks and benefits associated with this technology. In the discourse concerning the deployment of ChatGPT and similar large language models, it is sensible to recommend their use primarily for tasks a human user can execute accurately. As we transition into the subsequent phase of ChatGPT deployment, establishing realistic performance expectations and understanding users' perceptions of risk associated with its use are crucial in determining the successful integration of this artificial intelligence (AI) technology. OBJECTIVE: The aim of the study is to explore how perceived workload, satisfaction, performance expectancy, and risk-benefit perception influence users' trust in ChatGPT. METHODS: A semistructured, web-based survey was conducted with 607 adults in the United States who actively use ChatGPT. The survey questions were adapted from constructs used in various models and theories such as the technology acceptance model, the theory of planned behavior, the unified theory of acceptance and use of technology, and research on trust and security in digital environments. To test our hypotheses and structural model, we used the partial least squares structural equation modeling method, a widely used approach for multivariate analysis. RESULTS: A total of 607 people responded to our survey. A significant portion of the participants held at least a high school diploma (n=204, 33.6\%), and the majority had a bachelor's degree (n=262, 43.1\%). The primary motivations for participants to use ChatGPT were for acquiring information (n=219, 36.1\%), amusement (n=203, 33.4\%), and addressing problems (n=135, 22.2\%). Some participants used it for health-related inquiries (n=44, 7.2\%), while a few others (n=6, 1\%) used it for miscellaneous activities such as brainstorming, grammar verification, and blog content creation. Our model explained 64.6\% of the variance in trust. Our analysis indicated a significant relationship between (1) workload and satisfaction, (2) trust and satisfaction, (3) performance expectations and trust, and (4) risk-benefit perception and trust. CONCLUSIONS: The findings underscore the importance of ensuring user-friendly design and functionality in AI-based applications to reduce workload and enhance user satisfaction, thereby increasing user trust. Future research should further explore the relationship between risk-benefit perception and trust in the context of AI chatbots.},
  langid = {english},
  pmcid = {PMC11165287},
  pmid = {38801658},
  keywords = {Adult,algorithms,artificial intelligence,Artificial Intelligence,chatbots,ChatGPT,Cross-Sectional Studies,cross-sectional survey,decision-making,deep learning,Female,health care,health care decision-making,health care management,health-related decision-making,Humans,Male,Middle Aged,Personal Satisfaction,practical models,predictive analytics,predictive models,predictive system,Risk Assessment,Surveys and Questionnaires,Trust,United States,usability,usable,usableness,usefulness,user perception,Workload},
  file = {/Users/thomasgorman/Zotero/storage/34MP5ADR/Choudhury and Shamszare - 2024 - The Impact of Performance Expectancy, Workload, Risk, and Satisfaction on Trust in ChatGPT Cross-Se.pdf}
}

@article{choudhuryInvestigatingImpactUser2023,
  title = {Investigating the {{Impact}} of {{User Trust}} on the {{Adoption}} and {{Use}} of {{ChatGPT}}: {{Survey Analysis}}},
  shorttitle = {Investigating the {{Impact}} of {{User Trust}} on the {{Adoption}} and {{Use}} of {{ChatGPT}}},
  author = {Choudhury, Avishek and Shamszare, Hamid},
  year = {2023},
  month = jun,
  journal = {Journal of Medical Internet Research},
  volume = {25},
  pages = {e47184},
  issn = {1439-4456},
  doi = {10.2196/47184},
  urldate = {2025-05-02},
  abstract = {Background ChatGPT (Chat Generative Pre-trained Transformer) has gained popularity for its ability to generate human-like responses. It is essential to note that overreliance or blind trust in ChatGPT, especially in high-stakes decision-making contexts, can have severe consequences. Similarly, lacking trust in the technology can lead to underuse, resulting in missed opportunities. Objective This study investigated the impact of users' trust in ChatGPT on their intent and actual use of the technology. Four hypotheses were tested: (1) users' intent to use ChatGPT increases with their trust in the technology; (2) the actual use of ChatGPT increases with users' intent to use the technology; (3) the actual use of ChatGPT increases with users' trust in the technology; and (4) users' intent to use ChatGPT can partially mediate the effect of trust in the technology on its actual use. Methods This study distributed a web-based survey to adults in the United States who actively use ChatGPT (version 3.5) at least once a month between February 2023 through March 2023. The survey responses were used to develop 2 latent constructs: Trust and Intent to Use, with Actual Use being the outcome variable. The study used partial least squares structural equation modeling to evaluate and test the structural model and hypotheses. Results In the study, 607 respondents completed the survey. The primary uses of ChatGPT were for information gathering (n=219, 36.1\%), entertainment (n=203, 33.4\%), and problem-solving (n=135, 22.2\%), with a smaller number using it for health-related queries (n=44, 7.2\%) and other activities (n=6, 1\%). Our model explained 50.5\% and 9.8\% of the variance in Intent to Use and Actual Use, respectively, with path coefficients of 0.711 and 0.221 for Trust on Intent to Use and Actual Use, respectively. The bootstrapped results failed to reject all 4 null hypotheses, with Trust having a significant direct effect on both Intent to Use ({$\beta$}=0.711, 95\% CI 0.656-0.764) and Actual Use ({$\beta$}=0.302, 95\% CI 0.229-0.374). The indirect effect of Trust on Actual Use, partially mediated by Intent to Use, was also significant ({$\beta$}=0.113, 95\% CI 0.001-0.227). Conclusions Our results suggest that trust is critical to users' adoption of ChatGPT. It remains crucial to highlight that ChatGPT was not initially designed for health care applications. Therefore, an overreliance on it for health-related advice could potentially lead to misinformation and subsequent health risks. Efforts must be focused on improving the ChatGPT's ability to distinguish between queries that it can safely handle and those that should be redirected to human experts (health care professionals). Although risks are associated with excessive trust in artificial intelligence--driven chatbots such as ChatGPT, the potential risks can be reduced by advocating for shared accountability and fostering collaboration between developers, subject matter experts, and human factors researchers.},
  pmcid = {PMC10337387},
  pmid = {37314848},
  file = {/Users/thomasgorman/Zotero/storage/LIPMFQN3/Choudhury and Shamszare - 2023 - Investigating the Impact of User Trust on the Adoption and Use of ChatGPT Survey Analysis.pdf}
}

@article{chugunovaWeItInterdisciplinary2022,
  title = {We and {{It}}: {{An}} Interdisciplinary Review of the Experimental Evidence on How Humans Interact with Machines},
  shorttitle = {We and {{It}}},
  author = {Chugunova, Marina and Sele, Daniela},
  year = {2022},
  month = aug,
  journal = {Journal of Behavioral and Experimental Economics},
  volume = {99},
  pages = {101897},
  issn = {22148043},
  doi = {10.1016/j.socec.2022.101897},
  urldate = {2024-12-11},
  abstract = {Today, humans interact with automation frequently and in a variety of settings ranging from private to professional. Their behavior in these interactions has attracted considerable research interest across several fields, with sometimes little exchange among them and seemingly inconsistent findings. In this article, we review 136 experimental studies on how people interact with automated agents, that can assume different roles. We synthesize the evidence, suggest ways to reconcile inconsistencies between studies and disciplines, and discuss organizational and societal implications. The reviewed studies show that people react to automated agents differently than they do to humans: In general, they behave more rationally, and are less prone to emotional and social responses. Task context, performance expectations and the distribution of decision authority between humans and automated agents are all factors that systematically impact the willingness to accept automated agents in decision-making - that is, humans seem willing to (over-)rely on algorithmic support, yet averse to fully ceding their decision authority. The impact of these behavioral regularities for the deliberation of the benefits and risks of automation in organizations and society is discussed.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/SVE2AGFP/Chugunova and Sele - 2022 - We and It An interdisciplinary review of the experimental evidence on how humans interact with mach.pdf}
}

@inproceedings{collinsHumanUncertaintyConceptBased2023,
  title = {Human {{Uncertainty}} in {{Concept-Based AI Systems}}},
  booktitle = {Proceedings of the 2023 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Collins, Katherine Maeve and Barker, Matthew and Espinosa Zarlenga, Mateo and Raman, Naveen and Bhatt, Umang and Jamnik, Mateja and Sucholutsky, Ilia and Weller, Adrian and Dvijotham, Krishnamurthy},
  year = {2023},
  month = aug,
  pages = {869--889},
  publisher = {ACM},
  address = {Montr{\'e}al QC Canada},
  doi = {10.1145/3600211.3604692},
  urldate = {2025-05-13},
  isbn = {979-8-4007-0231-0},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/JUL3VRM6/Collins et al. - 2023 - Human Uncertainty in Concept-Based AI Systems.pdf}
}

@article{collinsVerbalNumericProbabilities2024,
  title = {Verbal and Numeric Probabilities Differentially Shape Decisions},
  author = {Collins, Robert N. and , David R., Mandel and {and MacLeod}, Brooke A.},
  year = {2024},
  month = jan,
  journal = {Thinking \& Reasoning},
  volume = {30},
  number = {1},
  pages = {235--257},
  issn = {1354-6783},
  doi = {10.1080/13546783.2023.2220971},
  urldate = {2025-05-13},
  abstract = {Experts often communicate probabilities verbally (e.g., unlikely) rather than numerically (e.g., 25\% chance). Although criticism has focused on the vagueness of verbal probabilities, less attention has been given to the potential unintended, biasing effects of verbal probabilities in communicating probabilities to decision-makers. In four experiments (Ns = 201, 439, 435, 696), we showed that probability format (i.e., verbal vs. numeric) influenced participants' inferences and decisions following a hypothetical financial expert's forecast. We observed a format effect for low probability forecasts: verbal probabilities were interpreted more pessimistically than numeric equivalents. We attributed the difference to directionality, a linguistic property that biases attention toward an outcome. In the high-probability conditions, the directionality of verbal and numeric probabilities aligned (both were positive), whereas they differed in the low-probability conditions (verbal probabilities were more negative). Participants inferred recommendations congruent with the communicated direction and these inferences mediated the effect of probability format on decisions.},
  keywords = {decision making,directionality,numeric probability,recommendation,Verbal probability},
  annotation = {https://osf.io/evwm8/},
  file = {/Users/thomasgorman/Zotero/storage/JT6Q2QQW/Collins et al. - 2024 - Verbal and numeric probabilities differentially shape decisions.pdf}
}

@article{cottonReducingEnergyDemand2021,
  title = {Reducing Energy Demand in {{China}} and the {{United Kingdom}}: {{The}} Importance of Energy Literacy},
  shorttitle = {Reducing Energy Demand in {{China}} and the {{United Kingdom}}},
  author = {Cotton, D. R. E. and Zhai, J. and Miller, W. and Dalla Valle, L. and Winter, J.},
  year = {2021},
  month = jan,
  journal = {Journal of Cleaner Production},
  volume = {278},
  pages = {123876},
  issn = {0959-6526},
  doi = {10.1016/j.jclepro.2020.123876},
  urldate = {2025-05-02},
  abstract = {As the impacts of climate change become increasingly visible across the globe, awareness of the need for cleaner energy and demand reduction is growing. Energy literacy offers a strong potential for explaining and predicting energy-related behaviours, yet research and policies focused on this topic remain limited. In this study, energy literacy was measured in a sample of 2806 university students in the United Kingdom and China, in addition to their wider environmental attitudes using the New Ecological Paradigm scale. Findings indicate that energy literacy was relatively high overall, but there were significant differences between the knowledge, attitudes and behavioural intentions of participants in the two countries. Whilst the UK respondents rated themselves significantly more highly on perceived knowledge of energy issues, Chinese respondents provided significantly more correct answers in a knowledge test. UK respondents demonstrated more positive attitudes towards energy conservation than those from China, and were more likely to report energy-saving behaviours. However, Chinese respondents exhibited higher levels of trust in government and businesses to take action on energy issues. This paper provides a novel insight into cultural differences which may be crucial to policy and practice, and evidences the potential benefits of utilising a combination of educational and structural change to support transition to a cleaner, low-energy society.},
  keywords = {Attitude,Behaviour,Energy literacy,Higher education,Knowledge,Policy},
  file = {/Users/thomasgorman/Zotero/storage/7CFDWEMU/Cotton et al. - 2021 - Reducing energy demand in China and the United Kingdom The importance of energy literacy.pdf;/Users/thomasgorman/Zotero/storage/BDY7UNPH/S0959652620339214.html}
}

@article{cowenTestingSizeHeuristic2017,
  title = {Testing for the Size Heuristic in Householders' Perceptions of Energy Consumption},
  author = {Cowen, Laura and Gatersleben, Birgitta},
  year = {2017},
  month = dec,
  journal = {Journal of Environmental Psychology},
  volume = {54},
  pages = {103--115},
  issn = {0272-4944},
  doi = {10.1016/j.jenvp.2017.10.002},
  urldate = {2025-05-05},
  abstract = {Few householders have the time or motivation to systematically weigh up all the facts when judging the energy consumption of their household appliances. It is likely that they instead rely on simple heuristics such as the size heuristic, which has been reported in a small number of previous studies. The studies showed that people's perceptions of the size and energy consumption of appliances were positively correlated but the studies differed in their methods and effect sizes. The present study re-tests the use of the size heuristic using two methods of data collection (between-participants and within-participants) and three methods of correlation. On average, correlations between size and energy estimates were moderately strong but they (and the accuracy of the energy estimates) varied greatly between individual participants. Understanding householders' perceptions of energy is vital to designing more effective energy-saving policies. The findings highlight the importance of choosing and clearly reporting methods.},
  keywords = {Correlation,Energy consumption,Methods,Perception,Size heuristic},
  file = {/Users/thomasgorman/Zotero/storage/TXKKNZ5Y/Cowen and Gatersleben - 2017 - Testing for the size heuristic in householders’ perceptions of energy consumption.pdf;/Users/thomasgorman/Zotero/storage/NWHBNT4J/S0272494417301147.html}
}

@misc{cruzEvaluatingLanguageModels2024,
  title = {Evaluating Language Models as Risk Scores},
  author = {Cruz, Andr{\'e} F. and Hardt, Moritz and {Mendler-D{\"u}nner}, Celestine},
  year = {2024},
  month = sep,
  number = {arXiv:2407.14614},
  eprint = {2407.14614},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.14614},
  urldate = {2025-05-08},
  abstract = {Current question-answering benchmarks predominantly focus on accuracy in realizable prediction tasks. Conditioned on a question and answer-key, does the most likely token match the ground truth? Such benchmarks necessarily fail to evaluate LLMs' ability to quantify ground-truth outcome uncertainty. In this work, we focus on the use of LLMs as risk scores for unrealizable prediction tasks. We introduce folktexts, a software package to systematically generate risk scores using LLMs, and evaluate them against US Census data products. A flexible API enables the use of different prompting schemes, local or web-hosted models, and diverse census columns that can be used to compose custom prediction tasks. We evaluate 17 recent LLMs across five proposed benchmark tasks. We find that zero-shot risk scores produced by multiple-choice question-answering have high predictive signal but are widely miscalibrated. Base models consistently overestimate outcome uncertainty, while instruction-tuned models underestimate uncertainty and produce over-confident risk scores. In fact, instruction-tuning polarizes answer distribution regardless of true underlying data uncertainty. This reveals a general inability of instruction-tuned LLMs to express data uncertainty using multiple-choice answers. A separate experiment using verbalized chat-style risk queries yields substantially improved calibration across instruction-tuned models. These differences in ability to quantify data uncertainty cannot be revealed in realizable settings, and highlight a blind-spot in the current evaluation ecosystem that folktexts covers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Zotero/storage/SI9RNZ24/Cruz et al. - 2024 - Evaluating language models as risk scores.pdf;/Users/thomasgorman/Zotero/storage/ENKXBVZ6/2407.html}
}

@article{dewatersDesigningEnergyLiteracy2013,
  title = {Designing an {{Energy Literacy Questionnaire}} for {{Middle}} and {{High School Youth}}},
  author = {DeWaters, Jan and Qaqish, Basil and Graham, Mary and Powers, Susan},
  year = {2013},
  month = jan,
  journal = {The Journal of Environmental Education},
  volume = {44},
  number = {1},
  pages = {56--78},
  issn = {0095-8964, 1940-1892},
  doi = {10.1080/00958964.2012.682615},
  urldate = {2025-04-30},
  abstract = {A measurement scale has been developed to assess secondary students' energy literacy---a citizenship understanding of energy that includes cognitive as well as affective and behavioral items. Instrument development procedures followed psychometric principles from educational and social psychology research. Initial exploration of the measure yielded promising results: internal consistencies for the cognitive, affective, and behavioral subscales, measured by Cronbach's {$\alpha$}, ranged from 0.75 to 0.83; average discrimination indices ranged from 0.27 to 0.46. The instrument's validity was supported with contrasted-groups and developmental-age progression comparisons, as well as factor analyses. The energy literacy questionnaire provides an opportunity to measure baseline levels of energy literacy and to assess broader impacts of educational interventions.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/BXGK3SBU/DeWaters et al. - 2013 - Designing an Energy Literacy Questionnaire for Middle and High School Youth.pdf}
}

@article{dewatersEnergyLiteracySecondary2011,
  title = {Energy Literacy of Secondary Students in {{New York State}} ({{USA}}): {{A}} Measure of Knowledge, Affect, and Behavior},
  shorttitle = {Energy Literacy of Secondary Students in {{New York State}} ({{USA}})},
  author = {DeWaters, Jan E. and Powers, Susan E.},
  year = {2011},
  month = mar,
  journal = {Energy Policy},
  volume = {39},
  number = {3},
  pages = {1699--1710},
  issn = {0301-4215},
  doi = {10.1016/j.enpol.2010.12.049},
  urldate = {2024-12-11},
  abstract = {Energy literacy, which encompasses broad content knowledge as well as affective and behavioral characteristics, will empower people to make appropriate energy-related choices and embrace changes in the way we harness and consume energy. Energy literacy was measured with a written questionnaire completed by 3708 secondary students in New York State, USA. Results indicate that students are concerned about energy problems (affective subscale mean 73\% of the maximum attainable score), yet relatively low cognitive (42\% correct) and behavioral (65\% of the maximum) scores suggest that students may lack the knowledge and skills they need to effectively contribute toward solutions. High school (HS) students scored significantly better than middle school (MS) students on the cognitive subscale; gains were greatest on topics included in NY State educational standards, and less on topics related to ``practical'' energy knowledge such as ways to save energy. Despite knowledge gains, there was a significant drop in energy conservation behavior between the MS and HS students. Intercorrelations between groups of questions indicate energy-related behaviors are more strongly related to affect than to knowledge. These findings underscore the need for education that improves energy literacy by impacting student attitudes, values and behaviors, as well as broad content knowledge.},
  keywords = {Assessment,Energy education,Energy literacy},
  annotation = {https://ars-els-cdn-com.proxyiub.uits.iu.edu/content/image/1-s2.0-S0301421511000073-mmc1.doc},
  file = {/Users/thomasgorman/Zotero/storage/QKSQ2SPA/DeWaters and Powers - 2011 - Energy literacy of secondary students in New York State (USA) A measure of knowledge, affect, and b.pdf;/Users/thomasgorman/Zotero/storage/6W7G5PQ9/S0301421511000073.html}
}

@article{dewatersEstablishingMeasurementCriteria2013,
  title = {Establishing {{Measurement Criteria}} for an {{Energy Literacy Questionnaire}}},
  author = {DeWaters, Jan and Powers, Susan},
  year = {2013},
  month = jan,
  journal = {The Journal of Environmental Education},
  volume = {44},
  number = {1},
  pages = {38--55},
  issn = {0095-8964, 1940-1892},
  doi = {10.1080/00958964.2012.711378},
  urldate = {2025-04-30},
  abstract = {Energy literacy is a broad term encompassing content knowledge as well as a citizenship understanding of energy that includes affective and behavioral aspects. This article presents explicit criteria that will serve as a foundation for developing measurable objectives for energy literacy in three dimensions: cognitive (knowledge, cognitive skills), affective (attitude, values, personal responsibility); and behavioral. The outcome of this research is a framework from which a quantitative survey of energy literacy for secondary students in New York State, United States, can be created. Efforts supported by this research may help assess the broader impacts of educational programs in terms of their effectiveness for improving students' energy literacy.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/SFF88D6A/DeWaters and Powers - 2013 - Establishing Measurement Criteria for an Energy Literacy Questionnaire.pdf}
}

@article{dhamiCommunicatingUncertaintyUsing2022,
  title = {Communicating Uncertainty Using Words and Numbers},
  author = {Dhami, Mandeep K. and Mandel, David R.},
  year = {2022},
  month = jun,
  journal = {Trends in Cognitive Sciences},
  volume = {26},
  number = {6},
  pages = {514--526},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2022.03.002},
  urldate = {2025-05-13},
  abstract = {Life in an increasingly information-rich but highly uncertain world calls for an effective means of communicating uncertainty to a range of audiences. Senders prefer to convey uncertainty using verbal (e.g., likely) rather than numeric (e.g., 75\% chance) probabilities, even in consequential domains, such as climate science. However, verbal probabilities can convey something other than uncertainty, and senders may exploit this. For instance, senders can maintain credibility after making erroneous predictions. While verbal probabilities afford ease of expression, they can be easily misunderstood, and the potential for miscommunication is not effectively mitigated by assigning (imprecise) numeric probabilities to words. When making consequential decisions, recipients prefer (precise) numeric probabilities.},
  keywords = {communication,imprecise probabilities,probability,risk,uncertainty,verbal probabilities},
  file = {/Users/thomasgorman/Zotero/storage/TG9MKEZQ/Dhami and Mandel - 2022 - Communicating uncertainty using words and numbers.pdf;/Users/thomasgorman/Zotero/storage/HH6PNUZE/S1364661322000602.html}
}

@article{dingInteractivityHumannessTrust2024,
  title = {Interactivity, Humanness, and Trust: A Psychological Approach to {{AI}} Chatbot Adoption in e-Commerce},
  shorttitle = {Interactivity, Humanness, and Trust},
  author = {Ding, Yi and Najaf, Muzammil},
  year = {2024},
  month = oct,
  journal = {BMC Psychology},
  volume = {12},
  number = {1},
  pages = {595},
  issn = {2050-7283},
  doi = {10.1186/s40359-024-02083-z},
  urldate = {2025-05-02},
  abstract = {This study aims to investigate the impact of interactivity and perceived humanness on trust toward AI chatbots in the e-commerce setting. Moreover, this study also aims to examine the mediation effect of trust toward AI chatbots in the relationship between interactivity and intention to adopt AI chatbots for e-commerce as well as in the relationship between perceived humanness and intention to adopt chatbots for e-commerce. This study used a time lag approach to collect the data from 343 customers from the southern region of China. The data were collected online through a questionnaire designed in Chinese language using a survey firm. The findings of this study indicated that there is a significant impact of interactivity and humanness on the trust toward chatbots. Moreover, the findings of this study indicated that there is a significant mediating effect of trust toward chatbots in the relationships of interactivity and perceived humanness to adopt chatbots for e-commerce. In addition, this study found a significant moderating influence on the perceived enjoyment of using chatbots in e-commerce settings. This study provides a unique perspective of expectation-confirmation theory for adopting emerging technologies for online shopping and also provides insights for designers and business firms to develop businesses to facilitate the AI chatbot feature for e-commerce.},
  keywords = {Expectation-confirmation theory,Intention to adopt Chatbot,Interactivity,Perceived enjoyment,Perceived humanness,Trust toward Chatbot},
  file = {/Users/thomasgorman/Zotero/storage/BT6VYK4V/Ding and Najaf - 2024 - Interactivity, humanness, and trust a psychological approach to AI chatbot adoption in e-commerce.pdf;/Users/thomasgorman/Zotero/storage/NHQB9LSQ/s40359-024-02083-z.html}
}

@article{doozandehQuantificationHumanConfidence2016,
  title = {Quantification of Human Confidence in Functional Relations},
  author = {Doozandeh, Pooyan},
  year = {2016},
  month = dec,
  journal = {Cognitive Systems Research},
  volume = {40},
  pages = {18--34},
  issn = {1389-0417},
  doi = {10.1016/j.cogsys.2016.02.001},
  urldate = {2022-05-27},
  abstract = {What makes people infer that two continuous-valued entities are functionally related? Involving factors influencing human confidence in the existence of a functional link between two supposed variables has not so far been discussed in function learning literature. By examining this problem and based on relevant results from cognitive psychology, I propose a hypothesis according to which human confidence in a link between cue and criterion is affected by three factors: The difficulty of functions, the level of noise in observed data, and the sample size. Here, the formalization of this hypothesis forms a novel mathematical model of function learning which can also be used for predictions; so the resulting model receives cue-criterion pairs of a supposed relation and produces two outputs: Confidence and predicting function. In an experiment, the performance of a computational implementation of the model is compared with human data. The results show that the model is successful in tracking changes in human confidence. A close correspondence between the predictions of the model and humans was also achieved.},
  langid = {english},
  keywords = {Confidence measurement,Function learning,Function recognition,Mathematical model},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/doozandeh_quantification_2016-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Doozandeh_2016_Quantification of human confidence in functional relations.pdf;/Users/thomasgorman/Zotero/storage/H3FVHVB7/S138904171530019X.html}
}

@misc{duroMeasuringIdentifyingFactors2025,
  title = {Measuring and Identifying Factors of Individuals' Trust in {{Large Language Models}}},
  author = {Duro, Edoardo Sebastiano De and Veltri, Giuseppe Alessandro and Golino, Hudson and Stella, Massimo},
  year = {2025},
  month = mar,
  number = {arXiv:2502.21028},
  eprint = {2502.21028},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.21028},
  urldate = {2025-03-21},
  abstract = {Large Language Models (LLMs) can engage in human-looking conversational exchanges. Although conversations can elicit trust between users and LLMs, scarce empirical research has examined trust formation in human-LLM contexts, beyond LLMs' trustworthiness or human trust in AI in general. Here, we introduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure individuals' trust in LLMs, extending McAllister's cognitive and affective trust dimensions to LLM-human interactions. We developed TILLMI as a psychometric scale, prototyped with a novel protocol we called LLM-simulated validity. The LLM-based scale was then validated in a sample of 1,000 US respondents. Exploratory Factor Analysis identified a two-factor structure. Two items were then removed due to redundancy, yielding a final 6-item scale with a 2-factor structure. Confirmatory Factor Analysis on a separate subsample showed strong model fit (\$CFI = .995\$, \$TLI = .991\$, \$RMSEA = .046\$, \$p\_\{X{\textasciicircum}2\} {$>$} .05\$). Convergent validity analysis revealed that trust in LLMs correlated positively with openness to experience, extraversion, and cognitive flexibility, but negatively with neuroticism. Based on these findings, we interpreted TILLMI's factors as "closeness with LLMs" (affective dimension) and "reliance on LLMs" (cognitive dimension). Younger males exhibited higher closeness with- and reliance on LLMs compared to older women. Individuals with no direct experience with LLMs exhibited lower levels of trust compared to LLMs' users. These findings offer a novel empirical foundation for measuring trust in AI-driven verbal communication, informing responsible design, and fostering balanced human-AI collaboration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/RM9JL3CF/Duro et al. - 2025 - Measuring and identifying factors of individuals' trust in Large Language Models.pdf;/Users/thomasgorman/Zotero/storage/JFCQ2P4Q/2502.html}
}

@article{dvorakAdverseReactionsUse2025,
  title = {Adverse Reactions to the Use of Large Language Models in Social Interactions},
  author = {Dvorak, Fabian and Stumpf, Regina and Fehrler, Sebastian and Fischbacher, Urs},
  year = {2025},
  month = apr,
  journal = {PNAS Nexus},
  volume = {4},
  number = {4},
  pages = {pgaf112},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgaf112},
  urldate = {2025-04-30},
  abstract = {Large language models (LLMs) are poised to reshape the way individuals communicate and interact. While this form of AI has the potential to efficiently make many human decisions, there is limited understanding of how individuals will respond to its use in social interactions. In particular, it remains unclear how individuals interact with LLMs when the interaction has consequences for other people. Here, we report the results of a large-scale, preregistered online experiment (n=3,552) showing that human players' fairness, trust, trustworthiness, cooperation, and coordination in economic two-player games decrease when the decision of the interaction partner is taken over by ChatGPT. On the contrary, we observe no adverse reactions when individuals are uncertain whether they are interacting with a human or a LLM. At the same time, participants often delegate decisions to the LLM, especially when the model's involvement is not disclosed, and individuals have difficulty distinguishing between decisions made by humans and those made by AI.},
  file = {/Users/thomasgorman/Zotero/storage/I7ACP96Y/Dvorak et al. - 2025 - Adverse reactions to the use of large language models in social interactions.pdf}
}

@misc{fangHowAIHuman2025,
  title = {How {{AI}} and {{Human Behaviors Shape Psychosocial Effects}} of {{Chatbot Use}}: {{A Longitudinal Randomized Controlled Study}}},
  shorttitle = {How {{AI}} and {{Human Behaviors Shape Psychosocial Effects}} of {{Chatbot Use}}},
  author = {Fang, Cathy Mengying and Liu, Auren R. and Danry, Valdemar and Lee, Eunhae and Chan, Samantha W. T. and Pataranutaporn, Pat and Maes, Pattie and Phang, Jason and Lampe, Michael and Ahmad, Lama and Agarwal, Sandhini},
  year = {2025},
  month = mar,
  number = {arXiv:2503.17473},
  eprint = {2503.17473},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.17473},
  urldate = {2025-04-30},
  abstract = {AI chatbots, especially those with voice capabilities, have become increasingly human-like, with more users seeking emotional support and companionship from them. Concerns are rising about how such interactions might impact users' loneliness and socialization with real people. We conducted a four-week randomized, controlled, IRB-approved experiment (n=981, {$>$}300K messages) to investigate how AI chatbot interaction modes (text, neutral voice, and engaging voice) and conversation types (open-ended, non-personal, and personal) influence psychosocial outcomes such as loneliness, social interaction with real people, emotional dependence on AI and problematic AI usage. Results showed that while voice-based chatbots initially appeared beneficial in mitigating loneliness and dependence compared with text-based chatbots, these advantages diminished at high usage levels, especially with a neutral-voice chatbot. Conversation type also shaped outcomes: personal topics slightly increased loneliness but tended to lower emotional dependence compared with open-ended conversations, whereas non-personal topics were associated with greater dependence among heavy users. Overall, higher daily usage - across all modalities and conversation types - correlated with higher loneliness, dependence, and problematic use, and lower socialization. Exploratory analyses revealed that those with stronger emotional attachment tendencies and higher trust in the AI chatbot tended to experience greater loneliness and emotional dependence, respectively. These findings underscore the complex interplay between chatbot design choices (e.g., voice expressiveness) and user behaviors (e.g., conversation content, usage frequency). We highlight the need for further research on whether chatbots' ability to manage emotional content without fostering dependence or replacing human relationships benefits overall well-being.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  annotation = {https://github.com/mitmedialab/chatbot-psychosocial-study},
  file = {/Users/thomasgorman/Zotero/storage/FYJJK8XH/Fang et al. - 2025 - How AI and Human Behaviors Shape Psychosocial Effects of Chatbot Use A Longitudinal Randomized Cont.pdf;/Users/thomasgorman/Zotero/storage/DIB2F23H/2503.html}
}

@article{flemingHowMeasureMetacognition2014,
  title = {How to Measure Metacognition},
  author = {Fleming, Stephen M. and Lau, Hakwan C.},
  year = {2014},
  month = jul,
  journal = {Frontiers in Human Neuroscience},
  volume = {8},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2014.00443},
  urldate = {2025-05-13},
  abstract = {The ability to recognize one's own successful cognitive processing, in e.g., perceptual or memory tasks, is often referred to as metacognition. How should we quantitatively measure such ability? Here we focus on a class of measures that assess the correspondence between trial-by-trial accuracy and one's own confidence. In general, for healthy subjects endowed with metacognitive sensitivity, when one is confident, one is more likely to be correct. Thus, the degree of association between accuracy and confidence can be taken as a quantitative measure of metacognition. However, many studies use a statistical correlation coefficient (e.g., Pearson's r) or its variant to assess this degree of association, and such measures are susceptible to undesirable influences from factors such as response biases. Here we review other measures based on signal detection theory and receiver operating characteristics (ROC) analysis that are ``bias free,'' and relate these quantities to the calibration and discrimination measures developed in the probability estimation literature. We go on to distinguish between the related concepts of metacognitive bias (a difference in subjective confidence despite basic task performance remaining constant), metacognitive sensitivity (how good one is at distinguishing between one's own correct and incorrect judgments) and metacognitive efficiency (a subject's level of metacognitive sensitivity given a certain level of task performance). Finally, we discuss how these three concepts pose interesting questions for the study of metacognition and conscious awareness.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/NYW2NPNJ/Fleming and Lau - 2014 - How to measure metacognition.pdf}
}

@article{frederickCharacterizingPerceptionsEnergy2011,
  title = {Characterizing Perceptions of Energy Consumption},
  author = {Frederick, Shane W. and Meyer, Andrew B. and Mochon, Daniel},
  year = {2011},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {8},
  pages = {E23-E23},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1014806108},
  urldate = {2025-05-05},
  abstract = {The adoption of energy-saving technologies is presumably deterred by underestimates of energy use and by corresponding underestimates of the difference between more-and lessefficient appliances. Thus, it is easy to grasp the potential policy significance of a recent study (1) concluding that Americans underestimate energy use by a factor of 2.8. However, the apparent precision of that statistic belies its arbitrary origins. By manipulating just two experimental details (the provided numeric referent and the units in which judgments were rendered), we show that one can readily reach qualitatively different conclusions.},
  file = {/Users/thomasgorman/Zotero/storage/XFTNG86D/Frederick et al. - 2011 - Characterizing perceptions of energy consumption.pdf}
}

@article{gerlichAIToolsSociety2025,
  title = {{{AI Tools}} in {{Society}}: {{Impacts}} on {{Cognitive Offloading}} and the {{Future}} of {{Critical Thinking}}},
  shorttitle = {{{AI Tools}} in {{Society}}},
  author = {Gerlich, Michael},
  year = {2025},
  month = jan,
  journal = {Societies},
  volume = {15},
  number = {1},
  pages = {6},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2075-4698},
  doi = {10.3390/soc15010006},
  urldate = {2025-01-27},
  abstract = {The proliferation of artificial intelligence (AI) tools has transformed numerous aspects of daily life, yet its impact on critical thinking remains underexplored. This study investigates the relationship between AI tool usage and critical thinking skills, focusing on cognitive offloading as a mediating factor. Utilising a mixed-method approach, we conducted surveys and in-depth interviews with 666 participants across diverse age groups and educational backgrounds. Quantitative data were analysed using ANOVA and correlation analysis, while qualitative insights were obtained through thematic analysis of interview transcripts. The findings revealed a significant negative correlation between frequent AI tool usage and critical thinking abilities, mediated by increased cognitive offloading. Younger participants exhibited higher dependence on AI tools and lower critical thinking scores compared to older participants. Furthermore, higher educational attainment was associated with better critical thinking skills, regardless of AI usage. These results highlight the potential cognitive costs of AI tool reliance, emphasising the need for educational strategies that promote critical engagement with AI technologies. This study contributes to the growing discourse on AI's cognitive implications, offering practical recommendations for mitigating its adverse effects on critical thinking. The findings underscore the importance of fostering critical thinking in an AI-driven world, making this research essential reading for educators, policymakers, and technologists.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {AI,AI tools,AI trust,artificial intelligence,cognitive development,cognitive offloading,critical thinking,digital dependence,Halpern Critical Thinking Assessment,technology and education},
  file = {/Users/thomasgorman/Zotero/storage/MX7CXN88/Gerlich - 2025 - AI Tools in Society Impacts on Cognitive Offloading and the Future of Critical Thinking.pdf}
}

@article{gigerenzer30ChanceRain2005,
  title = {``{{A}} 30\% {{Chance}} of {{Rain Tomorrow}}'': {{How Does}} the {{Public Understand Probabilistic Weather Forecasts}}?},
  shorttitle = {``{{A}} 30\% {{Chance}} of {{Rain Tomorrow}}''},
  author = {Gigerenzer, Gerd and Hertwig, Ralph and Van Den Broek, Eva and Fasolo, Barbara and Katsikopoulos, Konstantinos V.},
  year = {2005},
  journal = {Risk Analysis},
  volume = {25},
  number = {3},
  pages = {623--629},
  issn = {1539-6924},
  doi = {10.1111/j.1539-6924.2005.00608.x},
  urldate = {2025-05-12},
  abstract = {The weather forecast says that there is a ``30\% chance of rain,'' and we think we understand what it means. This quantitative statement is assumed to be unambiguous and to convey more information than does a qualitative statement like ``It might rain tomorrow.'' Because the forecast is expressed as a single-event probability, however, it does not specify the class of events it refers to. Therefore, even numerical probabilities can be interpreted by members of the public in multiple, mutually contradictory ways. To find out whether the same statement about rain probability evokes various interpretations, we randomly surveyed pedestrians in five metropolises located in countries that have had different degrees of exposure to probabilistic forecasts----Amsterdam, Athens, Berlin, Milan, and New York. They were asked what a ``30\% chance of rain tomorrow'' means both in a multiple-choice and a free-response format. Only in New York did a majority of them supply the standard meteorological interpretation, namely, that when the weather conditions are like today, in 3 out of 10 cases there will be (at least a trace of) rain the next day. In each of the European cities, this alternative was judged as the least appropriate. The preferred interpretation in Europe was that it will rain tomorrow ``30\% of the time,'' followed by ``in 30\% of the area.'' To improve risk communication with the public, experts need to specify the reference class, that is, the class of events to which a single-event probability refers.},
  langid = {english},
  keywords = {Cultural differences,risk communication,single-event probabilities,weather forecasts},
  file = {/Users/thomasgorman/Zotero/storage/7HIAJEL6/Gigerenzer et al. - 2005 - “A 30% Chance of Rain Tomorrow” How Does the Public Understand Probabilistic Weather Forecasts.pdf;/Users/thomasgorman/Zotero/storage/FMKKRNSB/j.1539-6924.2005.00608.html}
}

@article{gliksonHumanTrustArtificial2020,
  title = {Human {{Trust}} in {{Artificial Intelligence}}: {{Review}} of {{Empirical Research}}},
  shorttitle = {Human {{Trust}} in {{Artificial Intelligence}}},
  author = {Glikson, Ella and Woolley, Anita Williams},
  year = {2020},
  month = jul,
  journal = {Academy of Management Annals},
  volume = {14},
  number = {2},
  pages = {627--660},
  issn = {1941-6520, 1941-6067},
  doi = {10.5465/annals.2018.0057},
  urldate = {2025-05-02},
  abstract = {Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into organizations critically depends on workers' trust in AI technology. This review explains how AI differs from other technologies and presents the existing empirical research on the determinants of human ``trust'' in AI, conducted in multiple disciplines over the last 20 years. Based on the reviewed literature, we identify the form of AI representation (robot, virtual, and embedded) and its level of machine intelligence (i.e., its capabilities) as important antecedents to the development of trust and propose a framework that addresses the elements that shape users' cognitive and emotional trust. Our review reveals the important role of AI's tangibility, transparency, reliability, and immediacy behaviors in developing cognitive trust, and the role of AI's anthropomorphism specifically for emotional trust. We also note several limitations in the current evidence base, such as the diversity of trust measures and overreliance on short-term, small sample, and experimental studies, where the development of trust is likely to be different than in longer-term, higher stakes field environments. Based on our review, we suggest the most promising paths for future research.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/UBRDBYYV/Glikson and Woolley - 2020 - Human Trust in Artificial Intelligence Review of Empirical Research.pdf}
}

@article{gnambsAttitudesExperiencesUsage2025,
  title = {Attitudes, Experiences, and Usage Intentions of Artificial Intelligence: {{A}} Population Study in {{Germany}}},
  shorttitle = {Attitudes, Experiences, and Usage Intentions of Artificial Intelligence},
  author = {Gnambs, Timo and Stein, Jan-Philipp and Zinn, Sabine and Griese, Florian and Appel, Markus},
  year = {2025},
  month = apr,
  journal = {Telematics and Informatics},
  volume = {98},
  pages = {102265},
  issn = {0736-5853},
  doi = {10.1016/j.tele.2025.102265},
  urldate = {2025-04-30},
  abstract = {Artificial intelligence (AI) increasingly affects individuals' private and professional lives. Importantly, both the acceptance and adoption of new AI technologies in society is heavily impacted by the attitudes that people hold; yet, there is currently limited information on how people perceive and intend to use AI at the national and demographic levels. Therefore, this study examined a random sample of 1,098 German adults to assess their attitudes, experiences, and usage intentions regarding AI in work, healthcare, and education. The findings indicated that respondents generally held favorable attitudes towards AI, with AI applications in healthcare receiving more positive evaluations than AI in the context of work. Moreover, cognitive evaluations of AI were more positive than emotional or behavioral appraisals. Prior experiences with AI were, however, limited, particularly in healthcare and education. Demographic differences were generally small. Taken together, these findings demonstrate that, in Germany, AI is currently widely accepted in different domains, although most people have little first-hand experience with it. These insights can inform policymakers and stakeholders who care about the proliferation of AI in society.},
  keywords = {Artificial intelligence,Attitudes,Experience,Germany,Social survey},
  annotation = {https://osf.io/eha58/files/osfstorage\#\\
\\
https://ars-els-cdn-com.proxyiub.uits.iu.edu/content/image/1-s2.0-S0736585325000279-mmc1.pdf},
  file = {/Users/thomasgorman/Zotero/storage/5URMG7S8/Gnambs et al. - 2025 - Attitudes, experiences, and usage intentions of artificial intelligence A population study in Germa.pdf;/Users/thomasgorman/Zotero/storage/VAHFK223/S0736585325000279.html}
}

@article{grassiniDevelopmentValidationAI2023,
  title = {Development and Validation of the {{AI}} Attitude Scale ({{AIAS-4}}): A Brief Measure of General Attitude toward Artificial Intelligence},
  shorttitle = {Development and Validation of the {{AI}} Attitude Scale ({{AIAS-4}})},
  author = {Grassini, Simone},
  year = {2023},
  month = jul,
  journal = {Frontiers in Psychology},
  volume = {14},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2023.1191628},
  urldate = {2025-05-01},
  abstract = {The rapid advancement of artificial intelligence (AI) has generated an increasing demand for tools that can assess public attitudes toward AI. This study proposes the development and the validation of the AI Attitude Scale (AIAS), a concise self-report instrument designed to evaluate public perceptions of AI technology. The first version of the AIAS that the present manuscript proposes comprises five items, including one reverse-scored item, which aims to gauge individuals' beliefs about AI's influence on their lives, careers, and humanity overall. The scale is designed to capture attitudes toward AI, focusing on the perceived utility and potential impact of technology on society and humanity. The psychometric properties of the scale were investigated using diverse samples in two separate studies. An exploratory factor analysis was initially conducted on a preliminary 5-item version of the scale. Such exploratory validation study revealed the need to divide the scale into two factors. While the results demonstrated satisfactory internal consistency for the overall scale and its correlation with related psychometric measures, separate analyses for each factor showed robust internal consistency for Factor 1 but insufficient internal consistency for Factor 2. As a result, a second version of the scale is developed and validated, omitting the item that displayed weak correlation with the remaining items in the questionnaire. The refined final 1-factor, 4-item AIAS demonstrated superior overall internal consistency compared to the initial 5-item scale and the proposed factors. Further confirmatory factor analyses, performed on a different sample of participants, confirmed that the 1-factor model (4-items) of the AIAS exhibited an adequate fit to the data, providing additional evidence for the scale's structural validity and generalizability across diverse populations. In conclusion, the analyses reported in this article suggest that the developed and validated 4-items AIAS can be a valuable instrument for researchers and professionals working on AI development who seek to understand and study users' general attitudes toward AI.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1191628/full\#supplementary-material},
  file = {/Users/thomasgorman/Zotero/storage/92ZW78Q2/Grassini - 2023 - Development and validation of the AI attitude scale (AIAS-4) a brief measure of general attitude to.pdf}
}

@inproceedings{grassiniPsychometricValidationPAILQ62024,
  title = {A {{Psychometric Validation}} of the {{PAILQ-6}}: {{Perceived Artificial Intelligence Literacy Questionnaire}}},
  shorttitle = {A {{Psychometric Validation}} of the {{PAILQ-6}}},
  booktitle = {Nordic {{Conference}} on {{Human-Computer Interaction}}},
  author = {Grassini, Simone},
  year = {2024},
  month = oct,
  pages = {1--10},
  publisher = {ACM},
  address = {Uppsala Sweden},
  doi = {10.1145/3679318.3685359},
  urldate = {2025-05-02},
  abstract = {The present article introduces and implements an initial validation for the Perceived Artificial Intelligence Literacy Questionnaire (PAILQ-6), a brief tool designed to assess individuals' self-perceived AI literacy. Amidst the growing integration of AI in various aspects of life and its ethical implications, understanding AI becomes crucial for effective interaction with AI technologies. The PAILQ-6 emerges in response to the need for an accessible instrument that evaluates general AI literacy without compromising on clarity or depth, suitable for both academic and practical applications. This paper presents the development process of the PAILQ-6, consisting of six items derived from established components of AI literacy, structured as a seven-point Likert scale for easy administration and digital compatibility. The validation study was conducted from data of a gender-balanced sample of 232 UK adults. The article demonstrates the PAILQ-6's reliability and validity through exploratory factor analysis, showing a two-factor structure. The findings reveal the scale's good internal consistency and convergent validity. The study highlights demographic predictors of AI literacy perceptions, indicating a possible gender disparity and the positive influence of higher education on perceived AI competency.},
  isbn = {979-8-4007-0966-1},
  langid = {english},
  annotation = {https://zenodo.org/records/10888785/files/PAILQ-6\_questionnaire\_and\_scoring.pdf.},
  file = {/Users/thomasgorman/Zotero/storage/GH27XXUR/Grassini - 2024 - A Psychometric Validation of the PAILQ-6 Perceived Artificial Intelligence Literacy Questionnaire.pdf}
}

@article{gronauHowYouKnow2024,
  title = {How Do You Know That You Don't Know?},
  author = {Gronau, Quentin F. and Steyvers, Mark and Brown, Scott D.},
  year = {2024},
  month = aug,
  journal = {Cognitive Systems Research},
  volume = {86},
  pages = {101232},
  issn = {1389-0417},
  doi = {10.1016/j.cogsys.2024.101232},
  urldate = {2024-11-10},
  abstract = {Whenever someone in a team tries to help others, it is crucial that they have some understanding of other team members' goals. In modern teams, this applies equally to human and artificial (``bot'') assistants. Understanding when one does not know something is crucial for stopping the execution of inappropriate behavior and, ideally, attempting to learn more appropriate actions. From a statistical point of view, this can be translated to assessing whether none of the hypotheses in a considered set is correct. Here we investigate a novel approach for making this assessment based on monitoring the maximum a posteriori probability (MAP) of a set of candidate hypotheses as new observations arrive. Simulation studies suggest that this is a promising approach, however, we also caution that there may be cases where this is more challenging. The problem we study and the solution we propose are general, with applications well beyond human--bot teaming, including for example the scientific process of theory development.},
  keywords = {Maximum a posteriori probability,Model mis-specification,Model uncertainty},
  annotation = {https://osf.io/t4n63/},
  file = {/Users/thomasgorman/Zotero/storage/HFNTHSID/Gronau et al. - 2024 - How do you know that you don’t know.pdf;/Users/thomasgorman/Zotero/storage/3DSW8BCL/S1389041724000263.html}
}

@article{gronerInvestigatingImpactUser2024,
  title = {Investigating the {{Impact}} of {{User Interface Designs}} on {{Expectations About Large Language Models}}' {{Capabilities}}},
  author = {Gr{\"o}ner, Felix and Chiou, Erin K.},
  year = {2024},
  month = sep,
  journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  volume = {68},
  number = {1},
  pages = {155--161},
  publisher = {SAGE Publications Inc},
  issn = {1071-1813},
  doi = {10.1177/10711813241260399},
  urldate = {2025-04-30},
  abstract = {Large Language Models (LLMs) with their novel conversational interaction format could create incorrectly calibrated expectations about their capabilities. The present study investigates human expectations toward a generic LLM?s capabilities and limitations. Participants of an online study were shown a series of prompts that cover a wide range of tasks and asked to assess the likelihood of the LLM being able to help with those tasks. The result is a catalog of people?s general expectations of LLM capabilities across various task domains. Depending on the actual capabilities of a specific system, this could inform developers of potential over- or under-reliance on this technology due to these misconceptions. To explore a potential way of correcting misconceptions we also attempted to manipulate their expectations with three different interface designs. In most of the tested task domains, such as computation and text processing, however, these seem to be insufficient to overpower people?s initial expectations.},
  file = {/Users/thomasgorman/Zotero/storage/KDGIYGNT/Gröner and Chiou - 2024 - Investigating the Impact of User Interface Designs on Expectations About Large Language Models’ Capa.pdf}
}

@article{guExploringMechanismSustained2024,
  title = {Exploring the Mechanism of Sustained Consumer Trust in {{AI}} Chatbots after Service Failures: A Perspective Based on Attribution and {{CASA}} Theories},
  shorttitle = {Exploring the Mechanism of Sustained Consumer Trust in {{AI}} Chatbots after Service Failures},
  author = {Gu, Chenyu and Zhang, Yu and Zeng, Linhao},
  year = {2024},
  month = oct,
  journal = {Humanities and Social Sciences Communications},
  volume = {11},
  number = {1},
  pages = {1--12},
  publisher = {Palgrave},
  issn = {2662-9992},
  doi = {10.1057/s41599-024-03879-5},
  urldate = {2025-05-02},
  abstract = {In recent years, artificial intelligence (AI) technology has been widely employed in brand customer service. However, the inherent limitations of computer-generated natural language content occasionally lead to failures in human-computer interactions, potentially damaging a company's brand image. Therefore, it is crucial to explore how to maintain consumer trust after AI chatbots fail to provide successful service. This study constructs a model to examine the impact of social interaction cues and anthropomorphic factors on users' sustained trust by integrating the Computers As Social Actors (CASA) theory with attribution theory. An empirical analysis of 462 survey responses reveals that CASA factors (perceived anthropomorphic characteristics, perceived empathic abilities, and perceived interaction quality) can effectively enhance user trust in AI customer service following interaction failures. This process of sustaining trust is mediated through different attributions of failure. Furthermore, AI anxiety, as a cognitive characteristic of users, not only negatively impacts sustained trust but also significantly moderates the effect of internal attributions on sustained trust. These findings expand the research domain of human-computer interaction and provide insights for the practical development of AI chatbots in communication and customer service fields.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Business and management,Information systems and information technology,Psychology},
  file = {/Users/thomasgorman/Zotero/storage/B9TWLBCQ/Gu et al. - 2024 - Exploring the mechanism of sustained consumer trust in AI chatbots after service failures a perspec.pdf}
}

@inproceedings{guoExploringImpactAI2024,
  title = {Exploring the {{Impact}} of {{AI Value Alignment}} in {{Collaborative Ideation}}: {{Effects}} on {{Perception}}, {{Ownership}}, and {{Output}}},
  shorttitle = {Exploring the {{Impact}} of {{AI Value Alignment}} in {{Collaborative Ideation}}},
  booktitle = {Extended {{Abstracts}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Guo, Alicia and Pataranutaporn, Pat and Maes, Pattie},
  year = {2024},
  month = may,
  pages = {1--11},
  publisher = {ACM},
  address = {Honolulu HI USA},
  doi = {10.1145/3613905.3650892},
  urldate = {2025-04-30},
  annotation = {https://8values.github.io/},
  file = {/Users/thomasgorman/Zotero/storage/9CR4X2LH/Guo et al. - 2024 - Exploring the Impact of AI Value Alignment in Collaborative Ideation Effects on Perception, Ownersh.pdf}
}

@article{habibiasgarabadPromotingElectricityConservation2024,
  title = {Promoting Electricity Conservation through Behavior Change: {{A}} Study Protocol for a Web-Based Multiple-Arm Parallel Randomized Controlled Trial},
  shorttitle = {Promoting Electricity Conservation through Behavior Change},
  author = {Habibi Asgarabad, Mojtaba and Vesely, Stepan and Efe Biresselioglu, Mehmet and Caffaro, Federica and Carrus, Giuseppe and Hakan Demir, Muhittin and Kirchler, Benjamin and Kollmann, Andrea and Massullo, Chiara and Tiberio, Lorenza and Kl{\"o}ckner, Christian A.},
  year = {2024},
  month = mar,
  journal = {PLOS ONE},
  volume = {19},
  number = {3},
  pages = {e0293683},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0293683},
  urldate = {2024-12-18},
  abstract = {Background and aims As a part of the framework of the EU-funded Energy efficiency through Behavior CHANge Transition (ENCHANT) project, the present paper intends to provide a ``Research Protocol'' of a web-based trial to: (i) assess the effectiveness of behavioral intervention strategies----either single or in combination----on electricity saving, and (ii) unravel the psychological factors contributing to intervention effectiveness in households across Europe. Methods and materials Six distinct interventions (i.e., information provision, collective vs. individual message framing, social norms, consumption feedback, competitive elements, and commitment strategies) targeting electricity saving in households from six European countries (i.e., Austria, Germany, Italy, Norway, Romania, and T{\"u}rkiye) are evaluated, with an initial expected samples of about 1500 households per country randomly assigned to 12 intervention groups and two control groups, and data is collected through an ad-hoc online platform. The primary outcome is the weekly electricity consumption normalized to the last seven days before measurement per person per household. Secondary outcomes are the peak consumption during the last day before measurement and the self-reported implementation of electricity saving behaviors (e.g., deicing the refrigerator). The underlying psychological factors expected to mediate and/or moderate the intervention effects on these outcomes are intentions to save electricity, perceived difficulty of saving energy, attitudes to electricity saving, electricity saving habit strength, social norms to save electricity, personal norms, collective efficacy, emotional reaction to electricity consumption, and national identity. The intervention effectiveness will be evaluated by comparing psychological factors and consumption variables before and after the intervention, leading to a 14 (groups including 2 control groups) {\texttimes} 6 (time) mixed factorial design, with one factor between (group) and one factor within subjects (time)--6 measurements of the psychological factors and 6 readings of the electricity meters, which gives then 5 weeks of electricity consumption. Results Data collection for the present RCT started in January 2023, and by October 2023 data collection will conclude. Discussion Upon establishing feasibility and effectiveness, the outcomes of this study will assist policymakers, municipalities, NGOs, and other communal entities in identifying impactful interventions tailored to their unique circumstances and available resources. Researchers will benefit from a flexible, structured tool that allows the design, implementation and monitoring of complex interventions protocols. Crucially, the intervention participants will benefit from electricity saving strategies, fostering immediate effectiveness of the interventions in real-life contexts. Trial registration This trial was preregistered in the Open Science Framework: https://osf.io/9vtn4.},
  annotation = {https://osf.io/9vtn4},
  file = {/Users/thomasgorman/Zotero/storage/P4WRSJKZ/Habibi Asgarabad et al. - 2024 - Promoting electricity conservation through behavior change A study protocol for a web-based multipl.pdf}
}

@misc{hoffmanMetricsExplainableAI2019,
  title = {Metrics for {{Explainable AI}}: {{Challenges}} and {{Prospects}}},
  shorttitle = {Metrics for {{Explainable AI}}},
  author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
  year = {2019},
  month = feb,
  number = {arXiv:1812.04608},
  eprint = {1812.04608},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1812.04608},
  urldate = {2025-05-03},
  abstract = {The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/thomasgorman/Zotero/storage/4F6QCBP4/Hoffman et al. - 2019 - Metrics for Explainable AI Challenges and Prospects.pdf;/Users/thomasgorman/Zotero/storage/WV6J267W/1812.html}
}

@misc{holfordErringSideCaution2022,
  title = {Erring on the Side of Caution: {{People}} Misperceive Energy Consumption Metrics More When Energy Goals Are Not Achieved},
  shorttitle = {Erring on the Side of Caution},
  author = {Holford, Dawn and Jolles, Daniel and Juanchich, Marie and Buchanan, Kathryn},
  year = {2022},
  month = jul,
  publisher = {In Review},
  doi = {10.21203/rs.3.rs-1799156/v1},
  urldate = {2025-05-12},
  abstract = {Tackling the energy crisis will require a concerted effort to reduce energy consumption. Setting energy goals to meet could help people to adjust their energy consumption. However, people will struggle to know how well they are doing with respect to the goal unless they receive meaningful energy feedback that they are able to interpret consistently and accurately---which energy displays may not support. In this paper, we report four experiments (total n = 2086) that showed participants systematically misperceived the same xed energy quantity shown on a visual analogue gauge depending on whether they were achieving or not achieving their energy goal and whether the energy display used kilowatts, currency, or carbon dioxide emission units. We found that participants tended to provide underestimates of quantities ---whether these are amounts of energy consumed or saved---and they were extra cautious with estimations when the display signalled that they were performing poorly.},
  archiveprefix = {In Review},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  annotation = {https://osf.io/m7g9e/files/osfstorage?view\_only=c681b4a00ed2492e8de5ef214b9ac4c7},
  file = {/Users/thomasgorman/Zotero/storage/EQ5HEBMC/Holford et al. - 2022 - Erring on the side of caution People misperceive energy consumption metrics more when energy goals.pdf}
}

@article{houMeasuringUndergraduateStudents2025,
  title = {Measuring Undergraduate Students' Reliance on {{Generative AI}} during Problem-Solving: {{Scale}} Development and Validation},
  shorttitle = {Measuring Undergraduate Students' Reliance on {{Generative AI}} during Problem-Solving},
  author = {Hou, Chenyu and Zhu, Gaoxia and Sudarshan, Vidya and Lim, Fun Siong and Ong, Yew Soon},
  year = {2025},
  month = sep,
  journal = {Computers \& Education},
  volume = {234},
  pages = {105329},
  issn = {0360-1315},
  doi = {10.1016/j.compedu.2025.105329},
  urldate = {2025-04-30},
  abstract = {Reliance on AI describes the behavioral patterns of when and how individuals depend on AI suggestions, and appropriate reliance patterns are necessary to achieve effective human-AI collaboration. Traditional measures often link reliance to decision-making outcomes, which may not be suitable for complex problem-solving tasks where outcomes are not binary (i.e., correct or incorrect) or immediately clear. Therefore, this study aims to develop a scale to measure undergraduate students' behaviors of using Generative AI during problem-solving tasks without directly linking them to specific outcomes. We conducted an exploratory factor analysis on 800 responses collected after students finished one problem-solving activity, which revealed four distinct factors: reflective use, cautious use, thoughtless use, and collaborative use. The overall scale has reached sufficient internal reliability (Cronbach's alpha~=~.84). Two confirmatory factor analyses (CFAs) were conducted to validate the factors using the remaining 730 responses from this activity and 1173 responses from another problem-solving activity. CFA indices showed adequate model fit for data from both problem-solving tasks, suggesting that the scale can be applied to various human-AI problem-solving tasks. This study offers a validated scale to measure students' reliance behaviors in different human-AI problem-solving activities and provides implications for educators to responsively integrate Generative AI in higher education.},
  keywords = {Generative AI,Higher education,Human-AI collaboration,Problem-solving,Reliance on AI,Scale development},
  file = {/Users/thomasgorman/Zotero/storage/B2I2DJ4A/Hou et al. - 2025 - Measuring undergraduate students' reliance on Generative AI during problem-solving Scale developmen.pdf;/Users/thomasgorman/Zotero/storage/379UE2YJ/S0360131525000971.html}
}

@article{huebnerUnderstandingElectricityConsumption2016,
  title = {Understanding Electricity Consumption: {{A}} Comparative Contribution of Building Factors, Socio-Demographics, Appliances, Behaviours and Attitudes},
  shorttitle = {Understanding Electricity Consumption},
  author = {Huebner, Gesche and Shipworth, David and Hamilton, Ian and Chalabi, Zaid and Oreszczyn, Tadj},
  year = {2016},
  month = sep,
  journal = {Applied Energy},
  volume = {177},
  pages = {692--702},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2016.04.075},
  urldate = {2025-05-12},
  abstract = {This paper tests to what extent different types of variables (building factors, socio-demographics, appliance ownership and use, attitudes and self-reported behaviours) explain annualized electricity consumption in residential buildings with gas-fuelled space and water heating. It then shows which individual variables have the highest explanatory power. In contrast to many other studies, the study recognizes the problem of multicollinearity between predictors in regression analysis and uses Lasso regression to address this issue. Using data from a sample of 845 English households collected in 2011/12, a comparison of four separate regression models showed that a model with the predictors of appliance ownership and use, including lighting, explained the largest share, 34\%, of variability in electricity consumption. Socio-demographic variables on their own explained about 21\% of the variability in electricity consumption with household size the most important predictor. Building variables only played a small role, presumably because heating energy consumption is not included, with only building size being a significant predictor. Self-reported energy-related behaviour, opinions about climate change and `green lifestyle' were negligible. A combined model, encompassing all predictors, explained only 39\% of all variability (adjusted R2=34\%), i.e. adding little above an appliance and lighting model only. Appliance variables together with household size and dwelling size were consistently significant predictors of energy consumption. The study highlights that when attempting to explain English household non-heating electricity consumption that variables directly influenced by people in the household have the strongest predictive power when taken together.},
  keywords = {Appliances,Electricity demand,English homes,Gas-central heating,Lasso regression,Multicollinearity,Residential buildings,Socio-demographics},
  file = {/Users/thomasgorman/Zotero/storage/ACBDY6PV/Huebner et al. - 2016 - Understanding electricity consumption A comparative contribution of building factors, socio-demogra.pdf;/Users/thomasgorman/Zotero/storage/LAN6HKIS/S0306261916305360.html}
}

@article{jenkinsMaintainingCredibilityWhen2021,
  title = {Maintaining Credibility When Communicating Uncertainty: The Role of Directionality},
  shorttitle = {Maintaining Credibility When Communicating Uncertainty},
  author = {Jenkins, Sarah C. and Harris, Adam J. L.},
  year = {2021},
  month = jan,
  journal = {Thinking \& Reasoning},
  publisher = {Routledge},
  issn = {1354-6783},
  urldate = {2025-05-12},
  abstract = {Risk communicators are often tasked with providing an accurate and informative estimate of the likelihood of an uncertain event (Fong, Rempel, \& Hall, 1999). For such communications to be effective...},
  copyright = {{\copyright} 2020 Informa UK Limited, trading as Taylor \& Francis Group},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/ND4DT7S2/13546783.2020.html}
}

@misc{jinGLATGenerativeAI2024,
  title = {{{GLAT}}: {{The Generative AI Literacy Assessment Test}}},
  shorttitle = {{{GLAT}}},
  author = {Jin, Yueqiao and {Martinez-Maldonado}, Roberto and Ga{\v s}evi{\'c}, Dragan and Yan, Lixiang},
  year = {2024},
  month = nov,
  number = {arXiv:2411.00283},
  eprint = {2411.00283},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.00283},
  urldate = {2025-05-02},
  abstract = {The rapid integration of generative artificial intelligence (GenAI) technology into education necessitates precise measurement of GenAI literacy to ensure that learners and educators possess the skills to engage with and critically evaluate this transformative technology effectively. Existing instruments often rely on self-reports, which may be biased. In this study, we present the GenAI Literacy Assessment Test (GLAT), a 20-item multiple-choice instrument developed following established procedures in psychological and educational measurement. Structural validity and reliability were confirmed with responses from 355 higher education students using classical test theory and item response theory, resulting in a reliable 2-parameter logistic (2PL) model (Cronbach's alpha = 0.80; omega total = 0.81) with a robust factor structure (RMSEA = 0.03; CFI = 0.97). Critically, GLAT scores were found to be significant predictors of learners' performance in GenAI-supported tasks, outperforming self-reported measures such as perceived ChatGPT proficiency and demonstrating external validity. These results suggest that GLAT offers a reliable and valid method for assessing GenAI literacy, with the potential to inform educational practices and policy decisions that aim to enhance learners' and educators' GenAI literacy, ultimately equipping them to navigate an AI-enhanced future.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/UEKP27M7/Jin et al. - 2024 - GLAT The Generative AI Literacy Assessment Test.pdf;/Users/thomasgorman/Zotero/storage/B7ZHQRME/2411.html}
}

@article{jiun-yinjianFoundationsEmpiricallyDetermined2000,
  title = {Foundations for an {{Empirically Determined Scale}} of {{Trust}} in {{Automated System}}},
  author = {{Jiun-Yin Jian} and Bisantz, Ann M. and Drury, Colin G.},
  year = {2000},
  month = jan,
  journal = {International Journal of Cognitive Ergonomics},
  volume = {4},
  number = {1},
  pages = {53},
  issn = {10886362},
  doi = {10.1207/S15327566IJCE0401_04},
  urldate = {2025-04-30},
  abstract = {One component in the successful use of automated systems is the extent to which people trust the automation to perform effectively. In order to understand the relationship between trust in computerized systems and the use of those systems, we need to be able to effectively measure trust. Although questionnaires regarding trust have been used in prior studies, these questionnaires were theoretically rather than empirically generated and did not distinguish between 3 potentially different types of trust: human--human trust, human--machine trust, and trust in general. A 3-phased experiment, comprising a word elicitation study, a questionnaire study, and a paired comparison study, was performed to better understand similarities and differences in the concepts of trust and distrust, and among the different types of trust. Results indicated that trust and distrust can be considered opposites, rather than different concepts. Components of trust, in terms of words related to trust, were similar across the three types of trust. Results obtained from a cluster analysis were used to identify 12 potential factors of trust between people and automated systems. These 12 factors were then used to develop a proposed scale to measure trust in automation.},
  keywords = {Human-machine systems,Trust},
  annotation = {https://osf.io/7cdne/files/osfstorage?view\_only=ad812bc898154990959a50aaea43ca61\#\\
\\
https://osf.io/9ht7s?view\_only=ad812bc898154990959a50aaea43ca61},
  file = {/Users/thomasgorman/Zotero/storage/WGDQN6E2/Jiun-Yin Jian et al. - 2000 - Foundations for an Empirically Determined Scale of Trust in Automated System.pdf}
}

@article{juanchichAmUncertainVs2017,
  title = {``{{I}} Am Uncertain'' vs ``{{It}} Is Uncertain''. {{How}} Linguistic Markers of the Uncertainty Source Affect Uncertainty Communication.},
  author = {Juanchich, Marie and {Gourdon-Kanhukamwe}, Am{\'e}lie and Sirota, Miroslav},
  year = {2017},
  month = sep,
  journal = {Judgment and Decision Making},
  volume = {12},
  number = {5},
  pages = {445--465},
  issn = {1930-2975},
  doi = {10.1017/S1930297500006483},
  urldate = {2025-05-13},
  abstract = {Abstract             Two psychological sources of uncertainty bear implications for judgment and decision-making: external uncertainty is seen as stemming from properties of the world, whereas internal uncertainty is seen as stemming from lack of knowledge. The apparent source of uncertainty can be conveyed through linguistic markers, such as the pronoun of probability phrases (e.g., I am uncertain vs. It is uncertain). Here, we investigated whether and when speakers use different pronoun subjects as such linguistic markers (Exp. 1 and 2) and what hearers infer from them (Exp. 3 and 4). Speakers more often described higher probabilities and knowable outcomes with internal probability phrases. In dialogue, speakers mirrored the source of their conversational partner. Markers of the source had a main effect or interacted with the probability conveyed and speaker expertise to shape the judgments and decisions of hearers. For example, experts voicing an internal probability phrase were judged as more knowledgeable than experts using an external probability phrase whereas the result was the opposite for lay speakers. We discuss how these findings inform our understanding of subjective uncertainty and uncertainty communication theories.},
  copyright = {http://creativecommons.org/licenses/by-nc-nd/3.0/},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/HCTUSDRT/Juanchich et al. - 2017 - “I am uncertain” vs “It is uncertain”. How linguistic markers of the uncertainty source affect uncer.pdf}
}

@article{kantenbacherBetterRulesJudging2021,
  title = {Better Rules for Judging Joules: {{Exploring}} How Experts Make Decisions about Household Energy Use},
  shorttitle = {Better Rules for Judging Joules},
  author = {Kantenbacher, Joseph and Attari, Shahzeen Z.},
  year = {2021},
  month = mar,
  journal = {Energy Research \& Social Science},
  volume = {73},
  pages = {101911},
  issn = {2214-6296},
  doi = {10.1016/j.erss.2021.101911},
  urldate = {2024-07-03},
  abstract = {Public understanding of home energy use is rife with biases and misunderstandings that can stymie the adoption of efficient technologies and conservation practices. Studying how energy experts make energy-related judgments can help design decision support tools to correct misperceptions held by novices. Here we conduct interviews with electrical engineers (n~=~10), physicists (n~=~10), and energy analysts (n~=~10) to document expert judgments about energy use and to identify their cognitive shortcuts (heuristics) for household energy decision making. Performance on an energy estimation task confirmed that energy experts have more accurate estimates of home energy use than novices. We document 24 unique expert heuristics related to device functions, components, and observable cues used by experts while making energy-use judgments. A follow-up survey with the experts indicated that these expert heuristics are generally more accurate than novice heuristics. The library of heuristics created in this study can be useful additions to education programs designed to improve public energy literacy and decision making.},
  keywords = {Cognitive shortcuts,Decision support,Estimation,Expert elicitation,Interviews,Perception},
  annotation = {https://www.szattari.com/publications},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kantenbacher_Attari_2021_Better rules for judging joules.pdf;/Users/thomasgorman/Zotero/storage/UML45G5G/S2214629621000049.html}
}

@misc{kimImNotSure2024,
  title = {"{{I}}'m {{Not Sure}}, {{But}}...": {{Examining}} the {{Impact}} of {{Large Language Models}}' {{Uncertainty Expression}} on {{User Reliance}} and {{Trust}}},
  shorttitle = {"{{I}}'m {{Not Sure}}, {{But}}..."},
  author = {Kim, Sunnie S. Y. and Liao, Q. Vera and Vorvoreanu, Mihaela and Ballard, Stephanie and Vaughan, Jennifer Wortman},
  year = {2024},
  month = may,
  number = {arXiv:2405.00623},
  eprint = {2405.00623},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.00623},
  urldate = {2024-10-12},
  abstract = {Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs' expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants' reliance, trust, and overall task performance. We find that first-person expressions (e.g., "I'm not sure, but...") decrease participants' confidence in the system and tendency to agree with the system's answers, while increasing participants' accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., "It's not clear, but..."), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  annotation = {https://osf.io/mnrp9},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kim et al_2024_I'm Not Sure, But.pdf;/Users/thomasgorman/Zotero/storage/484BYZDB/2405.html}
}

@article{kochMetaAILiteracy2024,
  title = {Meta {{AI}} Literacy Scale: {{Further}} Validation and Development of a Short Version},
  shorttitle = {Meta {{AI}} Literacy Scale},
  author = {Koch, Martin J. and Carolus, Astrid and Wienrich, Carolin and Latoschik, Marc E.},
  year = {2024},
  month = nov,
  journal = {Heliyon},
  volume = {10},
  number = {21},
  pages = {e39686},
  issn = {24058440},
  doi = {10.1016/j.heliyon.2024.e39686},
  urldate = {2025-04-30},
  abstract = {The concept of AI literacy, its promotion, and measurement are important topics as they prepare society for the steadily advancing spread of AI technology. The first purpose of the current study is to advance the measurement of AI literacy by collecting evidence regarding the validity of the Meta AI Literacy Scale (MAILS) by Carolus and colleagues published in 2023: a self-assessment instrument for AI literacy and additional psychological competencies conducive for the use of AI. For this purpose, we first formulated the intended measurement purposes of the MAILS. In a second step, we derived empirically testable axioms and subaxioms from the purposes. We tested them in several already published and newly collected data sets. The results are presented in the form of three different empirical studies. We found overall evidence for the validity of the MAILS with some unexpected findings that require further research. We discuss the results for each study individually and also together. Also, avenues for future research are discussed. The study's second purpose is to develop a short version (10 items) of the original instrument (34 items). It was possible to find a selection of ten items that represent the factors of the MAILS and show a good model fit when tested with confirmatory factor analysis. Further research will be needed to validate the short scale. This paper advances the knowledge about the validity and provides a short measure for AI literacy. However, more research will be necessary to further our understanding of the relationships between AI literacy and other constructs.},
  langid = {english},
  annotation = {https://downloads.hci.informatik.uni-wuerzburg.de/MAILS-qustionnaire-english.pdf\\
\\
https://downloads.hci.informatik.uni-wuerzburg.de/MAILS-short-questionnaire-english.pdf\\
\\
https://hci.uni-wuerzburg.de/research/MAILS/},
  file = {/Users/thomasgorman/Zotero/storage/4PXPCY23/Koch et al. - 2024 - Meta AI literacy scale Further validation and development of a short version.pdf}
}

@article{kohnMeasurementTrustAutomation2021,
  title = {Measurement of {{Trust}} in {{Automation}}: {{A Narrative Review}} and {{Reference Guide}}},
  shorttitle = {Measurement of {{Trust}} in {{Automation}}},
  author = {Kohn, Spencer C. and De Visser, Ewart J. and Wiese, Eva and Lee, Yi-Ching and Shaw, Tyler H.},
  year = {2021},
  month = oct,
  journal = {Frontiers in Psychology},
  volume = {12},
  pages = {604977},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2021.604977},
  urldate = {2025-05-02},
  abstract = {With the rise of automated and autonomous agents, research examining Trust in Automation (TiA) has attracted considerable attention over the last few decades. Trust is a rich and complex construct which has sparked a multitude of measures and approaches to study and understand it. This comprehensive narrative review addresses known methods that have been used to capture TiA. We examined measurements deployed in existing empirical works, categorized those measures into self-report, behavioral, and physiological indices, and examined them within the context of an existing model of trust. The resulting work provides a reference guide for researchers, providing a list of available TiA measurement methods along with the model-derived constructs that they capture including judgments of trustworthiness, trust attitudes, and trusting behaviors. The article concludes with recommendations on how to improve the current state of TiA measurement.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/65AH2N7M/Kohn et al. - 2021 - Measurement of Trust in Automation A Narrative Review and Reference Guide.pdf}
}

@article{korberIntroductionMattersManipulating2018,
  title = {Introduction Matters: {{Manipulating}} Trust in Automation and Reliance in Automated Driving},
  shorttitle = {Introduction Matters},
  author = {K{\"o}rber, Moritz and Baseler, Eva and Bengler, Klaus},
  year = {2018},
  month = jan,
  journal = {Applied Ergonomics},
  volume = {66},
  pages = {18--31},
  issn = {0003-6870},
  doi = {10.1016/j.apergo.2017.07.006},
  urldate = {2025-05-02},
  abstract = {Trust in automation is a key determinant for the adoption of automated systems and their appropriate use. Therefore, it constitutes an essential research area for the introduction of automated vehicles to road traffic. In this study, we investigated the influence of trust promoting (Trust promoted group) and trust lowering (Trust lowered group) introductory information on reported trust, reliance behavior and take-over performance. Forty participants encountered three situations in a 17-min highway drive in a conditionally automated vehicle (SAE Level 3). Situation 1 and Situation 3 were non-critical situations where a take-over was optional. Situation 2 represented a critical situation where a take-over was necessary to avoid a collision. A non-driving-related task (NDRT) was presented between the situations to record the allocation of visual attention. Participants reporting a higher trust level spent less time looking at the road or instrument cluster and more time looking at the NDRT. The manipulation of introductory information resulted in medium differences in reported trust and influenced participants' reliance behavior. Participants of the Trust promoted group looked less at the road or instrument cluster and more at the NDRT. The odds of participants of the Trust promoted group to overrule the automated driving system in the non-critical situations were 3.65 times (Situation 1) to 5 times (Situation 3) higher. In Situation 2, the Trust promoted group's mean take-over time was extended by 1154~ms and the mean minimum time-to-collision was 933~ms shorter. Six participants from the Trust promoted group compared to no participant of the Trust lowered group collided with the obstacle. The results demonstrate that the individual trust level influences how much drivers monitor the environment while performing an NDRT. Introductory information influences this trust level, reliance on an automated driving system, and if a critical take-over situation can be successfully solved.},
  keywords = {Automated driving,Reliance,Trust in automation},
  annotation = {https://github.com/moritzkoerber/TiA\_Trust\_in\_Automation\_Questionnaire\\
\\
https://osf.io/xh3q5/},
  file = {/Users/thomasgorman/Zotero/storage/PFGZPC8A/Körber et al. - 2018 - Introduction matters Manipulating trust in automation and reliance in automated driving.pdf;/Users/thomasgorman/Zotero/storage/36MV7TYK/S0003687017301606.html}
}

@incollection{korberTheoreticalConsiderationsDevelopment2019,
  title = {Theoretical {{Considerations}} and {{Development}} of a {{Questionnaire}} to {{Measure Trust}} in {{Automation}}},
  booktitle = {Proceedings of the 20th {{Congress}} of the {{International Ergonomics Association}} ({{IEA}} 2018)},
  author = {K{\"o}rber, Moritz},
  editor = {Bagnara, Sebastiano and Tartaglia, Riccardo and Albolino, Sara and Alexander, Thomas and Fujita, Yushi},
  year = {2019},
  volume = {823},
  pages = {13--30},
  publisher = {Springer International Publishing},
  address = {Cham},
  urldate = {2025-05-02},
  abstract = {The increasing number of interactions with automated systems has sparked the interest of researchers in trust in automation because it predicts not only whether but also how an operator interacts with an automation. In this work, a theoretical model of trust in automation is established and the development and evaluation of a corresponding questionnaire (Trust in Automation, TiA) are described.},
  isbn = {978-3-319-96073-9 978-3-319-96074-6},
  langid = {english},
  annotation = {https://github.com/moritzkoerber/TiA\_Trust\_in\_Automation\_Questionnaire\\
\\
https://osf.io/y3jn5/\\
\\
https://osf.io/zvhe3/},
  file = {/Users/thomasgorman/Zotero/storage/RNS9EY75/Körber - 2019 - Theoretical Considerations and Development of a Questionnaire to Measure Trust in Automation.pdf}
}

@article{kuperPsychologicalFactorsInfluencing2025,
  title = {Psychological {{Factors Influencing Appropriate Reliance}} on {{AI-enabled Clinical Decision Support Systems}}: {{Experimental Web-Based Study Among Dermatologists}}},
  shorttitle = {Psychological {{Factors Influencing Appropriate Reliance}} on {{AI-enabled Clinical Decision Support Systems}}},
  author = {K{\"u}per, Alisa and Lodde, Georg Christian and Livingstone, Elisabeth and Schadendorf, Dirk and Kr{\"a}mer, Nicole},
  year = {2025},
  month = apr,
  journal = {Journal of Medical Internet Research},
  volume = {27},
  number = {1},
  pages = {e58660},
  doi = {10.2196/58660},
  urldate = {2025-05-03},
  abstract = {Background: Artificial intelligence (AI)--enabled decision support systems are critical tools in medical practice; however, their reliability is not absolute, necessitating human oversight for final decision-making. Human reliance on such systems can vary, influenced by factors such as individual psychological factors and physician experience. Objective: This study aimed to explore the psychological factors influencing subjective trust and reliance on medical AI's advice, specifically examining relative AI reliance and relative self-reliance to assess the appropriateness of reliance. Methods: A survey was conducted with 223 dermatologists, which included lesion image classification tasks and validated questionnaires assessing subjective trust, propensity to trust technology, affinity for technology interaction, control beliefs, need for cognition, as well as queries on medical experience and decision confidence. Results: A 2-tailed t test revealed that participants' accuracy improved significantly with AI support (t222=-3.3; P\&lt;.001; Cohen d=4.5), but only by an average of 1\% (1/100). Reliance on AI was stronger for correct advice than for incorrect advice (t222=4.2; P\&lt;.001; Cohen d=0.1). Notably, participants demonstrated a mean relative AI reliance of 10.04\% (139/1384) and a relative self-reliance of 85.6\% (487/569), indicating a high level of self-reliance but a low level of AI reliance. Propensity to trust technology influenced AI reliance, mediated by trust (indirect effect=0.024, 95\% CI 0.008-0.042; P\&lt;.001), and medical experience negatively predicted AI reliance (indirect effect=--0.001, 95\% CI --0.002 to -0.001; P\&lt;.001). Conclusions: The findings highlight the need to design AI support systems in a way that assists less experienced users with a high propensity to trust technology to identify potential AI errors, while encouraging experienced physicians to actively engage with system recommendations and potentially reassess initial decisions.},
  langid = {english},
  annotation = {https://osf.io/87tpu/files/osfstorage\#},
  file = {/Users/thomasgorman/Zotero/storage/7C8KLSSX/Küper et al. - 2025 - Psychological Factors Influencing Appropriate Reliance on AI-enabled Clinical Decision Support Syste.pdf;/Users/thomasgorman/Zotero/storage/88783Q3S/e58660.html}
}

@article{laupichlerDevelopmentScaleAssessment2023,
  title = {Development of the ``{{Scale}} for the Assessment of Non-Experts' {{AI}} Literacy'' -- {{An}} Exploratory Factor Analysis},
  author = {Laupichler, Matthias Carl and Aster, Alexandra and Haverkamp, Nicolas and Raupach, Tobias},
  year = {2023},
  month = dec,
  journal = {Computers in Human Behavior Reports},
  volume = {12},
  pages = {100338},
  issn = {2451-9588},
  doi = {10.1016/j.chbr.2023.100338},
  urldate = {2025-05-02},
  abstract = {Artificial Intelligence competencies will become increasingly important in the near future. Therefore, it is essential that the AI literacy of individuals can be assessed in a valid and reliable way. This study presents the development of the ``Scale for the assessment of non-experts' AI literacy'' (SNAIL). An existing AI literacy item set was distributed as an online questionnaire to a heterogeneous group of non-experts (i.e., individuals without a formal AI or computer science education). Based on the data collected, an exploratory factor analysis was conducted to investigate the underlying latent factor structure. The results indicated that a three-factor model had the best model fit. The individual factors reflected AI competencies in the areas of ``Technical Understanding'', ``Critical Appraisal'', and ``Practical Application''. In addition, eight items from the original questionnaire were deleted based on high intercorrelations and low communalities to reduce the length of the questionnaire. The final SNAIL-questionnaire consists of 31 items that can be used to assess the AI literacy of individual non-experts or specific groups and is also designed to enable the evaluation of AI literacy courses' teaching effectiveness.},
  keywords = {AI competencies,AI literacy,AI literacy questionnaire,AI literacy scale,Assessment,Exploratory factor analysis},
  annotation = {https://ars-els-cdn-com.proxyiub.uits.iu.edu/content/image/1-s2.0-S2451958823000714-mmc1.pdf},
  file = {/Users/thomasgorman/Zotero/storage/V6BSVFXF/Laupichler et al. - 2023 - Development of the “Scale for the assessment of non-experts’ AI literacy” – An exploratory factor an.pdf;/Users/thomasgorman/Zotero/storage/LVBV4DE8/S2451958823000714.html}
}

@misc{leeSuperintelligenceSuperstitionExploring2024,
  title = {Super-Intelligence or {{Superstition}}? {{Exploring Psychological Factors Influencing Belief}} in {{AI Predictions}} about {{Personal Behavior}}},
  shorttitle = {Super-Intelligence or {{Superstition}}?},
  author = {Lee, Eunhae and Pataranutaporn, Pat and Amores, Judith and Maes, Pattie},
  year = {2024},
  month = dec,
  number = {arXiv:2408.06602},
  eprint = {2408.06602},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.06602},
  urldate = {2025-04-30},
  abstract = {Could belief in AI predictions be just another form of superstition? This study investigates psychological factors that influence belief in AI predictions about personal behavior, comparing it to belief in astrology- and personality-based predictions. Through an experiment with 238 participants, we examined how cognitive style, paranormal beliefs, AI attitudes, personality traits, and other factors affect perceived validity, reliability, usefulness, and personalization of predictions from different sources. Our findings reveal that belief in AI predictions is positively correlated with belief in predictions based on astrology and personality psychology. Notably, paranormal beliefs and positive attitudes about AI significantly increased perceived validity, reliability, usefulness, and personalization of AI predictions. Conscientiousness was negatively correlated with belief in predictions across all sources, and interest in the prediction topic increased believability across predictions. Surprisingly, we found no evidence that cognitive style has an impact on belief in fictitious AI-generated predictions. These results highlight the "rational superstition" phenomenon in AI, where belief is driven more by mental heuristics and intuition than critical evaluation. This research advances our understanding of the psychology of human-AI interaction, offering insights into designing and promoting AI systems that foster appropriate trust and skepticism, critical for responsible integration in an increasingly AI-driven world.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  annotation = {https://github.com/mitmedialab/ai-superstition},
  file = {/Users/thomasgorman/Zotero/storage/WA8AQUEJ/Lee et al. - 2024 - Super-intelligence or Superstition Exploring Psychological Factors Influencing Belief in AI Predict.pdf;/Users/thomasgorman/Zotero/storage/F8JBGGLU/2408.html}
}

@article{lesicComparingConsumerPerceptions2019,
  title = {Comparing Consumer Perceptions of Appliances' Electricity Use to Appliances' Actual Direct-Metered Consumption},
  author = {Lesic, Vedran and Glasgo, Brock and Krishnamurti, Tamar and Bruine De Bruin, W{\"a}ndi and Davis, Matthew and Azevedo, In{\^e}s Lima},
  year = {2019},
  month = dec,
  journal = {Environmental Research Communications},
  volume = {1},
  number = {11},
  pages = {111002},
  issn = {2515-7620},
  doi = {10.1088/2515-7620/ab4a99},
  urldate = {2025-05-05},
  abstract = {Many strategies for reducing residential energy consumption---including product labelling programs, subsidies for the purchase of efficient devices, behavioral programs that encourage efficient energy use, and others---rely on building owners and end users to make informed investment and operational decisions. These strategies may be ineffective if consumers are unaware of how much electricity is used by different devices in their homes and buildings. This study therefore compares consumers' perceptions of their appliances' electricity use to these appliances' actual direct-metered electricity consumption. Using an online survey, 118 homeowners from Austin, Texas were asked to estimate the energy consumption of six household devices which were monitored in the participants' homes. Homeowners were randomly assigned to assess their appliance-specific electricity use in terms of energy units (kWh/month) or energy cost units (\$/month) for an average summer month. Consistent with previous studies, participants overestimated the energy consumed by their low energy consuming devices and slightly underestimated that of their most energy-consuming device. Results also showed that responses of the experimental groups estimating their consumption in energy units and energy cost units were similar, the accuracy of the two groups' perceptions was similar, and levels of confidence in the two groups were similar. These results suggest that targeted information campaigns focused on air conditioning energy consumption and device power reduction opportunities could improve consumer decision-making to save energy and reduce demand.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/THGZZSBT/Lesic et al. - 2019 - Comparing consumer perceptions of appliances’ electricity use to appliances’ actual direct-metered c.pdf}
}

@article{lesicConsumersPerceptionsEnergy2018,
  title = {Consumers' Perceptions of Energy Use and Energy Savings: {{A}} Literature Review},
  shorttitle = {Consumers' Perceptions of Energy Use and Energy Savings},
  author = {Lesic, Vedran and {de Bruin}, W{\"a}ndi Bruine and Davis, Matthew C and Krishnamurti, Tamar and Azevedo, In{\^e}s M L},
  year = {2018},
  month = mar,
  journal = {Environmental Research Letters},
  volume = {13},
  number = {3},
  pages = {033004},
  publisher = {IOP Publishing},
  issn = {1748-9326},
  doi = {10.1088/1748-9326/aaab92},
  urldate = {2025-03-16},
  abstract = {Background. Policy makers and program managers need to better understand consumers' perceptions of their energy use and savings to design effective strategies for promoting energy savings. Methods. We reviewed 14 studies from the emerging interdisciplinary literature examining consumers' perceptions electricity use by specific appliances, and potential savings. Results. We find that: (1) electricity use is often overestimated for low-energy consuming appliances, and underestimated for high-energy consuming appliances; (2) curtailment strategies are typically preferred over energy efficiency strategies; (3) consumers lack information about how much electricity can be saved through specific strategies; (4) consumers use heuristics for assessing the electricity use of specific appliances, with some indication that more accurate judgments are made among consumers with higher numeracy and stronger pro-environmental attitudes. However, design differences between studies, such as variations in reference points, reporting units and assessed time periods, may affect consumers' reported perceptions. Moreover, studies differ with regard to whether accuracy of perceptions was evaluated through comparisons with general estimates of actual use, self-reported use, household-level meter readings, or real-time smart meter readings. Conclusion. Although emerging findings are promising, systematic variations in the measurement of perceived and actual electricity use are potential cause for concern. We propose avenues for future research, so as to better understand, and possibly inform, consumers' perceptions of their electricity use. Ultimately, this literature will have implications for the design of effective electricity feedback for consumers, and related policies.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/VUQVPSBH/Lesic et al. - 2018 - Consumers’ perceptions of energy use and energy savings A literature review.pdf}
}

@article{liAIMotivationScale2025,
  title = {The {{AI Motivation Scale}} ({{AIMS}}): A Self-Determination Theory Perspective},
  shorttitle = {The {{AI Motivation Scale}} ({{AIMS}})},
  author = {Li, Jiajing and King, Ronnel B. and Chai, Ching Sing and Zhai, Xuesong and Lee, Vivian W. Y.},
  year = {2025},
  month = mar,
  journal = {Journal of Research on Technology in Education},
  publisher = {Routledge},
  issn = {1539-1523},
  urldate = {2025-05-02},
  abstract = {Artificial Intelligence (AI) has a profound impact on university teaching and learning. However, there is a lack of instruments for measuring university students' motivation to use AI in their learning. In this study, we developed and validated a questionnaire to measure students' motivation to learn with AI. In Study 1, we developed the AI Motivation Scale (AIMS). Rooted in self-determination theory, the scale measures university students' motivation to learn with AI across five dimensions: intrinsic motivation, identified regulation, introjected regulation, external regulation, and amotivation. Both within-network and between-network validation analyses indicated that the AIMS is psychometrically sound. In Study 2, we used the AIMS to explore whether students' motivation to learn with AI is influenced by their university environment and promotes their engagement in learning with AI. The results showed that motivation to learn with AI mediated the positive relationship between supportive environments and engagement in learning with AI. The study shows that AIMS is a psychometrically sound instrument that can be used to assess university students' motivation to learn with AI. It also sheds light on the pivotal role of motivation to learn with AI in the higher education context.},
  copyright = {{\copyright} 2025 The Author(s). Published with license by Taylor \& Francis Group, LLC.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/MMUJE5TS/Li et al. - 2025 - The AI Motivation Scale (AIMS) a self-determination theory perspective.pdf;/Users/thomasgorman/Zotero/storage/XGNN399L/15391523.2025.html}
}

@misc{liImSpartacusNo2024,
  title = {I'm {{Spartacus}}, {{No}}, {{I}}'m {{Spartacus}}: {{Measuring}} and {{Understanding LLM Identity Confusion}}},
  shorttitle = {I'm {{Spartacus}}, {{No}}, {{I}}'m {{Spartacus}}},
  author = {Li, Kun and Zhuang, Shichao and Zhang, Yue and Xu, Minghui and Wang, Ruoxi and Xu, Kaidi and Fu, Xinwen and Cheng, Xiuzhen},
  year = {2024},
  month = nov,
  number = {arXiv:2411.10683},
  eprint = {2411.10683},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.10683},
  urldate = {2025-05-03},
  abstract = {Large Language Models (LLMs) excel in diverse tasks such as text generation, data analysis, and software development, making them indispensable across domains like education, business, and creative industries. However, the rapid proliferation of LLMs (with over 560 companies developing or deploying them as of 2024) has raised concerns about their originality and trustworthiness. A notable issue, termed identity confusion, has emerged, where LLMs misrepresent their origins or identities. This study systematically examines identity confusion through three research questions: (1) How prevalent is identity confusion among LLMs? (2) Does it arise from model reuse, plagiarism, or hallucination? (3) What are the security and trust-related impacts of identity confusion? To address these, we developed an automated tool combining documentation analysis, self-identity recognition testing, and output similarity comparisons--established methods for LLM fingerprinting--and conducted a structured survey via Credamo to assess its impact on user trust. Our analysis of 27 LLMs revealed that 25.93\% exhibit identity confusion. Output similarity analysis confirmed that these issues stem from hallucinations rather than replication or reuse. Survey results further highlighted that identity confusion significantly erodes trust, particularly in critical tasks like education and professional use, with declines exceeding those caused by logical errors or inconsistencies. Users attributed these failures to design flaws, incorrect training data, and perceived plagiarism, underscoring the systemic risks posed by identity confusion to LLM reliability and trustworthiness.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {/Users/thomasgorman/Zotero/storage/PXM3QHHC/Li et al. - 2024 - I'm Spartacus, No, I'm Spartacus Measuring and Understanding LLM Identity Confusion.pdf;/Users/thomasgorman/Zotero/storage/BC9LQUYQ/2411.html}
}

@misc{linTeachingModelsExpress2022,
  title = {Teaching {{Models}} to {{Express Their Uncertainty}} in {{Words}}},
  author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  year = {2022},
  month = jun,
  number = {arXiv:2205.14334},
  eprint = {2205.14334},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.14334},
  urldate = {2025-05-13},
  abstract = {We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g. "90\% confidence" or "high confidence"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words ("verbalized probability") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Zotero/storage/TSYHWS6E/Lin et al. - 2022 - Teaching Models to Express Their Uncertainty in Words.pdf;/Users/thomasgorman/Zotero/storage/KCLE8Z7J/2205.html}
}

@misc{liOverconfidentUnconfidentAI2024,
  title = {Overconfident and {{Unconfident AI Hinder Human-AI Collaboration}}},
  author = {Li, Jingshu and Yang, Yitian and Zhang, Renwen and Lee, Yi-chieh},
  year = {2024},
  month = apr,
  number = {arXiv:2402.07632},
  eprint = {2402.07632},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.07632},
  urldate = {2025-05-09},
  abstract = {AI transparency is a central pillar of responsible AI deployment and effective human-AI collaboration. A critical approach is communicating uncertainty, such as displaying AI's confidence level, or its correctness likelihood (CL), to users. However, these confidence levels are often uncalibrated, either overestimating or underestimating actual CL, posing risks and harms to human-AI collaboration. This study examines the effects of uncalibrated AI confidence on users' trust in AI, AI advice adoption, and collaboration outcomes. We further examined the impact of increased transparency, achieved through trust calibration support, on these outcomes. Our results reveal that uncalibrated AI confidence leads to both the misuse of overconfident AI and disuse of unconfident AI, thereby hindering outcomes of human-AI collaboration. Deficiency of trust calibration support exacerbates this issue by making it harder to detect uncalibrated confidence, promoting misuse and disuse of AI. Conversely, trust calibration support aids in recognizing uncalibration and reducing misuse, but it also fosters distrust and causes disuse of AI. Our findings highlight the importance of AI confidence calibration for enhancing human-AI collaboration and suggest directions for AI design and regulation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/BFRHLL9W/Li et al. - 2024 - Overconfident and Unconfident AI Hinder Human-AI Collaboration.pdf;/Users/thomasgorman/Zotero/storage/TEMV2FV3/2402.html}
}

@misc{liuFullECEMetricTokenlevel2024,
  title = {Full-{{ECE}}: {{A Metric For Token-level Calibration}} on {{Large Language Models}}},
  shorttitle = {Full-{{ECE}}},
  author = {Liu, Han and Zhang, Yupeng and Wang, Bingning and Chen, Weipeng and Hu, Xiaolin},
  year = {2024},
  month = jun,
  number = {arXiv:2406.11345},
  eprint = {2406.11345},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.11345},
  urldate = {2025-05-13},
  abstract = {Deep Neural Networks (DNNs) excel in various domains but face challenges in providing accurate uncertainty estimates, which are crucial for high-stakes applications. Large Language Models (LLMs) have recently emerged as powerful tools, demonstrating exceptional performance in language tasks. However, traditional calibration metrics such as Expected Calibration Error (ECE) and classwise-ECE (cw-ECE) are inadequate for LLMs due to their vast vocabularies, data complexity, and distributional focus. To address this, we propose a novel calibration concept called full calibration and introduce its corresponding metric, Full-ECE. Full-ECE evaluates the entire predicted probability distribution, offering a more accurate and robust measure of calibration for LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Zotero/storage/28EWNQ7F/Liu et al. - 2024 - Full-ECE A Metric For Token-level Calibration on Large Language Models.pdf;/Users/thomasgorman/Zotero/storage/496AK44N/2406.html}
}

@inproceedings{longWhatAILiteracy2020,
  title = {What Is {{AI Literacy}}? {{Competencies}} and {{Design Considerations}}},
  shorttitle = {What Is {{AI Literacy}}?},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Long, Duri and Magerko, Brian},
  year = {2020},
  month = apr,
  pages = {1--16},
  publisher = {ACM},
  address = {Honolulu HI USA},
  doi = {10.1145/3313831.3376727},
  urldate = {2025-05-01},
  abstract = {Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learner-centered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper's contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.},
  file = {/Users/thomasgorman/Zotero/storage/RNET5I88/Long and Magerko - 2020 - What is AI Literacy Competencies and Design Considerations.pdf}
}

@article{lundbergEasyNotEffective2019,
  title = {Easy but Not Effective: {{Why}} ``Turning off the Lights'' Remains a Salient Energy Conserving Behaviour in the {{United States}}},
  shorttitle = {Easy but Not Effective},
  author = {Lundberg, Daniel C. and Tang, Janine A. and Attari, Shahzeen Z.},
  year = {2019},
  month = dec,
  journal = {Energy Research \& Social Science},
  volume = {58},
  pages = {101257},
  issn = {22146296},
  doi = {10.1016/j.erss.2019.101257},
  urldate = {2025-05-05},
  abstract = {When participants are asked how best to save energy in the home, the most frequent response since the 1980s has been ``turning off the lights''. Here, we use an online survey (N = 1418) to investigate why turning off the lights persists as a modal response despite decades of energy education promoting far more effective behaviors. We confirm that turning off the lights is still the modal response when participants are asked for the single most effective action they currently do to save energy (36.3\% of participants). We find that being taught to turn off the light is an important reason for why turning off the lights has remained so popular. When participants are asked to make a recommendation to a friend between turning off the lights (curtailment action) or replacing incandescent bulbs with CFL or LED bulbs (efficiency action), we observe a remarkable shift towards efficiency (77\%) rather than curtailment (23\%). We find that participants explain their choice of turning off lights or replacing bulbs with different heuristics. Participants who choose turning off the lights state that energy savings occur when an appliance is completely turned off. Alternatively, those who pick replacing inefficient light bulbs state that far less energy can be used for a given task.},
  langid = {english},
  annotation = {https://data.mendeley.com/datasets/5ft3m5t4f5/1},
  file = {/Users/thomasgorman/Zotero/storage/MFNRY8KU/Lundberg et al. - 2019 - Easy but not effective Why “turning off the lights” remains a salient energy conserving behaviour i.pdf}
}

@article{maertensMisinformationSusceptibilityTest2023,
  title = {The {{Misinformation Susceptibility Test}} ({{MIST}}): {{A}} Psychometrically Validated Measure of News Veracity Discernment},
  shorttitle = {The {{Misinformation Susceptibility Test}} ({{MIST}})},
  author = {Maertens, Rakoen and G{\"o}tz, Friedrich M. and Golino, Hudson F. and Roozenbeek, Jon and Schneider, Claudia R. and Kyrychenko, Yara and Kerr, John R. and Stieger, Stefan and McClanahan, William P. and Drabot, Karly and He, James and Van Der Linden, Sander},
  year = {2023},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {56},
  number = {3},
  pages = {1863--1899},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02124-2},
  urldate = {2025-03-12},
  abstract = {Interest in the psychology of misinformation has exploded in recent years. Despite ample research, to date there is no validated framework to measure misinformation susceptibility. Therefore, we introduce Verification done, a nuanced interpretation schema and assessment tool that simultaneously considers Veracity discernment, and its distinct, measurable abilities (real/fake news detection), and biases (distrust/na{\"i}vit{\'e}---negative/positive judgment bias). We then conduct three studies with seven independent samples (Ntotal = 8504) to show how to develop, validate, and apply the Misinformation Susceptibility Test (MIST). In Study 1 (N=409) we use a neural network language model to generate items, and use three psychometric methods---factor analysis, item response theory, and exploratory graph analysis---to create the MIST-20 (20 items; completion time {$<$} 2 minutes), the MIST-16 (16 items; {$<$} 2 minutes), and the MIST-8 (8 items; {$<$} 1 minute). In Study 2 (N = 7674) we confirm the internal and predictive validity of the MIST in five national quota samples (US, UK), across 2 years, from three different sampling platforms---Respondi, CloudResearch, and Prolific. We also explore the MIST's nomological net and generate age-, region-, and country-specific norm tables. In Study 3 (N=421) we demonstrate how the MIST---in conjunction with Verification done---can provide novel insights on existing psychological interventions, thereby advancing theory development. Finally, we outline the versatile implementations of the MIST as a screening tool, covariate, and intervention evaluation framework. As all methods are transparently reported and detailed, this work will allow other researchers to create similar scales or adapt them for any population of interest.},
  langid = {english},
  annotation = {https://osf.io/r7phc/files},
  file = {/Users/thomasgorman/Zotero/storage/76SVIZXQ/Maertens et al. - 2023 - The Misinformation Susceptibility Test (MIST) A psychometrically validated measure of news veracity.pdf}
}

@article{maguireSeeingPatternsRandomness2019,
  title = {Seeing {{Patterns}} in {{Randomness}}: {{A Computational Model}} of {{Surprise}}},
  shorttitle = {Seeing {{Patterns}} in {{Randomness}}},
  author = {Maguire, Phil and Moser, Philippe and Maguire, Rebecca and Keane, Mark T.},
  year = {2019},
  journal = {Topics in Cognitive Science},
  volume = {11},
  number = {1},
  pages = {103--118},
  issn = {1756-8765},
  doi = {10.1111/tops.12345},
  urldate = {2022-04-20},
  abstract = {While seemingly a ubiquitous cognitive process, the precise definition and function of surprise remains elusive. Surprise is often conceptualized as being related to improbability or to contrasts with higher probability expectations. In contrast to this probabilistic view, we argue that surprising observations are those that undermine an existing model, implying an alternative causal origin. Surprises are not merely improbable events; instead, they indicate a breakdown in the model being used to quantify probability. We suggest that the heuristic people rely on to detect such anomalous events is randomness deficiency. Specifically, people experience surprise when they identify patterns where their model implies there should only be random noise. Using algorithmic information theory, we present a novel computational theory which formalizes this notion of surprise as randomness deficiency. We also present empirical evidence that people respond to randomness deficiency in their environment and use it to adjust their beliefs about the causal origins of events. The connection between this pattern-detection view of surprise and the literature on learning and interestingness is discussed.},
  langid = {english},
  keywords = {Algorithmic information theory,Bayesian reasoning,Data compression,Interestingness,Randomness deficiency,Representational updating,Stochastic model,Surprise},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Maguire et al_2019_Seeing Patterns in Randomness.pdf;/Users/thomasgorman/Zotero/storage/ZJ3UPUU3/tops.html}
}

@article{maloneyComparisonHumanGPT42024,
  title = {A Comparison of Human and {{GPT-4}} Use of Probabilistic Phrases in a Coordination Game},
  author = {Maloney, Laurence T. and Dal Martello, Maria F. and Fei, Vivian and Ma, Valerie},
  year = {2024},
  month = mar,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {6835},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-56740-9},
  urldate = {2025-05-13},
  abstract = {There are significant patterned differences between median human probability estimates and those of GPT-4. In both contexts median human estimates of probability are compressed by a factor of 0.8 relative to the estimates by GPT-4. In the Medical Context but not in the Investment, human estimates of probabilities are also offset vertically by roughly 10\%. Human use of probability and relative frequency are typically distorted34,35 and the deviations we detect may be connected to probability distortion.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/KCW9RCCI/Maloney et al. - 2024 - A comparison of human and GPT-4 use of probabilistic phrases in a coordination game.pdf}
}

@article{maniscalcoSignalDetectionTheoretic2012,
  title = {A Signal Detection Theoretic Approach for Estimating Metacognitive Sensitivity from Confidence Ratings},
  author = {Maniscalco, Brian and Lau, Hakwan},
  year = {2012},
  month = mar,
  journal = {Consciousness and Cognition},
  volume = {21},
  number = {1},
  pages = {422--430},
  issn = {10538100},
  doi = {10.1016/j.concog.2011.09.021},
  urldate = {2025-05-13},
  abstract = {How should we measure metacognitive (``type 2'') sensitivity, i.e. the efficacy with which observers' confidence ratings discriminate between their own correct and incorrect stimulus classifications? We argue that currently available methods are inadequate because they are influenced by factors such as response bias and type 1 sensitivity (i.e. ability to distinguish stimuli). Extending the signal detection theory (SDT) approach of Galvin, Podd, Drga, and Whitmore (2003), we propose a method of measuring type 2 sensitivity that is free from these confounds. We call our measure meta-d0, which reflects how much information, in signal-to-noise units, is available for metacognition. Applying this novel method in a 2interval forced choice visual task, we found that subjects' metacognitive sensitivity was close to, but significantly below, optimality. We discuss the theoretical implications of these findings, as well as related computational issues of the method. We also provide free Matlab code for implementing the analysis.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  annotation = {https://brianmaniscalco.org/type2-sdt/},
  file = {/Users/thomasgorman/Zotero/storage/BNTGQMUQ/Maniscalco and Lau - 2012 - A signal detection theoretic approach for estimating metacognitive sensitivity from confidence ratin.pdf}
}

@article{marghetisSimpleInterventionsCan2019,
  title = {Simple Interventions Can Correct Misperceptions of Home Energy Use},
  author = {Marghetis, Tyler and Attari, Shahzeen Z. and Landy, David},
  year = {2019},
  month = oct,
  journal = {Nature Energy},
  volume = {4},
  number = {10},
  pages = {874--881},
  issn = {2058-7546},
  doi = {10.1038/s41560-019-0467-2},
  urldate = {2024-12-07},
  abstract = {Public estimates of energy use suffer from severe biases. Failure to correct these may hinder efforts to conserve energy and undermine support for evidence-based policies. Here we present a randomized online experiment that showed that home energy perceptions can be improved. We tested two simple, potentially scalable interventions: providing numerical information (in watt-hours) about extremes of energy use and providing an explicit heuristic that addressed a common misperception. Both succeeded in improving numerical estimates of energy use, but in different ways. Numerical information about extremes primarily improved the use of the watt-hours response scale, while the heuristic improved underlying understanding of relative energy use. As a result, only the heuristic significantly benefitted judgements about energy-conserving behaviours. Because understanding of energy use also predicted self-reported energy-conservation behaviour, belief in climate change, and support for climate policies, targeting energy misperceptions may have the potential to shape individual behaviour and national policy support.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Energy and behaviour,Energy conservation,Psychology and behaviour},
  annotation = {https://osf.io/2qbxt/\\
\\
https://www.szattari.com/publications},
  file = {/Users/thomasgorman/Zotero/storage/EL2PIXPK/Marghetis et al. - 2019 - Simple interventions can correct misperceptions of home energy use.pdf}
}

@incollection{marikyanTechnologyAcceptanceModel2023,
  title = {Technology Acceptance Model},
  booktitle = {{{TheoryHub Book}}},
  author = {Marikyan and Papagiannidis},
  year = {2023},
  urldate = {2025-05-02},
  abstract = {TheoryHub reviews a wide range of theories, acting as a starting point for theory exploration in different research and teaching and learning contexts.},
  langid = {english},
  annotation = {https://open.ncl.ac.uk/theories/1/technology-acceptance-model/},
  file = {/Users/thomasgorman/Zotero/storage/2LNMJE77/eBusiness@Newcastle - 2023 - Technology acceptance model.pdf}
}

@misc{markusObjectiveMeasurementAI2025,
  title = {Objective {{Measurement}} of {{AI Literacy}}: {{Development}} and {{Validation}} of the {{AI Competency Objective Scale}} ({{AICOS}})},
  shorttitle = {Objective {{Measurement}} of {{AI Literacy}}},
  author = {Markus, Andr{\'e} and Carolus, Astrid and Wienrich, Carolin},
  year = {2025},
  month = mar,
  number = {arXiv:2503.12921},
  eprint = {2503.12921},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.12921},
  urldate = {2025-05-02},
  abstract = {As Artificial Intelligence (AI) becomes more pervasive in various aspects of life, AI literacy is becoming a fundamental competency that enables individuals to move safely and competently in an AI-pervaded world. There is a growing need to measure this competency, e.g., to develop targeted educational interventions. Although several measurement tools already exist, many have limitations regarding subjective data collection methods, target group differentiation, validity, and integration of current developments such as Generative AI Literacy. This study develops and validates the AI Competency Objective Scale (AICOS) for measuring AI literacy objectively. The presented scale addresses weaknesses and offers a robust measurement approach that considers established competency and measurement models, captures central sub-competencies of AI literacy, and integrates the dimension of Generative AI Literacy. The AICOS provides a sound and comprehensive measure of AI literacy, and initial analyses show potential for a modular structure. Furthermore, a first edition of a short version of the AICOS is developed. Due to its methodological foundation, extensive validation, and integration of recent developments, the test represents a valuable resource for scientific research and practice in educational institutions and professional contexts. The AICOS significantly contributes to the development of standardized measurement instruments and enables the targeted assessment and development of AI skills in different target groups.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  annotation = {https://osf.io/ehk8u/files/osfstorage?view\_only=df9a6ea06d1446659437a73946f68e5c},
  file = {/Users/thomasgorman/Zotero/storage/L8RMU2TN/Markus et al. - 2025 - Objective Measurement of AI Literacy Development and Validation of the AI Competency Objective Scal.pdf;/Users/thomasgorman/Zotero/storage/86Z8P2SI/2503.html}
}

@article{mederDevelopmentalTrajectoriesUnderstanding2022,
  title = {Developmental {{Trajectories}} in the {{Understanding}} of {{Everyday Uncertainty Terms}}},
  author = {Meder, Bj{\"o}rn and Mayrhofer, Ralf and Ruggeri, Azzurra},
  year = {2022},
  journal = {Topics in Cognitive Science},
  volume = {14},
  number = {2},
  issn = {1756-8765},
  doi = {10.1111/tops.12564},
  urldate = {2022-04-10},
  abstract = {Dealing with uncertainty and different degrees of frequency and probability is critical in many everyday activities. However, relevant information does not always come in the form of numerical estimates or direct experiences, but is instead obtained through qualitative, rather vague verbal terms (e.g., ``the virus often causes coughing'' or ``the train is likely to be delayed''). Investigating how people interpret and utilize different natural language expressions of frequency and probability is therefore crucial to understand reasoning and behavior in real-world situations. While there is considerable work exploring how adults understand everyday uncertainty phrases, very little is known about how children interpret them and how their understanding develops with age. We take a developmental and computational perspective to address this issue and examine how 4- to 14-year-old children and adults interpret different terms. Each participant provided numerical estimates for 14 expressions, comprising both frequency and probability phrases. In total we obtained 2856 quantitative judgments, including 2240 judgments from children. Our findings demonstrate that adult-like intuitions about the interpretation of everyday uncertainty terms emerge fairly early in development, with the quantitative estimates of children converging to those of adults from around 9 years on. We also demonstrate how the vagueness of verbal terms can be represented through probability distributions, which provides additional leverage for tracking developmental shifts through cognitive modeling techniques. Taken together, our findings provide key insights into the developmental trajectories underlying the understanding of everyday uncertainty terms, and open up novel methodological pathways to formally model the vagueness of probability and frequency phrases, which are abundant in our everyday life and activities.},
  langid = {english},
  keywords = {Computational modeling,Development,Everyday activities,Everyday uncertainty terms,Frequency phrases,Probability phrases,Verbal uncertainty terms},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Meder et al_Developmental Trajectories in the Understanding of Everyday Uncertainty Terms.pdf;/Users/thomasgorman/Zotero/storage/GGVQFMYP/tops.html}
}

@article{mei-shiuDevelopmentValidationEnergyIssue2018,
  title = {Development and {{Validation}} of the {{Energy-Issue Attitude Questionnaire}}: {{Relations}} with {{Energy Knowledge}}, {{Affect}}, and {{Behavior}}},
  shorttitle = {Development and {{Validation}} of the {{Energy-Issue Attitude Questionnaire}}},
  author = {{Mei-Shiu}, Chiu and Jan, DeWaters and {Clarkson University, Potsdam, NY, USA}},
  year = {2018},
  month = feb,
  journal = {Journal of Advances in Education Research},
  volume = {3},
  number = {1},
  publisher = {Isaac Scientific Publishing Co., Ltd.},
  issn = {2519-7002, 2519-7010},
  doi = {10.22606/jaer.2018.31003},
  urldate = {2025-05-01},
  abstract = {This study aims to develop the Energy-Issue Attitude Questionnaire (EIAQ). The EIAQ focuses on student responses to energy issues in society and includes ten constructs, organized pairwise with tension: energy-saving vs. carbon-reducing knowledge, having vs. being lifestyles, questioning vs. conforming to authorities, technology vs. nature approaches, and future vs. present goals. The EIAQ was validated with a criterion questionnaire on energy literacy, including knowledge, affect and behavior. Research participants were 4,689 Taiwanese secondary students. The results show that the EIAQ has desirable construct validity and reliability. Significant differences occur between the two attitudes in each pair. Energy attitudes have medium correlations with energy affect and behavior but low correlations with energy knowledge. The results of structural equation modeling show that energy behavior is directly predicted by 'being' lifestyles and conformity to authorities, and indirectly predicted by energy-saving knowledge, mediated by energy affect.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/EXPLI3RB/Mei-Shiu et al. - 2018 - Development and Validation of the Energy-Issue Attitude Questionnaire Relations with Energy Knowled.pdf}
}

@article{menonChattingChatGPTAnalyzing2023,
  title = {``{{Chatting}} with {{ChatGPT}}'': {{Analyzing}} the Factors Influencing Users' Intention to {{Use}} the {{Open AI}}'s {{ChatGPT}} Using the {{UTAUT}} Model},
  shorttitle = {``{{Chatting}} with {{ChatGPT}}''},
  author = {Menon, Devadas and Shilpa, K},
  year = {2023},
  month = nov,
  journal = {Heliyon},
  volume = {9},
  number = {11},
  pages = {e20962},
  issn = {2405-8440},
  doi = {10.1016/j.heliyon.2023.e20962},
  urldate = {2025-05-02},
  abstract = {Open AI's ChatGPT has emerged as a popular AI language model that can engage in natural language conversations with users. Based on a qualitative research approach using semistructured interviews with 32 ChatGPT users from India, this study examined the factors influencing users' acceptance and use of ChatGPT using the unified theory of acceptance and usage of technology (UTAUT) model. The study results demonstrated that the four factors of UTAUT, along with two extended constructs, i.e. perceived interactivity and privacy concerns, can explain users' interaction and engagement with ChatGPT. The study also found that age and experience can moderate the impact of various factors on the use of ChatGPT. The theoretical and practical implications of the study were also discussed.},
  keywords = {Acceptance and use,Chatbots,ChatGPT,OpenAI,UTAUT},
  file = {/Users/thomasgorman/Zotero/storage/QW8IKTND/Menon and Shilpa - 2023 - “Chatting with ChatGPT” Analyzing the factors influencing users' intention to Use the Open AI's Cha.pdf;/Users/thomasgorman/Zotero/storage/34RQ4BAH/S2405844023081707.html}
}

@article{miniardSharedVisionDecarbonized2020,
  title = {Shared Vision for a Decarbonized Future Energy System in the {{United States}}},
  author = {Miniard, Deidra and Kantenbacher, Joseph and Attari, Shahzeen Z.},
  year = {2020},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {13},
  pages = {7108--7114},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1920558117},
  urldate = {2025-05-05},
  abstract = {How do people envision the future energy system in the United States with respect to using fossil fuels, renewable energy, and nuclear energy? Are there shared policy pathways of achieving a decarbonized energy system? Here, we present results of an online survey (               n               = 2,429) designed to understand public perceptions of the current and future energy mixes in the United States (i.e., energy sources used for electric power, transportation, industrial, commercial, and residential sectors). We investigate support for decarbonization policies and antidecarbonization policies and the relative importance of climate change as an issue. Surprisingly, we find bipartisan support for a decarbonized energy future. Although there is a shared vision for decarbonization, there are strong partisan differences regarding the policy pathways for getting there. On average, our participants think that climate change is not the most important problem facing the United States today, but they do view climate change as an important issue for the world today and for the United States and the world in the future.},
  langid = {english},
  annotation = {https://www.szattari.com/publications\\
\\
https://static1.squarespace.com/static/54e39dcfe4b033c7e0e77c20/t/5e6ff908c80356332ba7042a/1584396554209/EnergyMix\_SI.pdf},
  file = {/Users/thomasgorman/Zotero/storage/NFQSVXT3/Miniard et al. - 2020 - Shared vision for a decarbonized future energy system in the United States.pdf}
}

@article{montagCanWeAssess2025,
  title = {Can {{We Assess Attitudes Toward AI}} with {{Single Items}}? {{Associations}} with {{Existing Attitudes Toward AI Measures}} and {{Trust}} in {{ChatGPT}}},
  shorttitle = {Can {{We Assess Attitudes Toward AI}} with {{Single Items}}?},
  author = {Montag, Christian and Ali, Raian},
  year = {2025},
  month = feb,
  journal = {Journal of Technology in Behavioral Science},
  issn = {2366-5963},
  doi = {10.1007/s41347-025-00481-7},
  urldate = {2025-05-01},
  abstract = {A growing number of researchers investigate individual differences in attitudes toward Artificial Intelligence (AI), which is not surprising given that the AI revolution is impacting societies around the globe. Different frameworks have been proposed to study both positive and negative attitudes toward AI. To our knowledge, the present work is the first to simultaneously investigate the ATAI (Attitudes Toward Artificial Intelligence Scale) and the GAAIS (General Attitudes Towards Artificial Intelligence Scale). Further, two single items assessing positive and negative attitudes toward AI were added to the study to see if they would grasp substantial parts of the variance of the already established ATAI and GAAIS inventories. Correlations were of moderate to large effect size when comparing associations between the single-item measures and both ATAI and GAAI scales (German speaking sample 1 = 151 participants; German speaking sample 2 = 386). Finally, also associations with trusting the generative AI ChatGPT were included as external validation measurement in both investigated samples. Results revealed that all attitudes toward AI measures were associated with trusting ChatGPT. Moreover, a stepwise regression model demonstrated that the acceptance scale of the ATAI was the best predictor for trust in ChatGPT in sample 1, with more predictors in sample 2. The present work shows substantial overlap between the available attitudes towards AI measures, and this could be replicated in two samples. These insights can help future researchers and AI designers to choose the appropriate survey tool when considering to assess attitudes toward AI.},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  langid = {english},
  annotation = {https://osf.io/pmdhn/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/3N5ZK5KB/Montag and Ali - 2025 - Can We Assess Attitudes Toward AI with Single Items Associations with Existing Attitudes Toward AI.pdf}
}

@misc{mooreInvestigatingHumanAlignedLarge2025,
  title = {Investigating {{Human-Aligned Large Language Model Uncertainty}}},
  author = {Moore, Kyle and Roberts, Jesse and Watson, Daryl and Wisniewski, Pamela},
  year = {2025},
  month = mar,
  number = {arXiv:2503.12528},
  eprint = {2503.12528},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.12528},
  urldate = {2025-03-27},
  abstract = {Recent work has sought to quantify large language model uncertainty to facilitate model control and modulate user trust. Previous works focus on measures of uncertainty that are theoretically grounded or reflect the average overt behavior of the model. In this work, we investigate a variety of uncertainty measures, in order to identify measures that correlate with human group-level uncertainty. We find that Bayesian measures and a variation on entropy measures, top-k entropy, tend to agree with human behavior as a function of model size. We find that some strong measures decrease in human-similarity with model size, but, by multiple linear regression, we find that combining multiple uncertainty measures provide comparable human-alignment with reduced size-dependency.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Zotero/storage/4XZX6VTW/Moore et al. - 2025 - Investigating Human-Aligned Large Language Model Uncertainty.pdf;/Users/thomasgorman/Zotero/storage/R7T54CST/2503.html}
}

@article{morales-garciaAdaptationPsychometricProperties2024,
  title = {Adaptation and Psychometric Properties of a Brief Version of the General Self-Efficacy Scale for Use with Artificial Intelligence ({{GSE-6AI}}) among University Students},
  author = {{Morales-Garc{\'i}a}, Wilter C. and {Sairitupa-Sanchez}, Liset Z. and {Morales-Garc{\'i}a}, Sandra B. and {Morales-Garc{\'i}a}, Mardel},
  year = {2024},
  month = mar,
  journal = {Frontiers in Education},
  volume = {9},
  publisher = {Frontiers},
  issn = {2504-284X},
  doi = {10.3389/feduc.2024.1293437},
  urldate = {2025-05-02},
  abstract = {Background Individual beliefs about one's ability to carry out tasks and face challenges play a pivotal role in academic and professional formation. In the contemporary technological landscape, Artificial Intelligence (AI) is effecting profound changes across multiple sectors. Adaptation to this technology varies greatly among individuals. The integration of AI in the educational setting has necessitated a tool that measures self-efficacy concerning the adoption and use of this technology. Objective To adapt and validate a short version of the General Self-Efficacy Scale (GSE-6) for self-efficacy in the use of Artificial Intelligence (GSE-6AI) in a university student population. Methods An instrumental study was conducted with the participation of 469 medical students aged between 18 and 29 (M = 19.71; SD = 2.47). The GSE-6 was adapted to the AI context, following strict translation and cultural adaptation procedures. Its factorial structure was evaluated through confirmatory factorial analysis (CFA). Additionally, the factorial invariance of the scale based on gender was studied. Results The GSE-6AI exhibited a unidimensional structure with excellent fit indices. All item factorial loads surpassed the recommended threshold, and both Cronbach's Alpha ({$\alpha$}) and McDonald's Omega ({$\omega$}) achieved a value of 0.91. Regarding factorial invariance by gender, the scale proved to maintain its structure and meaning in both men and women. Conclusion The adapted GSE-6AI version is a valid and reliable tool for measuring self-efficacy in the use of Artificial Intelligence among university students. Its unidimensional structure and gender-related factorial invariance make it a robust and versatile tool for future research and practical applications in educational and technological contexts.},
  langid = {english},
  keywords = {adaptation,Artificial intelligence (AI),GSE-6AI,invariance,self-efficacy,Technological},
  file = {/Users/thomasgorman/Zotero/storage/7LGUXQNG/Morales-García et al. - 2024 - Adaptation and psychometric properties of a brief version of the general self-efficacy scale for use.pdf}
}

@misc{morrillShortformAILiteracy2023,
  title = {A Short-Form {{AI}} Literacy Intervention Can Reduce over-Reliance on {{AI}}},
  author = {Morrill, Jake and Noetel, Michael},
  year = {2023},
  month = dec,
  publisher = {OSF},
  doi = {10.31234/osf.io/hv9qc},
  urldate = {2025-04-28},
  abstract = {Artificial intelligence (AI) is becoming very capable, and can match or even exceed human performance in various tasks. Human-AI teams are necessary to maintain oversight over AI, however, humans tend to over-rely on AI, especially when their trust is high. New research has emerged showing that interventions that improve AI literacy---knowledge and understanding of AI---appear to reduce over-reliance on AI, however it is unclear what is mediating this effect. Further, AI literacy in the general public is poor, and short-form interventions that aim to teach about important AI concepts are sparse. We aimed to test whether a short, 5-minute text-based AI literacy intervention would improve AI literacy and reduce over-reliance on AI. We recruited 153 undergraduate psychology students from the University of Queensland and randomly assigned them to receive either the AI literacy intervention or a similarly-lengthed control material unrelated to AI, and subsequently tested their reliance on AI in a human-AI team context. We found that, compared with the control condition, those who received the literacy intervention had significantly improved AI literacy and significantly reduced over-reliance, though there was no indirect effect of the intervention on over-reliance through AI literacy or trust. Our findings highlight the potential for short-form AI literacy interventions in not only improving AI literacy but reducing over-reliance on AI in a human-AI team context, however, more research is necessary to bring clarity to what may be mediating this effect.},
  archiveprefix = {OSF},
  langid = {american},
  keywords = {AI,AI Literacy,Artificial Intelligence,Reliance,Trust,Trust Calibration},
  file = {/Users/thomasgorman/Zotero/storage/YE9WA6UP/Morrill and Noetel - 2023 - A short-form AI literacy intervention can reduce over-reliance on AI.pdf}
}

@article{ngDesignValidationAI2024,
  title = {Design and Validation of the {{AI}} Literacy Questionnaire: {{The}} Affective, Behavioural, Cognitive and Ethical Approach},
  shorttitle = {Design and Validation of the {{AI}} Literacy Questionnaire},
  author = {Ng, Davy Tsz Kit and Wu, Wenjie and Leung, Jac Ka Lok and Chiu, Thomas Kin Fung and Chu, Samuel Kai Wah},
  year = {2024},
  journal = {British Journal of Educational Technology},
  volume = {55},
  number = {3},
  pages = {1082--1104},
  issn = {1467-8535},
  doi = {10.1111/bjet.13411},
  urldate = {2025-05-02},
  abstract = {Artificial intelligence (AI) literacy is at the top of the agenda for education today in developing learners' AI knowledge, skills, attitudes and values in the 21st century. However, there are few validated research instruments for educators to examine how secondary students develop and perceive their learning outcomes. After reviewing the literature on AI literacy questionnaires, we categorized the identified competencies in four dimensions: (1) affective learning (intrinsic motivation and self-efficacy/confidence), (2) behavioural learning (behavioural commitment and collaboration), (3) cognitive learning (know and understand; apply, evaluate and create) and (4) ethical learning. Then, a 32-item self-reported questionnaire on AI literacy (AILQ) was developed and validated to measure students' literacy development in the four dimensions. The design and validation of AILQ were examined through theoretical review, expert judgement, interview, pilot study and first- and second-order confirmatory factor analysis. This article reports the findings of a pilot study using a preliminary version of the AILQ among 363 secondary school students in Hong Kong to analyse the psychometric properties of the instrument. Results indicated a four-factor structure of the AILQ and revealed good reliability and validity. The AILQ is recommended as a reliable measurement scale for assessing how secondary students foster their AI literacy and inform better instructional design based on the proposed affective, behavioural, cognitive and ethical (ABCE) learning framework. Practitioner notes What is already known about this topic AI literacy has drawn increasing attention in recent years and has been identified as an important digital literacy. Schools and universities around the world started to incorporate AI into their curriculum to foster young learners' AI literacy. Some studies have worked to design suitable measurement tools, especially questionnaires, to examine students' learning outcomes in AI learning programmes. What this paper adds Develops an AI literacy questionnaire (AILQ) to evaluate students' literacy development in terms of affective, behavioural, cognitive and ethical (ABCE) dimensions. Proposes a parsimonious model based on the ABCE framework and addresses a skill set of AI literacy. Implications for practice and/or policy Researchers are able to use the AILQ as a guide to measure students' AI literacy. Practitioners are able to use the AILQ to assess students' AI literacy development.},
  copyright = {{\copyright} 2023 The Authors. British Journal of Educational Technology published by John Wiley \& Sons Ltd on behalf of British Educational Research Association.},
  langid = {english},
  keywords = {AI education,AI literacy,AI literacy questionnaire (AILQ),artificial intelligence,questionnaire validation},
  annotation = {https://bera-journals.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1111\%2Fbjet.13411\&file=bjet13411-sup-0001-AppendixS1.docx},
  file = {/Users/thomasgorman/Zotero/storage/BSIK3K2M/Ng et al. - 2024 - Design and validation of the AI literacy questionnaire The affective, behavioural, cognitive and et.pdf;/Users/thomasgorman/Zotero/storage/LUZY3DG5/bjet.html}
}

@misc{niMeasurementLLMsPhilosophies2025,
  title = {Measurement of {{LLM}}'s {{Philosophies}} of {{Human Nature}}},
  author = {Ni, Minheng and Wu, Ennan and Gong, Zidong and Yang, Zhengyuan and Li, Linjie and Lin, Chung-Ching and Lin, Kevin and Wang, Lijuan and Zuo, Wangmeng},
  year = {2025},
  month = apr,
  number = {arXiv:2504.02304},
  eprint = {2504.02304},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.02304},
  urldate = {2025-05-05},
  abstract = {The widespread application of artificial intelligence (AI) in various tasks, along with frequent reports of conflicts or violations involving AI, has sparked societal concerns about interactions with AI systems. Based on Wrightsman's Philosophies of Human Nature Scale (PHNS), a scale empirically validated over decades to effectively assess individuals' attitudes toward human nature, we design the standardized psychological scale specifically targeting large language models (LLM), named the Machine-based Philosophies of Human Nature Scale (M-PHNS). By evaluating LLMs' attitudes toward human nature across six dimensions, we reveal that current LLMs exhibit a systemic lack of trust in humans, and there is a significant negative correlation between the model's intelligence level and its trust in humans. Furthermore, we propose a mental loop learning framework, which enables LLM to continuously optimize its value system during virtual interactions by constructing moral scenarios, thereby improving its attitude toward human nature. Experiments demonstrate that mental loop learning significantly enhances their trust in humans compared to persona or instruction prompts. This finding highlights the potential of human-based psychological assessments for LLM, which can not only diagnose cognitive biases but also provide a potential solution for ethical learning in artificial intelligence. We release the M-PHNS evaluation code and data at https://github.com/kodenii/M-PHNS.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/kodenii/M-PHNS},
  file = {/Users/thomasgorman/Zotero/storage/PGWR669A/Ni et al. - 2025 - Measurement of LLM's Philosophies of Human Nature.pdf;/Users/thomasgorman/Zotero/storage/FA7LJ3QF/2504.html}
}

@article{ovsyannikovaThirdpartyEvaluatorsPerceive2025,
  title = {Third-Party Evaluators Perceive {{AI}} as More Compassionate than Expert Humans},
  author = {Ovsyannikova, Dariya and De Mello, Victoria Oldemburgo and Inzlicht, Michael},
  year = {2025},
  month = jan,
  journal = {Communications Psychology},
  volume = {3},
  number = {1},
  issn = {2731-9121},
  doi = {10.1038/s44271-024-00182-6},
  urldate = {2025-04-30},
  abstract = {AbstractEmpathy connects us but strains under demanding settings. This study explored how third parties evaluated AI-generated empathetic responses versus human responses in terms of compassion, responsiveness, and overall preference across four preregistered experiments. Participants (N\,=\,556) read empathy prompts describing valenced personal experiences and compared the AI responses to select non-expert or expert humans. Results revealed that AI responses were preferred and rated as more compassionate compared to select human responders (Study 1). This pattern of results remained when author identity was made transparent (Study 2), when AI was compared to expert crisis responders (Study 3), and when author identity was disclosed to all participants (Study 4). Third parties perceived AI as being more responsive---conveying understanding, validation, and care---which partially explained AI's higher compassion ratings in Study 4. These findings suggest that AI has robust utility in contexts requiring empathetic interaction, with the potential to address the increasing need for empathy in supportive communication contexts.},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  langid = {english},
  annotation = {https://osf.io/wjx48/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/S4HCRTP4/Ovsyannikova et al. - 2025 - Third-party evaluators perceive AI as more compassionate than expert humans.pdf}
}

@misc{padhiCalibratingUncertaintyQuantification2025,
  title = {Calibrating {{Uncertainty Quantification}} of {{Multi-Modal LLMs}} Using {{Grounding}}},
  author = {Padhi, Trilok and Kaur, Ramneet and Cobb, Adam D. and Acharya, Manoj and Roy, Anirban and Samplawski, Colin and Matejek, Brian and Berenbeim, Alexander M. and Bastian, Nathaniel D. and Jha, Susmit},
  year = {2025},
  month = apr,
  number = {arXiv:2505.03788},
  eprint = {2505.03788},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.03788},
  urldate = {2025-05-12},
  abstract = {We introduce a novel approach for calibrating uncertainty quantification (UQ) tailored for multi-modal large language models (LLMs). Existing state-of-the-art UQ methods rely on consistency among multiple responses generated by the LLM on an input query under diverse settings. However, these approaches often report higher confidence in scenarios where the LLM is consistently incorrect. This leads to a poorly calibrated confidence with respect to accuracy. To address this, we leverage cross-modal consistency in addition to self-consistency to improve the calibration of the multi-modal models. Specifically, we ground the textual responses to the visual inputs. The confidence from the grounding model is used to calibrate the overall confidence. Given that using a grounding model adds its own uncertainty in the pipeline, we apply temperature scaling - a widely accepted parametric calibration technique - to calibrate the grounding model's confidence in the accuracy of generated responses. We evaluate the proposed approach across multiple multi-modal tasks, such as medical question answering (Slake) and visual question answering (VQAv2), considering multi-modal models such as LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework achieves significantly improved calibration on both tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/thomasgorman/Zotero/storage/PNRXQG7K/Padhi et al. - 2025 - Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding.pdf;/Users/thomasgorman/Zotero/storage/N7U8JNE2/2505.html}
}

@article{padillaUncertainUncertaintyHow2021,
  title = {Uncertain {{About Uncertainty}}: {{How Qualitative Expressions}} of {{Forecaster Confidence Impact Decision-Making With Uncertainty Visualizations}}},
  shorttitle = {Uncertain {{About Uncertainty}}},
  author = {Padilla, Lace M. K. and Powell, Maia and Kay, Matthew and Hullman, Jessica},
  year = {2021},
  journal = {Frontiers in Psychology},
  volume = {11},
  issn = {1664-1078},
  urldate = {2023-08-21},
  abstract = {When forecasting events, multiple types of uncertainty are often inherently present in the modeling process. Various uncertainty typologies exist, and each type of uncertainty has different implications a scientist might want to convey. In this work, we focus on one type of distinction between direct quantitative uncertainty and indirect qualitative uncertainty. Direct quantitative uncertainty describes uncertainty about facts, numbers, and hypotheses that can be communicated in absolute quantitative forms such as probability distributions or confidence intervals. Indirect qualitative uncertainty describes the quality of knowledge concerning how effectively facts, numbers, or hypotheses represent reality, such as evidence confidence scales proposed by the Intergovernmental Panel on Climate Change. A large body of research demonstrates that both experts and novices have difficulty reasoning with quantitative uncertainty, and visualizations of uncertainty can help with such traditionally challenging concepts. However, the question of if, and how, people may reason with multiple types of uncertainty associated with a forecast remains largely unexplored. In this series of studies, we seek to understand if individuals can integrate indirect uncertainty about how ``good'' a model is (operationalized as a qualitative expression of forecaster confidence) with quantified uncertainty in a prediction (operationalized as a quantile dotplot visualization of a predicted distribution). Our first study results suggest that participants utilize both direct quantitative uncertainty and indirect qualitative uncertainty when conveyed as quantile dotplots and forecaster confidence. In manipulations where forecasters were less sure about their prediction, participants made more conservative judgments. In our second study, we varied the amount of quantified uncertainty (in the form of the SD of the visualized distributions) to examine how participants' decisions changed under different combinations of quantified uncertainty (variance) and qualitative uncertainty (low, medium, and high forecaster confidence). The second study results suggest that participants updated their judgments in the direction predicted by both qualitative confidence information (e.g., becoming more conservative when the forecaster confidence is low) and quantitative uncertainty (e.g., becoming more conservative when the variance is increased). Based on the findings from both experiments, we recommend that forecasters present qualitative expressions of model confidence whenever possible alongside quantified uncertainty.},
  annotation = {https://osf.io/atr57/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Padilla et al_2021_Uncertain About Uncertainty.pdf}
}

@article{pataranutapornInfluencingHumanAI2023,
  title = {Influencing Human--{{AI}} Interaction by Priming Beliefs about {{AI}} Can Increase Perceived Trustworthiness, Empathy and Effectiveness},
  author = {Pataranutaporn, Pat and Liu, Ruby and Finn, Ed and Maes, Pattie},
  year = {2023},
  month = oct,
  journal = {Nature Machine Intelligence},
  volume = {5},
  number = {10},
  pages = {1076--1086},
  issn = {2522-5839},
  doi = {10.1038/s42256-023-00720-7},
  urldate = {2024-11-15},
  abstract = {As conversational agents powered by large language models become more human-like, users are starting to view them as companions rather than mere assistants. Our study explores how changes to a person's mental model of an AI system affects their interaction with the system. Participants interacted with the same conversational AI, but were influenced by different priming statements regarding the AI's inner motives: caring, manipulative or no motives. Here we show that those who perceived a caring motive for the AI also perceived it as more trustworthy, empathetic and better-performing, and that the effects of priming and initial mental models were stronger for a more sophisticated AI model. Our work also indicates a feedback loop in which the user and AI reinforce the user's mental model over a short time; further work should investigate long-term effects. The research highlights the importance of how AI systems are introduced can notably affect the interaction and how the AI is experienced.},
  langid = {english},
  annotation = {https://github.com/mitmedialab/nmi-ai-2023\\
\\
https://zenodo.org/records/8136979},
  file = {/Users/thomasgorman/Zotero/storage/CYI4H6N5/Pataranutaporn et al. - 2023 - Influencing human–AI interaction by priming beliefs about AI can increase perceived trustworthiness,.pdf}
}

@inproceedings{perrigTrustIssuesTrust2023,
  title = {Trust {{Issues}} with {{Trust Scales}}: {{Examining}} the {{Psychometric Quality}} of {{Trust Measures}} in the {{Context}} of {{AI}}},
  shorttitle = {Trust {{Issues}} with {{Trust Scales}}},
  booktitle = {Extended {{Abstracts}} of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Perrig, Sebastian A. C. and Scharowski, Nicolas and Br{\"u}hlmann, Florian},
  year = {2023},
  month = apr,
  pages = {1--7},
  publisher = {ACM},
  address = {Hamburg Germany},
  doi = {10.1145/3544549.3585808},
  urldate = {2025-05-01},
  file = {/Users/thomasgorman/Zotero/storage/9CPQD79F/Perrig et al. - 2023 - Trust Issues with Trust Scales Examining the Psychometric Quality of Trust Measures in the Context.pdf}
}

@article{pope-caldwellVariabilityHarshnessShape2024,
  title = {Variability and Harshness Shape Flexible Strategy-Use in Support of the Constrained Flexibility Framework},
  author = {{Pope-Caldwell}, Sarah and Deffner, Dominik and Maurits, Luke and Neumann, Terrence and Haun, Daniel},
  year = {2024},
  month = mar,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {7236},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-57800-w},
  urldate = {2025-05-13},
  abstract = {Human cognition is incredibly flexible, allowing us to thrive within diverse environments. However, humans also tend to stick to familiar strategies, even when there are better solutions available. How do we exhibit flexibility in some contexts, yet inflexibility in others? The constrained flexibility framework (CFF) proposes that cognitive flexibility is shaped by variability, predictability, and harshness within decision-making environments. The CFF asserts that high elective switching (switching away from a working strategy) is maladaptive in stable or predictably variable environments, but adaptive in unpredictable environments, so long as harshness is low. Here we provide evidence for the CFF using a decision-making task completed across two studies with a total of 299 English-speaking adults. In line with the CFF, we found that elective switching was suppressed by harshness, using both within- and between-subjects harshness manipulations. Our results highlight the need to study how cognitive flexibility adapts to diverse contexts.},
  langid = {english},
  annotation = {https://github.com/sarahpopecaldwell/HarshFlex\_BanditJars},
  file = {/Users/thomasgorman/Zotero/storage/5DEATW6S/Pope-Caldwell et al. - 2024 - Variability and harshness shape flexible strategy-use in support of the constrained flexibility fram.pdf}
}

@misc{puppartShorttermAILiteracy2025,
  title = {Short-Term {{AI}} Literacy Intervention Does Not Reduce over-Reliance on Incorrect {{ChatGPT}} Recommendations},
  author = {Puppart, Brett and Aru, Jaan},
  year = {2025},
  month = mar,
  number = {arXiv:2503.10556},
  eprint = {2503.10556},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.10556},
  urldate = {2025-04-28},
  abstract = {In this study, we examined whether a short-form AI literacy intervention could reduce the adoption of incorrect recommendations from large language models. High school seniors were randomly assigned to either a control or an intervention group, which received an educational text explaining ChatGPT's working mechanism, limitations, and proper use. Participants solved math puzzles with the help of ChatGPT's recommendations, which were incorrect in half of the cases. Results showed that students adopted incorrect suggestions 52.1\% of the time, indicating widespread over-reliance. The educational intervention did not significantly reduce over-reliance. Instead, it led to an increase in ignoring ChatGPT's correct recommendations. We conclude that the usage of ChatGPT is associated with over-reliance and it is not trivial to increase AI literacy to counter over-reliance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Quantitative Biology - Neurons and Cognition},
  file = {/Users/thomasgorman/Zotero/storage/R4QHZNXK/Puppart and Aru - 2025 - Short-term AI literacy intervention does not reduce over-reliance on incorrect ChatGPT recommendatio.pdf;/Users/thomasgorman/Zotero/storage/XN87SGPV/2503.html}
}

@misc{quillienGuessesCompressedProbability2023,
  title = {Guesses as Compressed Probability Distributions},
  author = {Quillien, Tadeg and Bramley, Neil R and Lucas, Christopher G.},
  year = {2023},
  month = nov,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/gy2fv},
  urldate = {2025-05-13},
  abstract = {People often make judgments about uncertain facts and events, for example `Germany will win the world cup'. Here we present a rational analysis of these judgments: we argue that a guess functions as a compressed encoding of the speaker's subjective probability distribution over relevant possibilities. So, a statement like `X will happen' encodes information not only about the probability of X but also, implicitly, about the probability of other possible outcomes. We test formal computational models derived from our theory, showing in four experiments that they accurately predict how people make and interpret guesses. Our account naturally explains why people dislike vacuously-correct guesses (like `Some country will win the world cup'), and it might shed light on apparently sub-optimal patterns of judgment such as the conjunction fallacy.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  annotation = {https://osf.io/wz649/?view\_only=8d7019ee2b8d456d8c3c9b29049b75aa\\
\\
https://www.bramleylab.ppls.ed.ac.uk/publication/2025-01-01\_quillien2025guesses/},
  file = {/Users/thomasgorman/Zotero/storage/P6PFAF5I/Quillien et al. - 2023 - Guesses as compressed probability distributions.pdf}
}

@article{rahnevConfidenceDatabase2020,
  title = {The {{Confidence Database}}},
  author = {Rahnev, Dobromir and Desender, Kobe and Lee, Alan L. F. and Adler, William T. and {Aguilar-Lleyda}, David and Akdo{\u g}an, Ba{\c s}ak and Arbuzova, Polina and Atlas, Lauren Y. and Balc{\i}, Fuat and Bang, Ji Won and B{\`e}gue, Indrit and Birney, Damian P. and Brady, Timothy F. and {Calder-Travis}, Joshua and Chetverikov, Andrey and Clark, Torin K. and Davranche, Karen and Denison, Rachel N. and Dildine, Troy C. and Double, Kit S. and Duyan, Yal{\c c}{\i}n A. and Faivre, Nathan and Fallow, Kaitlyn and Filevich, Elisa and Gajdos, Thibault and Gallagher, Regan M. and {de Gardelle}, Vincent and Gherman, Sabina and Haddara, Nadia and Hainguerlot, Marine and Hsu, Tzu-Yu and Hu, Xiao and Iturrate, I{\~n}aki and Jaquiery, Matt and Kantner, Justin and Koculak, Marcin and Konishi, Mahiko and Ko{\ss}, Christina and Kvam, Peter D. and Kwok, Sze Chai and Lebreton, Ma{\"e}l and Lempert, Karolina M. and Ming Lo, Chien and Luo, Liang and Maniscalco, Brian and Martin, Antonio and Massoni, S{\'e}bastien and Matthews, Julian and Mazancieux, Audrey and Merfeld, Daniel M. and O'Hora, Denis and Palser, Eleanor R. and Paulewicz, Borys{\l}aw and Pereira, Michael and Peters, Caroline and Philiastides, Marios G. and Pfuhl, Gerit and Prieto, Fernanda and Rausch, Manuel and Recht, Samuel and Reyes, Gabriel and Rouault, Marion and Sackur, J{\'e}r{\^o}me and Sadeghi, Saeedeh and Samaha, Jason and Seow, Tricia X. F. and Shekhar, Medha and Sherman, Maxine T. and Siedlecka, Marta and Sk{\'o}ra, Zuzanna and Song, Chen and Soto, David and Sun, Sai and {van Boxtel}, Jeroen J. A. and Wang, Shuo and Weidemann, Christoph T. and Weindel, Gabriel and Wierzcho{\'n}, Micha{\l} and Xu, Xinming and Ye, Qun and Yeon, Jiwon and Zou, Futing and Zylberberg, Ariel},
  year = {2020},
  month = mar,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {3},
  pages = {317--325},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0813-1},
  urldate = {2022-05-29},
  abstract = {Understanding how people rate their confidence is critical for the characterization of a wide range of perceptual, memory, motor and cognitive processes. To enable the continued exploration of these processes, we created a large database of confidence studies spanning a broad set of paradigms, participant populations and fields of study. The data from each study are structured in a common, easy-to-use format that can be easily imported and analysed using multiple software packages. Each dataset is accompanied by an explanation regarding the nature of the collected data. At the time of publication, the Confidence Database (which is available at https://osf.io/s46pr/) contained 145 datasets with data from more than 8,700 participants and almost 4\,million trials. The database will remain open for new submissions indefinitely and is expected to continue to grow. Here we show the usefulness of this large collection of datasets in four different analyses that provide precise estimations of several foundational confidence-related effects.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Human behaviour,Psychology},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/rahnevConfidenceDatabase2020-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Rahnev et al_2020_The Confidence Database.pdf;/Users/thomasgorman/Zotero/storage/A2AGJVHN/s41562-019-0813-1.html}
}

@article{ravseljHigherEducationStudents2025,
  title = {Higher Education Students' Perceptions of {{ChatGPT}}: {{A}} Global Study of Early Reactions},
  shorttitle = {Higher Education Students' Perceptions of {{ChatGPT}}},
  author = {Rav{\v s}elj, Dejan and Ker{\v z}i{\v c}, Damijana and Toma{\v z}evi{\v c}, Nina and Umek, Lan and Brezovar, Nejc and A. Iahad, Noorminshah and Abdulla, Ali Abdulla and Akopyan, Anait and Aldana Segura, Magdalena Waleska and AlHumaid, Jehan and Allam, Mohamed Farouk and All{\'o}, Maria and Andoh, Raphael Papa Kweku and Andronic, Octavian and Arthur, Yarhands Dissou and Ayd{\i}n, Fatih and Badran, Amira and {Balbont{\'i}n-Alvarado}, Roxana and Ben Saad, Helmi and Bencsik, Andrea and Benning, Isaac and Besimi, Adrian and Bezerra, Denilson da Silva and Buizza, Chiara and Burro, Roberto and Bwalya, Anthony and Cachero, Cristina and {Castillo-Briceno}, Patricia and Castro, Harold and Chai, Ching Sing and Charalambous, Constadina and Chiu, Thomas K. F. and Clipa, Otilia and Colombari, Ruggero and Corral Escobedo, Luis Jos{\'e} H. and Costa, El{\'i}sio and Cre{\textcommabelow t}ulescu, Radu George and Crispino, Marta and Cucari, Nicola and Dalton, Fergus and Demir Kaya, Meva and {Dumi{\'c}-{\v C}ule}, Ivo and Dwidienawati, Diena and Ebardo, Ryan and Egbenya, Daniel Lawer and Faris, MoezAlIslam Ezzat and Fe{\v c}ko, Miroslav and Ferrinho, Paulo and Florea, Adrian and Fong, Chun Yuen and Francis, Zo{\"e} and Ghilardi, Alberto and {Gonz{\'a}lez-Fern{\'a}ndez}, Belinka and Hau, Daniela and Hossain, Md. Shamim and Hug, Theo and Inasius, Fany and Ismail, Maryam Jaffar and Jahi{\'c}, Hatid{\v z}a and Jessa, Morrison Omokiniovo and Kapanadze, Marika and Kar, Sujita Kumar and Kateeb, Elham Talib and Kaya, Feridun and Khadri, Hanaa Ouda and Kikuchi, Masao and Kobets, Vitaliy Mykolayovych and Kostova, Katerina Metodieva and Krasmane, Evita and Lau, Jesus and Law, Wai Him Crystal and Laz{\u a}r, Florin and {Lazovi{\'c}-Pita}, Lejla and Lee, Vivian Wing Yan and Li, Jingtai and {L{\'o}pez-Aguilar}, Diego Vinicio and Luca, Adrian and Luciano, Ruth Garcia and {Machin-Mastromatteo}, Juan D. and Madi, Marwa and Manguele, Alexandre Louren{\c c}o and Manrique, Rub{\'e}n Francisco and Mapulanga, Thumah and Marimon, Frederic and Marinova, Galia Ilieva and {Mas-Machuca}, Marta and {Mej{\'i}a-Rodr{\'i}guez}, Oliva and {Meletiou-Mavrotheris}, Maria and {M{\'e}ndez-Prado}, Silvia Mariela and {Meza-Cano}, Jos{\'e} Manuel and Mir{\c k}e, Evija and Mishra, Alpana and Mital, Ondrej and Mollica, Cristina and Morariu, Daniel Ionel and Mospan, Natalia and Mukuka, Angel and Navarro Jim{\'e}nez, Silvana Guadalupe and Nikaj, Irena and Nisheva, Maria Mihaylova and Nisiforou, Efi and Njiku, Joseph and Nomnian, Singhanat and {Nuredini-Mehmedi}, Lulzime and Nyamekye, Ernest and Obadi{\'c}, Alka and Okela, Abdelmohsen Hamed and {Olenik-Shemesh}, Dorit and Ostoj, Izabela and {Peralta-Rizzo}, Kevin Javier and Pe{\v s}tek, Almir and {Pilav-Veli{\'c}}, Amila and Pires, Dilma Rosanda Miranda and Rabin, Eyal and Raccanello, Daniela and Ramie, Agustine and ur Rashid, Md. Mamun and Reuter, Robert A. P. and Reyes, Valentina and Rodrigues, Ana Sofia and Rodway, Paul and Ru{\v c}insk{\'a}, Silvia and Sadzaglishvili, Shorena and Salem, Ashraf Atta M. S. and Savi{\'c}, Gordana and Schepman, Astrid and Shahpo, Samia Mokhtar and Snouber, Abdelmajid and Soler, Emma and Sonyel, Bengi and Stefanova, Eliza and Stone, Anna and Strzelecki, Artur and Tanaka, Tetsuji and Tapia Cortes, Carolina and {Teira-Fachado}, Andrea and Tilga, Henri and Titko, Jelena and Tolmach, Maryna and Turmudi, Dedi and {Varela-Candamio}, Laura and Vekiri, Ioanna and Vicentini, Giada and Woyo, Erisher and Yorulmaz, {\"O}zlem and Yunus, Said A. S. and Zamfir, Ana-Maria and Zhou, Munyaradzi and Aristovnik, Aleksander},
  year = {2025},
  month = feb,
  journal = {PLOS ONE},
  volume = {20},
  number = {2},
  pages = {e0315011},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0315011},
  urldate = {2025-05-02},
  abstract = {The paper presents the most comprehensive and large-scale global study to date on how higher education students perceived the use of ChatGPT in early 2024. With a sample of 23,218 students from 109 countries and territories, the study reveals that students primarily used ChatGPT for brainstorming, summarizing texts, and finding research articles, with a few using it for professional and creative writing. They found it useful for simplifying complex information and summarizing content, but less reliable for providing information and supporting classroom learning, though some considered its information clearer than that from peers and teachers. Moreover, students agreed on the need for AI regulations at all levels due to concerns about ChatGPT promoting cheating, plagiarism, and social isolation. However, they believed ChatGPT could potentially enhance their access to knowledge and improve their learning experience, study efficiency, and chances of achieving good grades. While ChatGPT was perceived as effective in potentially improving AI literacy, digital communication, and content creation skills, it was less useful for interpersonal communication, decision-making, numeracy, native language proficiency, and the development of critical thinking skills. Students also felt that ChatGPT would boost demand for AI-related skills and facilitate remote work without significantly impacting unemployment. Emotionally, students mostly felt positive using ChatGPT, with curiosity and calmness being the most common emotions. Further examinations reveal variations in students' perceptions across different socio-demographic and geographic factors, with key factors influencing students' use of ChatGPT also being identified. Higher education institutions' managers and teachers may benefit from these findings while formulating the curricula and instructions/regulations for ChatGPT use, as well as when designing the teaching methods and assessment tools. Moreover, policymakers may also consider the findings when formulating strategies for secondary and higher education system development, especially in light of changing labor market needs and related digital skills development.},
  annotation = {https://data.mendeley.com/datasets/ymg9nsn6kn/2},
  file = {/Users/thomasgorman/Zotero/storage/CKJUXMHM/Ravšelj et al. - 2025 - Higher education students’ perceptions of ChatGPT A global study of early reactions.pdf}
}

@article{razinConvergingMeasuresEmergent2024,
  title = {Converging {{Measures}} and an {{Emergent Model}}: {{A Meta-Analysis}} of {{Human-Machine Trust Questionnaires}}},
  shorttitle = {Converging {{Measures}} and an {{Emergent Model}}},
  author = {Razin, Yosef S. and Feigh, Karen M.},
  year = {2024},
  month = nov,
  journal = {J. Hum.-Robot Interact.},
  volume = {13},
  number = {4},
  pages = {58:1--58:41},
  doi = {10.1145/3677614},
  urldate = {2025-05-02},
  abstract = {Trust is crucial for technological acceptance, continued usage, and teamwork. However, human-robot trust, and human-machine trust more generally, suffer from terminological disagreement and construct proliferation. By comparing, mapping, and analyzing well-constructed trust survey instruments, this work uncovers a consensus structure of trust in human--machine interaction. To do so, we identify the most frequently cited and best-validated human-machine and human-robot trust questionnaires as well as the best-established factors that form the dimensions and antecedents of such trust. To reduce both confusion and construct proliferation, we provide a detailed mapping of terminology between questionnaires. Furthermore, we perform a meta-analysis of the regression models which emerged from the experiments that employed multi-factorial survey instruments. Based on this meta-analysis, we provide the most complete, experimentally validated model of human-machine and human-robot trust to date. This convergent model establishes an integrated framework for future research. It determines the current boundaries of trust measurement and where further investigation and validation are necessary. We close by discussing how to choose an appropriate trust survey instrument and how to design for trust. By identifying the internal workings of trust, a more complete basis for measuring trust is developed that is widely applicable.},
  file = {/Users/thomasgorman/Zotero/storage/YGIQGGFS/Razin and Feigh - 2024 - Converging Measures and an Emergent Model A Meta-Analysis of Human-Machine Trust Questionnaires.pdf}
}

@inproceedings{rheuTrapAILiteracy2025,
  title = {The {{Trap}} of {{AI Literacy}}: {{The Paradoxical Relationships Between College Students}}' {{Use}} of {{LLMs}}, {{AI Literacy}}, and {{Fact-checking Behavior}}},
  shorttitle = {The {{Trap}} of {{AI Literacy}}},
  booktitle = {Proceedings of the {{Extended Abstracts}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Rheu, Minjin (MJ) and Cho, Janghee},
  year = {2025},
  month = apr,
  series = {{{CHI EA}} '25},
  pages = {1--7},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3706599.3719681},
  urldate = {2025-04-29},
  abstract = {This study examines factors influencing users' critical engagement with large language models (LLMs), focusing on fact-checking behavior. While LLMs transform how individuals acquire knowledge, their rapid adoption raises concerns about uncritical acceptance due to limitations like hallucinations. A survey of college students and young professionals revealed nuanced effects of LLM literacy. Understanding LLM processes, such as input, processing, and output, encourages fact-checking. However, self-efficacy and knowledge of LLM features paradoxically reduce verification by fostering reliance on machine heuristics and elevating the perceived credibility of outputs. These findings highlight the complex role of AI literacy in promoting critical engagement and the importance of education to deepen users' technological understanding.},
  isbn = {979-8-4007-1395-8},
  file = {/Users/thomasgorman/Zotero/storage/HQKXC9UF/Rheu and Cho - 2025 - The Trap of AI Literacy The Paradoxical Relationships Between College Students’ Use of LLMs, AI Lit.pdf}
}

@article{ryanUncertainWorldHow2025,
  title = {Uncertain World: {{How}} Children's Curiosity and Intolerance of Uncertainty Relate to Their Behaviour and Emotion under Uncertainty},
  shorttitle = {Uncertain World},
  author = {Ryan, Zoe J and Dodd, Helen F and FitzGibbon, Lily},
  year = {2025},
  month = apr,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {78},
  number = {4},
  pages = {842--860},
  publisher = {SAGE Publications},
  issn = {1747-0218},
  doi = {10.1177/17470218241252651},
  urldate = {2025-05-13},
  abstract = {Curiosity and intolerance of uncertainty (IU) are both thought to drive information seeking but may have different affective profiles; curiosity is often associated with positive affective responses to uncertainty and improved learning outcomes, whereas IU is associated with negative affective responses and anxiety. Curiosity and IU have not previously been examined together in children but may both play an important role in understanding how children respond to uncertainty. Our research aimed to examine how individual differences in parent-reported curiosity and IU were associated with behavioural and emotional responses to uncertainty. Children aged 8 to 12 (n?=?133) completed a game in which they were presented with an array of buttons on the screen that, when clicked, played neutral or aversive sounds. Children pressed buttons (information seeking) and rated their emotions and worry under conditions of high and low uncertainty. Facial expressions were also monitored for affective responses. Analyses revealed that children sought more information under high uncertainty than low uncertainty trials and that more curious children reported feeling happier. Contrary to expectations, IU and curiosity were not related to the number of buttons children pressed, nor to their self-reported emotion or worry. However, exploratory analyses suggest that children who are high in IU may engage in more information seeking that reflects checking or safety-seeking than those who are low in IU. In addition, our findings suggest that there may be age-related change in the effects of IU on worry, with IU more strongly related to worry in uncertain situations for older children than younger children.},
  file = {/Users/thomasgorman/Zotero/storage/T5R7DE7W/Ryan et al. - 2025 - Uncertain world How children’s curiosity and intolerance of uncertainty relate to their behaviour a.pdf}
}

@article{salahChattingChatGPTDecoding2024,
  title = {Chatting with {{ChatGPT}}: Decoding the Mind of {{Chatbot}} Users and Unveiling the Intricate Connections between User Perception, Trust and Stereotype Perception on Self-Esteem and Psychological Well-Being},
  shorttitle = {Chatting with {{ChatGPT}}},
  author = {Salah, Mohammed and Alhalbusi, Hussam and Ismail, Maria Mohd and Abdelfattah, Fadi},
  year = {2024},
  month = mar,
  journal = {Current Psychology},
  volume = {43},
  number = {9},
  pages = {7843--7858},
  issn = {1936-4733},
  doi = {10.1007/s12144-023-04989-0},
  urldate = {2025-03-12},
  abstract = {Artificial Intelligence (AI) technology has revolutionized how we interact with information and entertainment, with ChatGPT, a language model developed by OpenAI, being among its prominent applications. However, knowledge regarding the psychological impact of interacting with ChatGPT is limited. This study investigated the relationships between trust in ChatGPT; ChatGPT's user perceptions; perceived stereotyping by ChatGPT; and two psychological outcomes, namely, psychological well-being and self-esteem. This study hypothesized that the former three variables exhibit a positive direct relationship with self-esteem. Additionally, the study proposed that job anxiety moderates the associations among trust in ChatGPT, user perceptions of ChatGPT, and psychological well-being. Using a survey design, data were collected from 732 participants and analyzed using SEM and SmartPLS analysis. Notably, perceived stereotyping by ChatGPT significantly predicted self-esteem, while user perceptions of ChatGPT and trust in ChatGPT exhibited a positive direct relationship with self-esteem. Additionally, job anxiety moderated the relationship between ChatGPT's user perceptions and psychological well-being. These results provide important insights into the psychological effects of interacting with AI technology and highlight job anxiety's role in moderating these effects. This study's findings have implications for developing and using AI technology in various fields, including mental health and human-robot interactions.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature},
  langid = {english},
  annotation = {https://educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-024-00471-4/tables/6},
  file = {/Users/thomasgorman/Zotero/storage/RMK89WZB/Salah et al. - 2024 - Chatting with ChatGPT decoding the mind of Chatbot users and unveiling the intricate connections be.pdf}
}

@article{scantamburloArtificialIntelligenceEurope2025,
  title = {Artificial {{Intelligence Across Europe}}: {{A Study}} on {{Awareness}}, {{Attitude}} and {{Trust}}},
  shorttitle = {Artificial {{Intelligence Across Europe}}},
  author = {Scantamburlo, Teresa and Cort{\'e}s, Atia and Foffano, Francesca and Barru{\'e}, Cristian and Distefano, Veronica and Pham, Long and Fabris, Alessandro},
  year = {2025},
  month = feb,
  journal = {IEEE Transactions on Artificial Intelligence},
  volume = {6},
  number = {2},
  pages = {477--490},
  issn = {2691-4581},
  doi = {10.1109/TAI.2024.3461633},
  urldate = {2025-04-30},
  abstract = {This article presents the results of an extensive study investigating the opinions on artificial intelligence (AI) of a sample of 4006 European citizens from eight distinct countries (France, Germany, Italy, Netherlands, Poland, Romania, Spain, and Sweden). The aim of the study is to gain a better understanding of people's views and perceptions within the European context, which is already marked by important policy actions and regulatory processes. To survey the perceptions of the citizens of Europe, we design and validate a new questionnaire (PAICE) structured around three dimensions: people's awareness, attitude, and trust. We observe that while awareness is characterized by a low level of self-assessed competency, the attitude toward AI is very positive for more than half of the population. Reflecting on the collected results, we highlight implicit contradictions and identify trends that may interfere with the creation of an ecosystem of trust and the development of inclusive AI policies. The introduction of rules that ensure legal and ethical standards, along with the activity of high-level educational entities, and the promotion of AI literacy are identified as key factors in supporting a trustworthy AI ecosystem. We make some recommendations for AI governance focused on the European context and conclude with suggestions for future work.},
  keywords = {AI,AI Policy,Artificial intelligence,Ecosystems,Ethics,Europe,Market research,Privacy,Public Perception,Surveys},
  annotation = {https://github.com/EU-Survey/Material},
  file = {/Users/thomasgorman/Zotero/storage/AKM3623B/Scantamburlo et al. - 2025 - Artificial Intelligence Across Europe A Study on Awareness, Attitude and Trust.pdf}
}

@misc{scharowskiTrustDistrustTrust2025,
  title = {To {{Trust}} or {{Distrust Trust Measures}}: {{Validating Questionnaires}} for {{Trust}} in {{AI}}},
  shorttitle = {To {{Trust}} or {{Distrust Trust Measures}}},
  author = {Scharowski, Nicolas and Perrig, Sebastian A. C. and Aeschbach, Lena Fanya and von Felten, Nick and Opwis, Klaus and Wintersberger, Philipp and Br{\"u}hlmann, Florian},
  year = {2025},
  month = jan,
  number = {arXiv:2403.00582},
  eprint = {2403.00582},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.00582},
  urldate = {2025-04-28},
  abstract = {Despite the importance of trust in human-AI interactions, researchers must adopt questionnaires from other disciplines that lack validation in the AI context. Motivated by the need for reliable and valid measures, we investigated the psychometric quality of two trust questionnaires, the Trust between People and Automation scale (TPA) by Jian et al. (2000) and the Trust Scale for the AI Context (TAI) by Hoffman et al. (2023). In a pre-registered online experiment (N = 1485), participants observed interactions with trustworthy and untrustworthy AI (autonomous vehicle and chatbot). Results support the psychometric quality of the TAI while revealing opportunities to improve the TPA, which we outline in our recommendations for using the two questionnaires. Furthermore, our findings provide additional empirical evidence of trust and distrust as two distinct constructs that may coexist independently. Building on our findings, we highlight the opportunities and added value of measuring both trust and distrust in human-AI research and advocate for further work on both constructs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  annotation = {https://osf.io/7cdne/files/osfstorage?view\_only=ad812bc898154990959a50aaea43ca61},
  file = {/Users/thomasgorman/Zotero/storage/YYR5W4MR/Scharowski et al. - 2025 - To Trust or Distrust Trust Measures Validating Questionnaires for Trust in AI.pdf;/Users/thomasgorman/Zotero/storage/824DVBMF/2403.html}
}

@article{schepmanGeneralAttitudesArtificial2023,
  title = {The {{General Attitudes}} towards {{Artificial Intelligence Scale}} ({{GAAIS}}): {{Confirmatory Validation}} and {{Associations}} with {{Personality}}, {{Corporate Distrust}}, and {{General Trust}}},
  shorttitle = {The {{General Attitudes}} towards {{Artificial Intelligence Scale}} ({{GAAIS}})},
  author = {Schepman, Astrid and {and Rodway}, Paul},
  year = {2023},
  month = aug,
  journal = {International Journal of Human--Computer Interaction},
  volume = {39},
  number = {13},
  pages = {2724--2741},
  publisher = {Taylor \& Francis},
  issn = {1044-7318},
  doi = {10.1080/10447318.2022.2085400},
  urldate = {2025-05-03},
  abstract = {Acceptance of Artificial Intelligence (AI) may be predicted by individual psychological correlates, examined here. Study 1 reports confirmatory validation of the General Attitudes towards Artificial Intelligence Scale (GAAIS) following initial validation elsewhere. Confirmatory Factor Analysis confirmed the two-factor structure (Positive, Negative) and showed good convergent and divergent validity with a related scale. Study 2 tested whether psychological factors (Big Five personality traits, corporate distrust, and general trust) predicted attitudes towards AI. Introverts had more positive attitudes towards AI overall, likely because of algorithm appreciation. Conscientiousness and agreeableness were associated with forgiving attitudes towards negative aspects of AI. Higher corporate distrust led to negative attitudes towards AI overall, while higher general trust led to positive views of the benefits of AI. The dissociation between general trust and corporate distrust may reflect the public's attributions of the benefits and drawbacks of AI. Results are discussed in relation to theory and prior findings.},
  file = {/Users/thomasgorman/Zotero/storage/ULUSAKYZ/Schepman and and Rodway - 2023 - The General Attitudes towards Artificial Intelligence Scale (GAAIS) Confirmatory Validation and Ass.pdf}
}

@inproceedings{schille-hudsonBigHotBright2019,
  title = {Big, Hot, or Bright? {{Integrating}} Cues to Perceive Home Energy Use},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {{Schille-Hudson}, Eleanor B and Margehtis, Tyler and Miniard, Deidra and Landy, David and Attari, Shahzeen Z.},
  year = {2019},
  volume = {41},
  abstract = {Despite constantly using energy and having extensive interactions with household appliances, people consistently mis-estimate the amount of energy that is used by home appliances. This poses major problems for conservation efforts, while also presenting an interesting case study in human perception. Since many forms of energy used are not directly perceptible, and since the amount of energy that is being used by an appliance is often difficult to infer from appearances alone, people often rely on cues. Some of these cues are more reliable than others and previous literature has investigated which of these cues people rely on. However, past literature has always studied these proximal cues in isolation---despite the fact that, during real-world perception, people are always integrating a variety of cues. Here, we investigate how people rely on a variety of cues, and how individual differences in the reliance on those cues predicts the ability to estimate home energy use.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Schille-Hudson et al. - 2019 - Big, hot, or bright Integrating cues to perceive home energy use.pdf}
}

@article{schleyCognitiveAccessibilityJudgments2015,
  title = {Cognitive Accessibility in Judgments of Household Energy Consumption},
  author = {Schley, Dan R. and DeKay, Michael L.},
  year = {2015},
  month = sep,
  journal = {Journal of Environmental Psychology},
  volume = {43},
  pages = {30--41},
  issn = {02724944},
  doi = {10.1016/j.jenvp.2015.05.004},
  urldate = {2024-12-07},
  abstract = {Individuals appear to use the frequency that they interact with and think about various energyconsuming devices (the cognitive accessibility of those devices) when estimating relative energy consumption. This conclusion is based on 3 studies in which 608 participants estimated the percentages of total individual and household energy consumed annually in the U.S. by several categories of devices (e.g., lighting, cooking, water heating, air conditioning, computers, private motor vehicles) and 1 study in which 125 participants made similar estimates for their own consumption. Additionally, seasonal and geographical variations in local temperature predicted national annual consumption estimates for home heating and air conditioning, with these relationships being mediated by cognitive accessibility. Changes in available information, including more accessible cross-category and cross-fuel comparisons, greater media attention to high-consumption categories and high-impact solutions, and more disaggregated feedback regarding household energy use, could potentially improve consumers' understanding of relative consumption and hence their energy-conservation decisions.},
  langid = {english},
  annotation = {https://ars-els-cdn-com.proxyiub.uits.iu.edu/content/image/1-s2.0-S0272494415300049-mmc1.docx},
  file = {/Users/thomasgorman/Zotero/storage/2BFGJABS/Schley and DeKay - 2015 - Cognitive accessibility in judgments of household energy consumption.pdf}
}

@misc{schoeneggerWisdomSiliconCrowd2024,
  title = {Wisdom of the {{Silicon Crowd}}: {{LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy}}},
  shorttitle = {Wisdom of the {{Silicon Crowd}}},
  author = {Schoenegger, Philipp and Tuminauskaite, Indre and Park, Peter S. and Tetlock, Philip E.},
  year = {2024},
  month = jul,
  number = {arXiv:2402.19379},
  eprint = {2402.19379},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-16},
  abstract = {Human forecasting accuracy in practice relies on the `wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our preregistered main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is not statistically different from the human crowd. In exploratory analyses, we find that these two approaches are equivalent with respect to medium-effect-size equivalence bounds. We also observe an acquiescence effect, with mean model predictions being significantly above 50\%, despite an almost even split of positive and negative resolutions. Moreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2) can be improved by drawing on human cognitive output. We find that both models' forecasting accuracy benefits from exposure to the median human prediction as information, improving accuracy by between 17\% and 28\%: though this leads to less accurate predictions than simply averaging human and machine forecasts. Our results suggest that LLMs can achieve forecasting accuracy rivaling that of human crowd forecasting tournaments: via the simple, practically applicable method of forecast aggregation. This replicates the `wisdom of the crowd' effect for LLMs, and opens up their use for a variety of applications throughout society.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Schoenegger et al_2024_Wisdom of the Silicon Crowd.pdf}
}

@article{scholzMeasuringPropensityTrust2025,
  title = {Measuring the {{Propensity}} to {{Trust}} in {{Automated Technology}}: {{Examining Similarities}} to {{Dispositional Trust}} in {{Other Humans}} and {{Validation}} of the {{PTT-A Scale}}},
  shorttitle = {Measuring the {{Propensity}} to {{Trust}} in {{Automated Technology}}},
  author = {Scholz, David D. and , Johannes, Kraus and {and Miller}, Linda},
  year = {2025},
  month = jan,
  journal = {International Journal of Human--Computer Interaction},
  volume = {41},
  number = {2},
  pages = {970--993},
  issn = {1044-7318},
  doi = {10.1080/10447318.2024.2307691},
  urldate = {2025-05-02},
  abstract = {In this work, an integrative theoretical structure for the propensity to trust (PTT) is derived from literature. In an online study (N = 669), the validity of the structure was assessed and compared in two domains: propensity to trust in humans (PTT-H) and propensity to trust in automated technology (PTT-A). Based on this, an economic scale to measure PTT-A was derived and its psychometric quality was explored based on the first and an additional second study. The observed correlational pattern to basic personality traits supports the convergent validity of PTT-A. Moreover, discriminative predictive validity of PTT-A over PTT-H was supported by its higher relationships to technology-related outcomes. Additionally, incremental validity of PTT-A over basic personality traits was supported. Finally, the internal validity of the scale was replicated in an independent sample and re-test reliability was established. The findings support the added value of integrating PTT-A in research on the interaction with automated technology.},
  keywords = {dispositional trust,interpersonal trust,Propensity to trust,trust in AI,trust in artificial intelligence,trust in automation,trust in technology},
  annotation = {https://osf.io/bkw23/files/osfstorage\\
\\
https://osf.io/ntypg/\\
\\
https://osf.io/bkw23/files/osfstorage\\
\\
https://osf.io/wmukf/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/WHZ627G3/Scholz et al. - 2025 - Measuring the Propensity to Trust in Automated Technology Examining Similarities to Dispositional T.pdf}
}

@article{schustekHumanConfidenceJudgments2019,
  title = {Human Confidence Judgments Reflect Reliability-Based Hierarchical Integration of Contextual Information},
  author = {Schustek, Philipp and Hyafil, Alexandre and {Moreno-Bote}, Rub{\'e}n},
  year = {2019},
  month = nov,
  journal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {5430},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13472-z},
  urldate = {2022-04-15},
  abstract = {Our immediate observations must be supplemented with contextual information to resolve ambiguities. However, the context is often ambiguous too, and thus it should be inferred itself to guide behavior. Here, we introduce a novel hierarchical task (airplane task) in which participants should infer a higher-level, contextual variable to inform probabilistic inference about a hidden dependent variable at a lower level. By controlling the reliability of past sensory evidence through varying the sample size of the observations, we find that humans estimate the reliability of the context and combine it with current sensory uncertainty to inform their confidence reports. Behavior closely follows inference by probabilistic message passing between latent variables across hierarchical state representations. Commonly reported inferential fallacies, such as sample size insensitivity, are not present, and neither did participants appear to rely on simple heuristics. Our results reveal uncertainty-sensitive integration of information at different hierarchical levels and temporal scales.},
  copyright = {2019 The Author(s)},
  langid = {english},
  keywords = {Decision,Decision making,Human behaviour,matlab code},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Schustek et al_2019_Human confidence judgments reflect reliability-based hierarchical integration.pdf;/Users/thomasgorman/Zotero/storage/JH73VJLD/s41467-019-13472-z.html}
}

@article{schustekInstancebasedGeneralizationHuman2018,
  title = {Instance-Based Generalization for Human Judgments about Uncertainty},
  author = {Schustek, Philipp and {Moreno-Bote}, Rub{\'e}n},
  year = {2018},
  month = jun,
  journal = {PLOS Computational Biology},
  volume = {14},
  number = {6},
  pages = {e1006205},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006205},
  urldate = {2020-11-03},
  abstract = {While previous studies have shown that human behavior adjusts in response to uncertainty, it is still not well understood how uncertainty is estimated and represented. As probability distributions are high dimensional objects, only constrained families of distributions with a low number of parameters can be specified from finite data. However, it is unknown what the structural assumptions are that the brain uses to estimate them. We introduce a novel paradigm that requires human participants of either sex to explicitly estimate the dispersion of a distribution over future observations. Judgments are based on a very small sample from a centered, normally distributed random variable that was suggested by the framing of the task. This probability density estimation task could optimally be solved by inferring the dispersion parameter of a normal distribution. We find that although behavior closely tracks uncertainty on a trial-by-trial basis and resists an explanation with simple heuristics, it is hardly consistent with parametric inference of a normal distribution. Despite the transparency of the simple generating process, participants estimate a distribution biased towards the observed instances while still strongly generalizing beyond the sample. The inferred internal distributions can be well approximated by a nonparametric mixture of spatially extended basis distributions. Thus, our results suggest that fluctuations have an excessive effect on human uncertainty judgments because of representations that can adapt overly flexibly to the sample. This might be of greater utility in more general conditions in structurally uncertain environments.},
  langid = {english},
  keywords = {Behavior,Human learning,Kernel functions,Kernel methods,Normal distribution,Probability density,Probability distribution,Statistical dispersion},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/schustekInstancebasedGeneralizationHuman2018-zotero.md;/Users/thomasgorman/Zotero/storage/WGYFHXN7/Schustek and Moreno-Bote - 2018 - Instance-based generalization for human judgments .pdf;/Users/thomasgorman/Zotero/storage/ZY2YMABA/article.html}
}

@inproceedings{shangTrustingYourAI2025,
  title = {Trusting {{Your AI Agent Emotionally}} and {{Cognitively}}: {{Development}} and {{Validation}} of a {{Semantic Differential Scale}} for {{AI Trust}}},
  shorttitle = {Trusting {{Your AI Agent Emotionally}} and {{Cognitively}}},
  booktitle = {Proceedings of the 2024 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Shang, Ruoxi and Hsieh, Gary},
  year = {2025},
  month = feb,
  series = {{{AIES}} '24},
  pages = {1343--1356},
  publisher = {AAAI Press},
  address = {San Jose, California, USA},
  urldate = {2025-04-28},
  abstract = {Trust is not just a cognitive issue but also an emotional one, yet the research in human-AI interactions has primarily focused on the cognitive route of trust development. Recent work has highlighted the importance of studying affective trust towards AI, especially in the context of emerging human-like LLM-powered conversational agents. However, there is a lack of validated and generalizable measures for the two-dimensional construct of trust in AI agents. To address this gap, we developed and validated a set of 27-item semantic differential scales for affective and cognitive trust through a scenario-based survey study. We then further validated and applied the scale through an experiment study. Our empirical findings showed how the emotional and cognitive aspects of trust interact with each other and collectively shape a person's overall trust in AI agents. Our study methodology and findings also provide insights into the capability of the state-of-art LLMs to foster trust through different routes.},
  file = {/Users/thomasgorman/Zotero/storage/SSJ758G6/Shang et al. - 2025 - Trusting Your AI Agent Emotionally and Cognitively Development and Validation of a Semantic Differe.pdf}
}

@article{sindermannAssessingAttitudeArtificial2021,
  title = {Assessing the {{Attitude Towards Artificial Intelligence}}: {{Introduction}} of a {{Short Measure}} in {{German}}, {{Chinese}}, and {{English Language}}},
  shorttitle = {Assessing the {{Attitude Towards Artificial Intelligence}}},
  author = {Sindermann, Cornelia and Sha, Peng and Zhou, Min and Wernicke, Jennifer and Schmitt, Helena S. and Li, Mei and Sariyska, Rayna and Stavrou, Maria and Becker, Benjamin and Montag, Christian},
  year = {2021},
  month = mar,
  journal = {KI - K{\"u}nstliche Intelligenz},
  volume = {35},
  number = {1},
  pages = {109--118},
  publisher = {Springer Berlin Heidelberg},
  issn = {1610-1987},
  doi = {10.1007/s13218-020-00689-0},
  urldate = {2025-05-01},
  abstract = {In the context of (digital) human--machine interaction, people are increasingly dealing with artificial intelligence in everyday life. Through this, we observe humans who embrace technological advances with a positive attitude. Others, however, are particularly sceptical and claim to foresee substantial problems arising from such uses of technology. The aim of the present study was to introduce a short measure to assess the Attitude Towards Artificial Intelligence (ATAI scale) in the German, Chinese, and English languages. Participants from Germany (N\,=\,461; 345 females), China (N\,=\,413; 145 females), and the UK (N\,=\,84; 65 females) completed the ATAI scale, for which the factorial structure was tested and compared between the samples. Participants from Germany and China were additionally asked about their willingness to interact with/use self-driving cars, Siri, Alexa, the social robot Pepper, and the humanoid robot Erica, which are representatives of popular artificial intelligence products. The results showed that the five-item ATAI scale comprises two negatively associated factors assessing (1) acceptance and (2) fear of artificial intelligence. The factor structure was found to be similar across the German, Chinese, and UK samples. Additionally, the ATAI scale was validated, as the items on the willingness to use specific artificial intelligence products were positively associated with the ATAI Acceptance scale and negatively with the ATAI Fear scale, in both the German and Chinese samples. In conclusion we introduce a short, reliable, and valid measure on the attitude towards artificial intelligence in German, Chinese, and English language.},
  copyright = {2020 The Author(s)},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/JVC4MCCX/Sindermann et al. - 2021 - Assessing the Attitude Towards Artificial Intelligence Introduction of a Short Measure in German, C.pdf}
}

@article{smithNavigatingAIConvergence2025,
  title = {Navigating {{AI Convergence}} in {{Human}}--{{Artificial Intelligence Teams}}: {{A Signaling Theory Approach}}},
  shorttitle = {Navigating {{AI Convergence}} in {{Human}}--{{Artificial Intelligence Teams}}},
  author = {Smith, Andria and {van Wagoner}, Hunter Phoenix and Keplinger, Ksenia and Celebi, Can},
  year = {2025},
  journal = {Journal of Organizational Behavior},
  issn = {1099-1379},
  doi = {10.1002/job.2856},
  urldate = {2025-01-23},
  abstract = {Teams that combine human intelligence with artificial intelligence (AI) have become indispensable for solving complex tasks in various decision-making contexts in modern organizations. However, the factors that contribute to AI convergence, where human team members align their decisions with those of their AI counterparts, still remain unclear. This study integrates signaling theory with self-determination theory to investigate how specific signals---such as signal fit, optional AI advice, and signal set congruence---affect employees' AI convergence in human--AI teams. Based on four experimental studies conducted in facial recognition and hiring contexts with approximately 1100 participants, the findings highlight the significant positive impact of congruent signals from both human and AI team members on AI convergence. Moreover, providing an option for employees to solicit AI advice also enhances AI convergence; when AI signals are chosen by employees rather than forced upon them, participants are more likely to accept AI advice. This research advances knowledge on human--AI teaming by (1) expanding signaling theory into the human--AI team context; (2) developing a deeper understanding of AI convergence and its drivers in human--AI teams; (3) providing actionable insights for designing teams and tasks to optimize decision-making in high-stakes, uncertain environments; and (4) introducing facial recognition as an innovative context for human--AI teaming.},
  langid = {english},
  keywords = {AI convergence,artificial intelligence,human-AI teaming,optional advice,signaling theory},
  annotation = {https://osf.io/zjwta/?view\_only=74a15f71b21c4b0abb69f3e5de543537\\
\\
.},
  file = {/Users/thomasgorman/Zotero/storage/IME48J8B/Smith et al. - 2025 - Navigating AI Convergence in Human–Artificial Intelligence Teams A Signaling Theory Approach.pdf}
}

@article{smitsHowProbableProbably2005,
  title = {How Probable Is Probably? {{It}} Depends on Whom You're Talking About},
  shorttitle = {How Probable Is Probably?},
  author = {Smits, Tim and Hoorens, Vera},
  year = {2005},
  journal = {Journal of Behavioral Decision Making},
  volume = {18},
  number = {2},
  pages = {83--96},
  issn = {1099-0771},
  doi = {10.1002/bdm.485},
  urldate = {2025-05-12},
  abstract = {Two studies tested whether people interpreted verbal chance terms in a self-serving manner. Participants read statements describing the likelihood of events in their own future and in the future of a randomly chosen other. They interpreted the chance terms numerically. Chance terms were interpreted as denoting a higher probability when they were used to describe the likelihood of pleasant events in one's own future than when they were used to describe the likelihood of pleasant events in someone else's future (Study 1). Similarly, chance terms were interpreted as denoting a lower probability when they were used to describe the likelihood of unpleasant events in one's own future than when they were used to describe the likelihood of unpleasant events in someone else's future (Studies 1 and 2). These differences occurred primarily when the risk statements were threatening. Copyright {\copyright} 2005 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2005 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {comparative optimism,interpretation of verbal probabilities,risk communication,self-other differences,unrealistic optimism},
  file = {/Users/thomasgorman/Zotero/storage/6EWIB53S/Smits and Hoorens - 2005 - How probable is probably It depends on whom you're talking about.pdf;/Users/thomasgorman/Zotero/storage/EAYURESA/bdm.html}
}

@article{songMultiAgentsAreSocial2024,
  title = {Multi-{{Agents}} Are {{Social Groups}}: {{Investigating Social Influence}} of {{Multiple Agents}} in {{Human-Agent Interactions}}},
  shorttitle = {Multi-{{Agents}} Are {{Social Groups}}},
  author = {Song, Tianqi and Tan, Yugin and Zhu, Zicheng and Feng, Yibin and Lee, Yi-Chieh},
  year = {2024},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2411.04578},
  urldate = {2025-02-25},
  abstract = {Multi-agent systems - systems with multiple independent AI agents working together to achieve a common goal - are becoming increasingly prevalent in daily life. Drawing inspiration from the phenomenon of human group social influence, we investigate whether a group of AI agents can create social pressure on users to agree with them, potentially changing their stance on a topic. We conducted a study in which participants discussed social issues with either a single or multiple AI agents, and where the agents either agreed or disagreed with the user's stance on the topic. We found that conversing with multiple agents (holding conversation content constant) increased the social pressure felt by participants, and caused a greater shift in opinion towards the agents' stances on each topic. Our study shows the potential advantages of multi-agent systems over single-agent platforms in causing opinion change. We discuss design implications for possible multi-agent systems that promote social good, as well as the potential for malicious actors to use these systems to manipulate public opinion.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,Human-Computer Interaction (cs.HC)},
  file = {/Users/thomasgorman/Zotero/storage/B62UC52D/Song et al. - 2024 - Multi-Agents are Social Groups Investigating Social Influence of Multiple Agents in Human-Agent Int.pdf}
}

@misc{soto-sanfielScaleArtificialIntelligence2024,
  title = {The {{Scale}} of {{Artificial Intelligence Literacy}} for All ({{SAIL4ALL}}): {{A Tool}} for {{Assessing Knowledge}} on {{Artificial Intelligence}} in {{All Adult Populations}} and {{Settings}}},
  shorttitle = {The {{Scale}} of {{Artificial Intelligence Literacy}} for All ({{SAIL4ALL}})},
  author = {{Soto-Sanfiel}, Mar{\'i}a T. and {Angulo-Brunet}, Ariadna and Lutz, Christoph},
  year = {2024},
  month = apr,
  publisher = {OSF},
  doi = {10.31235/osf.io/bvyku},
  urldate = {2025-05-02},
  abstract = {This study provides evidence of the psychometric quality of a new artificial intelligence (AI) literacy scale for comprehensive assessment of the concept across adult populations, regardless of the setting in which it is applied: the SAIL4ALL. It contains 56 items distributed across four themes [(1) What is AI? (a: Recognizing AI, Understanding Intelligence and Interdisciplinarity; b: General vs. Narrow); (2) What can AI do?; (3) How does AI work?; and (4) How should AI be used?], which can be used in combination or independently. Moreover, the scale is validated in two different response formats (true/false and 5-point Likert scale), each of which is applied depending on the context. The version with a true/false format is ideal for when AI literacy levels need to be assessed quickly because of its simplicity and ease of interpretation. Conversely, the 5-point Likert scale yields more nuanced responses based on the degree of confidence, providing richer insights into the respondents' perceptions of their AI literacy. SAIL4ALL has demonstrated positive evidence of psychometric quality, and serves as a valuable tool for determining both actual and perceived knowledge of AI, thus guiding educational, organizational, and institutional AI literacy initiatives.},
  archiveprefix = {OSF},
  langid = {american},
  keywords = {AI ethics,AI literacy,artificial intelligence,education,literacy,machine learning,scale development},
  annotation = {https://osf.io/preprints/socarxiv/bvyku\_v1},
  file = {/Users/thomasgorman/Zotero/storage/Q5CN35PA/Soto-Sanfiel et al. - 2024 - The Scale of Artificial Intelligence Literacy for all (SAIL4ALL) A Tool for Assessing Knowledge on.pdf}
}

@incollection{starkePsychologicallyInformedDesign2024,
  title = {Psychologically {{Informed Design}} of {{Energy Recommender Systems}}: {{Are Nudges Still Effective}} in {{Tailored Choice Environments}}?},
  shorttitle = {Psychologically {{Informed Design}} of {{Energy Recommender Systems}}},
  booktitle = {A {{Human-Centered Perspective}} of {{Intelligent Personalized Environments}} and {{Systems}}},
  author = {Starke, Alain D. and Willemsen, Martijn C.},
  editor = {Ferwerda, Bruce and Graus, Mark and Germanakos, Panagiotis and Tkal{\v c}i{\v c}, Marko},
  year = {2024},
  pages = {221--259},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-55109-3_9},
  urldate = {2025-05-05},
  isbn = {978-3-031-55108-6 978-3-031-55109-3},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/RN6774H7/Starke and Willemsen - 2024 - Psychologically Informed Design of Energy Recommender Systems Are Nudges Still Effective in Tailore.pdf}
}

@phdthesis{starkeSupportingEnergyefficientChoices2019,
  type = {Phd {{Thesis}} 1 ({{Research TU}}/e / {{Graduation TU}}/e)},
  title = {Supporting Energy-Efficient Choices Using {{Rasch-based}} Recommender Interfaces},
  author = {Starke, A.D.},
  year = {2019},
  month = feb,
  address = {Eindhoven},
  abstract = {`Netflix for energy': tailored home energy-saving recommendations},
  school = {Technische Universiteit Eindhoven},
  annotation = {https://osf.io/jz4wt/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/X7DUT52K/Starke - 2019 - Supporting energy-efficient choices using Rasch-based recommender interfaces.pdf}
}

@inproceedings{starkeUsingExplanationsEnergySaving2021,
  title = {Using {{Explanations}} as {{Energy-Saving Frames}}: {{A User-Centric Recommender Study}}},
  shorttitle = {Using {{Explanations}} as {{Energy-Saving Frames}}},
  booktitle = {Adjunct {{Proceedings}} of the 29th {{ACM Conference}} on {{User Modeling}}, {{Adaptation}} and {{Personalization}}},
  author = {Starke, Alain D. and Willemsen, Martijn C. and Snijders, Chris},
  year = {2021},
  month = jun,
  pages = {229--237},
  publisher = {ACM},
  address = {Utrecht Netherlands},
  doi = {10.1145/3450614.3464477},
  urldate = {2025-05-05},
  isbn = {978-1-4503-8367-7},
  langid = {english},
  annotation = {https://osf.io/jz4wt/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/SLP752W7/Starke et al. - 2021 - Using Explanations as Energy-Saving Frames A User-Centric Recommender Study.pdf}
}

@article{stavrovaExpressionsUncertaintyOnline2024,
  title = {Expressions of Uncertainty in Online Science Communication Hinder Information Diffusion},
  author = {Stavrova, Olga and Kleinberg, Bennett and Evans, Anthony M and Ivanovi{\'c}, Milena},
  editor = {Contractor, Noshir},
  year = {2024},
  month = oct,
  journal = {PNAS Nexus},
  volume = {3},
  number = {10},
  pages = {pgae439},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgae439},
  urldate = {2025-05-13},
  abstract = {Despite the importance of transparent communication of uncertainty surrounding scientific findings, there are concerns that communicating uncertainty might damage the public perception and dissemination of science. Yet, a lack of empirical research on the potential impact of uncertainty communication on the diffusion of scientific findings poses challenges in assessing such claims. We studied the effect of uncertainty in a field study and a controlled experiment. In Study 1, a natural language processing analysis of over 2 million social media (Twitter/X) messages about scientific findings revealed that more uncertain messages were shared less often. Study 2 replicated this pattern using an experimental design where participants were presented with large-language-model (LLM)-generated high- and low-uncertainty messages. These results underscore the role of uncertainty in the dissemination of scientific findings and inform the ongoing debates regarding the benefits and the risks of uncertainty in science communication.},
  copyright = {https://creativecommons.org/licenses/by-nc/4.0/},
  langid = {english},
  annotation = {https://osf.io/ye3b4/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/ZH8BDPX7/Stavrova et al. - 2024 - Expressions of uncertainty in online science communication hinder information diffusion.pdf}
}

@article{steinAttitudesAIMeasurement2024,
  title = {Attitudes towards {{AI}}: Measurement and Associations with Personality},
  shorttitle = {Attitudes towards {{AI}}},
  author = {Stein, Jan-Philipp and Messingschlager, Tanja and Gnambs, Timo and Hutmacher, Fabian and Appel, Markus},
  year = {2024},
  month = feb,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {2909},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-53335-2},
  urldate = {2024-11-15},
  abstract = {Artificial intelligence (AI) has become an integral part of many contemporary technologies, such as social media platforms, smart devices, and global logistics systems. At the same time, research on the public acceptance of AI shows that many people feel quite apprehensive about the potential of such technologies---an observation that has been connected to both demographic and sociocultural user variables (e.g., age, previous media exposure). Yet, due to divergent and often ad-hoc measurements of AI-related attitudes, the current body of evidence remains inconclusive. Likewise, it is still unclear if attitudes towards AI are also affected by users' personality traits. In response to these research gaps, we offer a two-fold contribution. First, we present a novel, psychologically informed questionnaire (ATTARI-12) that captures attitudes towards AI as a single construct, independent of specific contexts or applications. Having observed good reliability and validity for our new measure across two studies (N1\,=\,490; N2\,=\,150), we examine several personality traits---the Big Five, the Dark Triad, and conspiracy mentality---as potential predictors of AI-related attitudes in a third study (N3\,=\,298). We find that agreeableness and younger age predict a more positive view towards artificially intelligent technology, whereas the susceptibility to conspiracy beliefs connects to a more negative attitude. Our findings are discussed considering potential limitations and future directions for research and practice.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Information technology,Psychology},
  annotation = {https://osf.io/3j67a/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/KJGQ95YM/Stein et al. - 2024 - Attitudes towards AI measurement and associations with personality.pdf}
}

@misc{steyversMetacognitionUncertaintyCommunication2025,
  title = {Metacognition and {{Uncertainty Communication}} in {{Humans}} and {{Large Language Models}}},
  author = {Steyvers, Mark and Peters, Megan A. K.},
  year = {2025},
  month = apr,
  number = {arXiv:2504.14045},
  eprint = {2504.14045},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.14045},
  urldate = {2025-04-26},
  abstract = {Metacognition, the capacity to monitor and evaluate one's own knowledge and performance, is foundational to human decision-making, learning, and communication. As large language models (LLMs) become increasingly embedded in high-stakes decision contexts, it is critical to assess whether, how, and to what extent they exhibit metacognitive abilities. Here, we provide an overview of current knowledge of LLMs' metacognitive capacities, how they might be studied, and how they relate to our knowledge of metacognition in humans. We show that while humans and LLMs can sometimes appear quite aligned in their metacognitive capacities and behaviors, it is clear many differences remain. Attending to these differences is crucial not only for enhancing human-AI collaboration, but also for promoting the development of more capable and trustworthy artificial systems. Finally, we discuss how endowing future LLMs with more sensitive and more calibrated metacognition may also help them develop new capacities such as more efficient learning, self-direction, and curiosity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Zotero/storage/RX4FJCEK/Steyvers and Peters - 2025 - Metacognition and Uncertainty Communication in Humans and Large Language Models.pdf;/Users/thomasgorman/Zotero/storage/K6N4M2HW/2504.html}
}

@article{steyversWhatLargeLanguage2025,
  title = {What Large Language Models Know and What People Think They Know},
  author = {Steyvers, Mark and Tejeda, Heliodoro and Kumar, Aakriti and Belem, Catarina and Karny, Sheer and Hu, Xinyue and Mayer, Lukas and Smyth, Padhraic},
  year = {2025},
  journal = {Nature Machine Intelligence},
  pages = {1--11},
  doi = {10.1038/s42256-024-00976-7},
  abstract = {As AI systems, particularly large language models (LLMs), become increasingly integrated into decision-making processes, the ability to trust their outputs is crucial. To earn human trust, LLMs must be well-calibrated such that they can accurately assess and communicate the likelihood of their predictions being correct. While recent work has focused on LLMs' internal confidence, less is understood about how effectively they convey uncertainty to users. This study explores the ``calibration gap'' which refers to the difference between human confidence in LLM-generated answers and the models' actual confidence, and the ``discrimination gap'' which reflects how well humans and models can distinguish between correct and incorrect answers. Our experiments with multiple-choice and short-answer questions reveal that users tend to overestimate the accuracy of LLM responses when provided with default explanations. Additionally, longer explanations increased user confidence, even when the extra length did not improve answer accuracy. By adjusting LLM explanations to better reflect the models' internal confidence, both the calibration and discrimination gaps narrowed, significantly improving user perception of LLM accuracy. These findings underscore the importance of accurate uncertainty communication and highlight the effect of explanation length in influencing user trust in AI-assisted decision-making environments.},
  langid = {english},
  keywords = {matlab code},
  annotation = {https://osf.io/y7pr6/\\
\\
https://madlabatuci.github.io/working-with-chatGPT/\\
\\
https://osf.io/y7pr6/files/osfstorage\\
\\
https://static-content.springer.com/esm/art\%3A10.1038\%2Fs42256-024-00976-7/MediaObjects/42256\_2024\_976\_MOESM1\_ESM.pdf},
  file = {/Users/thomasgorman/Zotero/storage/DQSYRXZW/Steyvers et al. - 2025 - What large language models know and what people think they know.pdf}
}

@article{struederCombiningForecastsAdvisors2024,
  title = {Combining Forecasts from Advisors: {{The}} Impact of Advice Independence and Verbal versus Numeric Format},
  shorttitle = {Combining Forecasts from Advisors},
  author = {Strueder, Jeremy D. and Windschitl, Paul D.},
  year = {2024},
  journal = {Journal of Experimental Psychology: General},
  volume = {153},
  number = {8},
  pages = {2088--2099},
  issn = {1939-2222},
  doi = {10.1037/xge0001611},
  abstract = {Past research on advice-taking has suggested that people are often insensitive to the level of advice independence when combining forecasts from advisors. However, this has primarily been tested for cases in which people receive numeric forecasts. Recent work by Mislavsky and Gaertig (2022) shows that people sometimes employ different strategies when combining verbal versus numeric forecasts about the likelihood of future events. Specifically, likelihood judgments based on two verbal forecasts (e.g., ``rather likely'') are more often extreme (relative to the forecasts) than are likelihood judgments based on two numeric forecasts (e.g., ``70\% probability''). The goal of the present research was to investigate whether advice-takers' use of combination strategies can be sensitive to advice independence when differences in independence are highly salient and whether sensitivity to advice independence depends on the format in which advice is given. In two studies, we found that advice-takers became more extreme with their own likelihood estimate when combining forecasts from advisors who use separate evidence, as opposed to the same evidence. We also found that two verbal forecasts generally resulted in more extreme combined likelihood estimates than two numeric forecasts. However, the results did not suggest that sensitivity to advice independence depends on the format of advice. (PsycInfo Database Record (c) 2024 APA, all rights reserved)},
  keywords = {Judgment,Numbers (Numerals),Prediction,Probability},
  annotation = {https://researchbox.org/1748\&PEER\_REVIEW\_passcode=BGZDAK},
  file = {/Users/thomasgorman/Zotero/storage/QXR4K2S5/2025-10514-004.html}
}

@article{subramanianCombiningUncertaintyInformation2023,
  title = {Combining Uncertainty Information with {{AI}} Recommendations Supports Calibration with Domain Knowledge},
  author = {Subramanian, Harishankar Vasudevanallur and Canfield, Casey and Shank, Daniel B. and Kinnison, Matthew},
  year = {2023},
  month = oct,
  journal = {Journal of Risk Research},
  issn = {1366-9877},
  urldate = {2025-05-13},
  abstract = {Increasingly, artificial intelligence (AI) recommender systems are being integrated into high-risk contexts ranging from healthcare to the military to the finance system. However, numerous high-pro...},
  copyright = {{\copyright} 2023 Informa UK Limited, trading as Taylor \& Francis Group},
  langid = {english},
  annotation = {https://osf.io/bjgu9/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/UJTI6AA7/Subramanian et al. - 2023 - Combining uncertainty information with AI recommendations supports calibration with domain knowledge.pdf;/Users/thomasgorman/Zotero/storage/3NGJLBJW/13669877.2023.html}
}

@phdthesis{sunDelegationVirtualAgents2023,
  title = {Delegation to {{Virtual Agents}} in {{Critical Scenarios}}: {{Influencing Factors}} and {{Immersive Settings}}},
  shorttitle = {Delegation to {{Virtual Agents}} in {{Critical Scenarios}}},
  author = {Sun, Ningyuan},
  year = {2023},
  month = nov,
  urldate = {2025-05-03},
  abstract = {Favored by the rapid advance of technologies such as artificial intelligence and computer graphics, virtual agents have been increasingly accessible, capable, and autonomous over the past decades. As a result of their growing technological prowess, interaction with virtual agents has been gradually evolving from a traditional user-tool relationship to one resembling interpersonal delegation, where users empower virtual agents to autonomously carry out specific tasks on their behalf. Forming a delegatory relationship with virtual agents can facilitate the user-agent interaction in numerous aspects, particularly regarding convenience and efficiency. Yet, it also comes with problems and challenges that may harm users drastically in critical scenarios and thus deserves extensive research. This thesis presents a thorough discussion of delegation to virtual agents based on a series of studies my colleagues and I conducted over the past four years. Several factors --including agent representation, theory of mind, rapport, and technological immersion-- are examined individually via empirical approaches to reveal their impacts on delegation to virtual agents. A conceptual model featuring three interrelated dimensions is proposed, constituting a theoretical framework to integrate the empirical findings. An overall evaluation of these works indicates that users' decisions on delegating critical tasks to virtual agents are mainly based on rational thinking. Performance-related factors have a significant impact on delegation, whereas affective cues --such as rapport, agent representation, and theory of mind-- are influential only to a limited extent. Furthermore, the usage of immersive media devices (e.g., head-mounted displays) has a marginal effect on users' delegatory decisions. Thus, it is advisable for developers to focus on performance-related aspects when designing virtual agents for critical tasks.},
  langid = {english},
  school = {University of Luxembourg},
  file = {/Users/thomasgorman/Zotero/storage/VU2W5R3X/Sun - 2023 - Delegation to Virtual Agents in Critical Scenarios Influencing Factors and Immersive Settings.pdf}
}

@misc{taubenfeldConfidenceImprovesSelfConsistency2025,
  title = {Confidence {{Improves Self-Consistency}} in {{LLMs}}},
  author = {Taubenfeld, Amir and Sheffer, Tom and Ofek, Eran and Feder, Amir and Goldstein, Ariel and Gekhman, Zorik and Yona, Gal},
  year = {2025},
  month = feb,
  number = {arXiv:2502.06233},
  eprint = {2502.06233},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.06233},
  urldate = {2025-05-14},
  abstract = {Self-consistency decoding enhances LLMs' performance on reasoning tasks by sampling diverse reasoning paths and selecting the most frequent answer. However, it is computationally expensive, as sampling many of these (lengthy) paths is required to increase the chances that the correct answer emerges as the most frequent one. To address this, we introduce Confidence-Informed Self-Consistency (CISC). CISC performs a weighted majority vote based on confidence scores obtained directly from the model. By prioritizing high-confidence paths, it can identify the correct answer with a significantly smaller sample size. When tested on nine models and four datasets, CISC outperforms self-consistency in nearly all configurations, reducing the required number of reasoning paths by over 40\% on average. In addition, we introduce the notion of within-question confidence evaluation, after showing that standard evaluation methods are poor predictors of success in distinguishing correct and incorrect answers to the same question. In fact, the most calibrated confidence method proved to be the least effective for CISC. Lastly, beyond these practical implications, our results and analyses show that LLMs can effectively judge the correctness of their own outputs, contributing to the ongoing debate on this topic.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Zotero/storage/ZFFUXJK5/Taubenfeld et al. - 2025 - Confidence Improves Self-Consistency in LLMs.pdf;/Users/thomasgorman/Zotero/storage/TWAYCARQ/2502.html}
}

@article{teigenDimensionsUncertaintyCommunication2023,
  title = {Dimensions of Uncertainty Communication: {{What}} Is Conveyed by Verbal Terms and Numeric Ranges},
  shorttitle = {Dimensions of Uncertainty Communication},
  author = {Teigen, Karl Halvor},
  year = {2023},
  month = nov,
  journal = {Current Psychology},
  volume = {42},
  number = {33},
  pages = {29122--29137},
  issn = {1046-1310, 1936-4733},
  doi = {10.1007/s12144-022-03985-0},
  urldate = {2025-05-12},
  abstract = {The paper reviews two strands of research on communication of uncertainty that usually have been investigated separately: (1) Probabilities attached to specific outcomes, and (2) Range judgments. Probabilities are sometimes expressed by verbal phrases (``rain is likely'') and at other times in a numeric format (``70\% chance of rain''), whereas range judgments describe the potential amounts expected (``1--4 mm of rain''). Examination of previous research shows that both descriptions convey, in addition to the strength of expectations, pragmatic information about the communicative situation. For instance, so-called verbal probability expressions (VPE), as likely, unlikely, a chance, or not certain give some, albeit vague, probabilistic information, but carry in addition an implicit message about the sources of uncertainty, the outcome's valence and severity, along with information about the speakers' attitudes and their communicative intentions. VPEs are directional by drawing attention either to an outcome's occurrence (``it is possible'') or to its non-occurrence (``it is doubtful''). In this sense they may be more informative than numbers. Uncertainties about outcomes in a distribution (continuous quantities) are alternatively expressed as interval estimates. The width of such intervals can function as a cue to credibility and expertise. Incomplete, one-sided intervals, where only one boundary is stated, imply directionality. ``More than 100 people'' suggests a crowd, while ``less than 200'' implies a shortfall. As with VPEs, directionally positive intervals are more frequent, and perhaps more neutral than negative ones. To convey expectancies and uncertainty in a balanced way, communicators may have to alternate between complementary frames.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/YRD9LHX7/Teigen - 2023 - Dimensions of uncertainty communication What is conveyed by verbal terms and numeric ranges.pdf}
}

@article{tomsettRapidTrustCalibration2020,
  title = {Rapid {{Trust Calibration}} through {{Interpretable}} and {{Uncertainty-Aware AI}}},
  author = {Tomsett, Richard and Preece, Alun and Braines, Dave and Cerutti, Federico and Chakraborty, Supriyo and Srivastava, Mani and Pearson, Gavin and Kaplan, Lance},
  year = {2020},
  month = jul,
  journal = {Patterns},
  volume = {1},
  number = {4},
  pages = {100049},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2020.100049},
  urldate = {2025-05-08},
  abstract = {Artificial intelligence (AI) systems hold great promise as decision-support tools, but we must be able to identify and understand their inevitable mistakes if they are to fulfill this potential. This is particularly true in domains where the decisions are high-stakes, such as law, medicine, and the military. In this Perspective, we describe the particular challenges for AI decision support posed in military coalition operations. These include having to deal with limited, low-quality data, which inevitably compromises AI performance. We suggest that these problems can be mitigated by taking steps that allow rapid trust calibration so that decision makers understand the AI system's limitations and likely failures and can calibrate their trust in its outputs appropriately. We propose that AI services can achieve this by being both interpretable and uncertainty-aware. Creating such AI systems poses various technical and human factors challenges. We review these challenges and recommend directions for future research., This article is about artificial intelligence (AI) used to inform high-stakes decisions, such as those arising in legal, healthcare, or military contexts. Users must have an understanding of the capabilities and limitations of an AI system when making high-stakes decisions. Usually this requires the user to interact with the system and learn over time how it behaves in different circumstances. We propose that long-term interaction would not be necessary for an AI system with the properties of interpretability and uncertainty awareness. Interpretability makes clear what the system ``knows'' while uncertainty awareness reveals what the system does not ``know.'' This allows the user to rapidly calibrate their trust in the system's outputs, spotting flaws in its reasoning or seeing when it is unsure. We illustrate these concepts in the context of a military coalition operation, where decision makers may be using AI systems with which they are unfamiliar and which are operating in rapidly changing environments. We review current research in these areas, considering both technical and human factors challenges, and propose a framework for future work based on Lasswell's communication model., We introduce the concept of rapid trust calibration for AI decision support, and propose how this can be achieved by building AI systems that are both interpretable and uncertainty-aware. We provide a literature review of these research areas and describe a military scenario illustrating the relevant concepts. We propose a framework inspired by Lasswell's communication model to structure future work in this area.},
  pmcid = {PMC7660448},
  pmid = {33205113}
}

@article{tullyLowerArtificialIntelligence2025,
  title = {Lower {{Artificial Intelligence Literacy Predicts Greater AI Receptivity}}},
  author = {Tully, Stephanie and Longoni, Chiara and Appel, Gil},
  year = {2025},
  journal = {Journal of Marketing},
  doi = {10.1177/00222429251314491},
  urldate = {2025-04-30},
  abstract = {As artificial intelligence (AI) transforms society, understanding factors that influence AI receptivity is increasingly important. The current research investigates which types of consumers have greater AI receptivity. Contrary to expectations revealed in four surveys, cross country data and six additional studies find that people with lower AI literacy are typically more receptive to AI. This lower literacy-greater receptivity link is not explained by differences in perceptions of AI's capability, ethicality, or feared impact on humanity. Instead, this link occurs because people with lower AI literacy are more likely to perceive AI as magical and experience feelings of awe in the face of AI's execution of tasks that seem to require uniquely human attributes. In line with this theorizing, the lower literacy-higher receptivity link is mediated by perceptions of AI as magical and is moderated among tasks not assumed to require distinctly human attributes. These findings suggest that companies may benefit from shifting their marketing efforts and product development towards consumers with lower AI literacy. Additionally, efforts to demystify AI may inadvertently reduce its appeal, indicating that maintaining an aura of magic around AI could be beneficial for adoption.},
  langid = {american},
  keywords = {AI adoption,AI knowledge,algorithms,artificial intelligence,human-computer interaction,literacy,technology},
  file = {/Users/thomasgorman/Zotero/storage/7XEX6F8K/Tully et al. - 2023 - Lower Artificial Intelligence Literacy Predicts Greater AI Receptivity.pdf}
}

@article{vandenbroekHeuristicsEnergyJudgement2019,
  title = {Heuristics in Energy Judgement Tasks},
  author = {Van Den Broek, Karlijn L. and Walker, Ian},
  year = {2019},
  month = apr,
  journal = {Journal of Environmental Psychology},
  volume = {62},
  pages = {95--104},
  issn = {02724944},
  doi = {10.1016/j.jenvp.2019.02.008},
  urldate = {2024-12-11},
  abstract = {To save energy effectively, householders need to be aware of the energy consumption in their homes, in particular the energy use of their household appliances. People's perception of the energy use of their appliances has been found to be influenced by the use of heuristics (simple rules for making quick decisions), yet these heuristics have received little research attention. Three studies investigated the use of these energy judgement heuristics using mixed methods. Findings show that 1) participants used as many as twenty-four different heuristics in an energy judgement task -- an order of magnitude more than identified in existing literature; 2) participants are aware they use the heuristics, but awareness varies per heuristic; 3) the use of these heuristics can be changed and this in turn can improve energy literacy. These studies demonstrate for the first time that the energy judgement process is much more complex than previously thought and provides a promising starting point for future research to uncover opportunities to improve energy literacy.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/UFLRKFVD/Van Den Broek and Walker - 2019 - Heuristics in energy judgement tasks.pdf}
}

@article{vanderblesCommunicatingUncertaintyFacts2019,
  title = {Communicating Uncertainty about Facts, Numbers and Science},
  author = {Van Der Bles, Anne Marthe and Van Der Linden, Sander and Freeman, Alexandra L. J. and Mitchell, James and Galvao, Ana B. and Zaval, Lisa and Spiegelhalter, David J.},
  year = {2019},
  month = may,
  journal = {Royal Society Open Science},
  volume = {6},
  number = {5},
  pages = {181870},
  issn = {2054-5703},
  doi = {10.1098/rsos.181870},
  urldate = {2025-05-12},
  abstract = {Uncertainty is an inherent part of knowledge, and yet in an era of contested expertise, many shy away from openly communicating their uncertainty about what they know, fearful of their audience's reaction. But what effect does communication of such epistemic uncertainty have? Empirical research is widely scattered across many disciplines. This interdisciplinary review structures and summarizes current practice and research across domains, combining a statistical and psychological perspective. This informs a framework for uncertainty communication in which we identify three objects of uncertainty---facts, numbers and science---and two levels of uncertainty: direct and indirect. An examination of current practices provides a scale of nine expressions of direct uncertainty. We discuss attempts to codify indirect uncertainty in terms of quality of the underlying evidence. We review the limited literature about the effects of communicating epistemic uncertainty on cognition, affect, trust and decision-making. While there is some evidence that communicating epistemic uncertainty does not necessarily affect audiences negatively, impact can vary between individuals and communication formats. Case studies in economic statistics and climate change illustrate our framework in action. We conclude with advice to guide both communicators and future researchers in this important but so far rather neglected field.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/I6A8YRVP/Van Der Bles et al. - 2019 - Communicating uncertainty about facts, numbers and science.pdf}
}

@incollection{vargheseEnergyLiteracyScale2023,
  title = {Energy {{Literacy Scale}} ({{ELS}}): {{Validated Survey Instrument}} to {{Measure Energy Knowledge}}, {{Attitude}}, and {{Behaviour}}},
  shorttitle = {Energy {{Literacy Scale}} ({{ELS}})},
  booktitle = {The 9th {{International Conference}} on {{Energy}} and {{Environment Research}}},
  author = {Varghese, Annie Feba and Chandrasenan, Divya},
  year = {2023},
  pages = {793--800},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-43559-1_75},
  urldate = {2025-05-02},
  abstract = {The Energy Literacy Scale (ELS) was created to assess students' energy-related knowledge and awareness of the implications of energy production and consumption, everyday energy use, and the adoption of energy-saving behaviors. The energy literacy scale was drafted and pilot tested among elementary school students across Kerala, India. Initial exploration of the measure yielded promising results: Cronbach's reliability coefficients for cognitive, emotional, and behavioral subscales varied from 0.68 to 0.78, while average discrimination indices ranged from 0.28 to 0.43. Factor Analysis was used to select appropriate questions for the Energy Literacy Scale with due importance being given to each of the Knowledge, Attitudinal and Behavioral domains. The field-tested ELS includes three knowledge factors namely (1) Energy sources, Efficiency and Conservation, (2) Energy Use and Implications and (3) Basic Energy Concepts. Three behaviour and two attitude dimension sub-scales are included in the accepted instrument. The ELS is particularly useful for determining the baseline energy literacy skills of potential responders and evaluating the broader effects of educational initiatives.},
  isbn = {978-3-031-43558-4 978-3-031-43559-1},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/V2ZGESI4/Varghese and Chandrasenan - 2023 - Energy Literacy Scale (ELS) Validated Survey Instrument to Measure Energy Knowledge, Attitude, and.pdf}
}

@inproceedings{vodrahalliUncalibratedModelsCan2022,
  title = {Uncalibrated {{Models Can Improve Human-AI Collaboration}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vodrahalli, Kailas and Gerstenberg, Tobias and Zou, James},
  year = {2022},
  volume = {35},
  pages = {4004--4016},
  abstract = {In many practical applications of AI, an AI model is used as a decision aid for human users. The AI provides advice that a human (sometimes) incorporates into their decision-making process. The AI advice is often presented with some measure of ``confidence'' that the human can use to calibrate how much they depend on or trust the advice. In this paper, we present an initial exploration that suggests showing AI models as more confident than they actually are, even when the original AI is well-calibrated, can improve human-AI performance (measured as the accuracy and confidence of the human's final prediction after seeing the AI advice). We first train a model to predict human incorporation of AI advice using data from thousands of human-AI interactions. This enables us to explicitly estimate how to transform the AI's prediction confidence, making the AI uncalibrated, in order to improve the final human prediction. We empirically validate our results across four different tasks---dealing with images, text and tabular data---involving hundreds of human participants. We further support our findings with simulation analysis. Our findings suggest the importance of jointly optimizing the human-AI system as opposed to the standard paradigm of optimizing the AI model alone.},
  langid = {english},
  annotation = {https://github.com/kailas-v/human-ai-interactions},
  file = {/Users/thomasgorman/Zotero/storage/YHYM38ET/Vodrahalli et al. - Uncalibrated Models Can Improve Human-AI Collaboration.pdf}
}

@incollection{vogelInterpretationVerbalProbabilities2022,
  title = {The {{Interpretation}} of {{Verbal Probabilities}}: {{A Systematic Literature Review}} and {{Meta-Analysis}}},
  shorttitle = {The {{Interpretation}} of {{Verbal Probabilities}}},
  booktitle = {Studies in {{Health Technology}} and {{Informatics}}},
  author = {Vogel, Hannah and Appelbaum, Sebastian and Haller, Heidemarie and Ostermann, Thomas},
  editor = {R{\"o}hrig, Rainer and Grabe, Niels and Hoffmann, Verena S. and H{\"u}bner, Ursula and K{\"o}nig, Jochem and Sax, Ulrich and Schreiweis, Bj{\"o}rn and Sedlmayr, Martin},
  year = {2022},
  month = aug,
  publisher = {IOS Press},
  doi = {10.3233/SHTI220798},
  urldate = {2025-05-13},
  abstract = {Introduction: Verbal probabilities such as ``likely'' or ``probable'' are commonly used to describe situations of uncertainty or risk and are easy and natural to most people. Numerous studies are devoted to the translation of verbal probability expressions to numerical probabilities. Methods: The present work aims to summarize existing research on the numerical interpretation of verbal probabilities. This was accomplished by means of a systematic literature review and meta-analysis conducted alongside the MOOSE-guidelines for meta-analysis of observational studies in epidemiology. Studies were included, if they provided empirical assignments of verbal probabilities to numerical values. Results: The literature search identified 181 publications and finally led to 21 included articles and the procession of 35 verbal probability expressions. Sample size of the studies ranged from 11 to 683 participants and covered a period of half a century from 1967 to 2018. In half of the studies, verbal probabilities were delivered in a neutral context followed by a medical context. Mean values of the verbal probabilities range from 7.24\% for the term ``impossible'' up to 94.79\% for the term ``definite''. Discussion: According to the results, there is a common `across-study' consensus of 35 probability expressions for describing different degrees of probability, whose numerical interpretation follows a linear course. However, heterogeneity of studies was considerably high and should be considered as a limiting factor.},
  copyright = {https://creativecommons.org/licenses/by-nc/4.0/},
  isbn = {978-1-64368-302-7 978-1-64368-303-4},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/5KIPFP8P/Vogel et al. - 2022 - The Interpretation of Verbal Probabilities A Systematic Literature Review and Meta-Analysis.pdf}
}

@inproceedings{wangDecouplingMetacognitionCognition2025,
  title = {Decoupling {{Metacognition}} from {{Cognition}}: {{A Framework}} for {{Quantifying Metacognitive Ability}} in {{LLMs}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Wang, Guoqing and Wu, Wen and Ye, Guangze and Cheng, Zhenxiao and Chen, Xi and Zheng, Hong},
  year = {2025},
  abstract = {Large Language Models (LLMs) are known to hallucinate facts and make non-factual statements which can undermine trust in their output. The essence of hallucination lies in the absence of metacognition in LLMs, namely the understanding of their own cognitive processes. However, there has been limited research on quantitatively measuring metacognition within LLMs. Drawing inspiration from cognitive psychology theories, we first quantify the metacognitive ability of LLMs as their ability to evaluate the correctness of responses through confidence. Subsequently, we introduce a general framework called DMC designed to decouple metacognitive ability and cognitive ability. This framework tackles the challenge of noisy quantification caused by the coupling of metacognition and cognition in current research, such as calibration-based metrics. Specifically, the DMC framework comprises two key steps. Initially, the framework tasks the LLM with failure prediction, aiming to evaluate the model's performance in predicting failures, a performance jointly determined by both cognitive and metacognitive abilities of the LLM. Following this, the framework disentangles metacognitive ability and cognitive ability based on the failure prediction performance, providing a quantification of the LLM's metacognitive ability independent of cognitive influences. Experiments conducted on eight datasets across five domains reveal that (1) Our proposed DMC framework effectively separates the metacognition and cognition of LLMs; (2) Various confidence elicitation methods impact the quantification of metacognitve ability differently; (3) Stronger metacognitive ability are exhibited by LLMs with better overall performance; (4) Enhancing metacognition holds promise for alleviating hallucination issues.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/LYPGXZZT/Wang et al. - Decoupling Metacognition from Cognition A Framework for Quantifying Metacognitive Ability in LLMs.pdf}
}

@article{wangEffectChatGPTStudents2025,
  title = {The Effect of {{ChatGPT}} on Students' Learning Performance, Learning Perception, and Higher-Order Thinking: Insights from a Meta-Analysis},
  shorttitle = {The Effect of {{ChatGPT}} on Students' Learning Performance, Learning Perception, and Higher-Order Thinking},
  author = {Wang, Jin and Fan, Wenxiang},
  year = {2025},
  month = may,
  journal = {Humanities and Social Sciences Communications},
  volume = {12},
  number = {1},
  pages = {1--21},
  publisher = {Palgrave},
  issn = {2662-9992},
  doi = {10.1057/s41599-025-04787-y},
  urldate = {2025-05-10},
  abstract = {As a new type of artificial intelligence, ChatGPT is becoming widely used in learning. However, academic consensus regarding its efficacy remains elusive. This study aimed to assess the effectiveness of ChatGPT in improving students' learning performance, learning perception, and higher-order thinking through a meta-analysis of 51 research studies published between November 2022 and February 2025. The results indicate that ChatGPT has a large positive impact on improving learning performance (g\,=\,0.867) and a moderately positive impact on enhancing learning perception (g\,=\,0.456) and fostering higher-order thinking (g\,=\,0.457). The impact of ChatGPT on learning performance was moderated by type of course (QB\,=\,64.249, P\,{$<$}\,0.001), learning model (QB\,=\,76.220, P\,{$<$}\,0.001), and duration (QB\,=\,55.998, P\,{$<$}\,0.001); its effect on learning perception was moderated by duration (QB\,=\,19.839, P\,{$<$}\,0.001); and its influence on the development of higher-order thinking was moderated by type of course (QB\,=\,7.811, P\,{$<$}\,0.05) and the role played by ChatGPT (QB\,=\,4.872, P\,{$<$}\,0.05). This study suggests that: (1) appropriate learning scaffolds or educational frameworks (e.g., Bloom's taxonomy) should be provided when using ChatGPT to develop students' higher-order thinking; (2) the broad use of ChatGPT at various grade levels and in different types of courses should be encouraged to support diverse learning needs; (3) ChatGPT should be actively integrated into different learning modes to enhance student learning, especially in problem-based learning; (4) continuous use of ChatGPT should be ensured to support student learning, with a recommended duration of 4--8 weeks for more stable effects; (5) ChatGPT should be flexibly integrated into teaching as an intelligent tutor, learning partner, and educational tool. Finally, due to the limited sample size for learning perception and higher-order thinking, and the moderately positive effect, future studies with expanded scope should further explore how to use ChatGPT more effectively to cultivate students' learning perception and higher-order thinking.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Education,Science,technology and society},
  file = {/Users/thomasgorman/Zotero/storage/LBVQHD82/Wang and Fan - 2025 - The effect of ChatGPT on students’ learning performance, learning perception, and higher-order think.pdf}
}

@article{weberObjectiveMeasurementAI2023,
  title = {Toward an {{Objective Measurement}} of {{AI Literacy}}},
  author = {Weber, Patrick and Pinski, Marc and Baum, Lorenz},
  year = {2023},
  abstract = {Humans multitudinously interact with Artificial Intelligence (AI) as it permeates every aspect of contemporary professional and private life. The socio-technical competencies of humans, i.e., their AI literacy, shape human-AI interactions. While academia does explore AI literacy measurement, current literature exclusively approaches the topic from a subjective perspective. This study draws on a well-established scale development procedure employing ten expert interviews, two card-sorting rounds, and a betweensubject comparison study with 88 participants in two groups to define, conceptualize, and empirically validate an objective measurement instrument for AI literacy. With 16 items, our developed instrument discriminates between an AI-literate test and a control group. Furthermore, the structure of our instrument allows us to distinctly assess AI literacy aspects. We contribute to IS education research by providing a new instrument and conceptualizing AI literacy, incorporating critical themes from the literature. Practitioners may employ our instrument to assess AI literacy in their organizations.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/PY8GMC47/Weber et al. - 2023 - Toward an Objective Measurement of AI Literacy.pdf}
}

@misc{wischnewskiDevelopmentValidationTrust2025,
  title = {Development and Validation of the {{Trust}} in {{AI Scale}} ({{TAIS}})},
  author = {Wischnewski, Magdalena and Doebler, Philipp and Kr{\"a}mer, Nicole},
  year = {2025},
  month = feb,
  publisher = {OSF},
  doi = {10.31234/osf.io/eqa9y_v1},
  urldate = {2025-05-03},
  abstract = {In everyday life, users increasingly interact and communicate with AI systems. Despite the importance of trust in AI as an influencing factor for this interaction, there is a shortage of validated scales to reliably measure users' trust. In this paper, we present a theory-driven development and validation of the Trust in AI scale (TAIS) that consists of the subdimensions ability, integrity, transparency, unbiasedness, vigilance, and global trust. To validate the scale, we conducted two studies. In study 1 (N = 883 participants), we derived 57 items from theory and existing scales for which an exploratory factor analysis resulted in a 30 item scale. In study 2 (N = 1204 participants), we tested the psychometric quality of the scale through confirmatory factor analysis for ordinal data. Employing a bifactor model with global trust as the higher-order factor, our results confirm the six-factor structure. Correlational results of context variables and related scales support the convergent validity of the scale. Results show that existing scales rather correlate with the global trust factor but less with specific factors (especially vigilance) - indicating that the TAIS scale helps to uncover new facets of trust and thereby goes beyond what existing, less validated scales can provide.},
  archiveprefix = {OSF},
  langid = {american},
  keywords = {artificial intelligence,bifactor model,calibrated trust,scale development},
  annotation = {https://osf.io/9zp6y/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/BBE87LYI/Wischnewski et al. - 2025 - Development and validation of the Trust in AI Scale (TAIS).pdf}
}

@article{wojtonInitialValidationTrust2020,
  title = {Initial Validation of the Trust of Automated Systems Test ({{TOAST}})},
  author = {Wojton, Heather M. and Porter, Daniel and T Lane, Stephanie and Bieber, Chad and Madhavan, Poornima},
  year = {2020},
  month = nov,
  journal = {The Journal of Social Psychology},
  volume = {160},
  number = {6},
  pages = {735--750},
  issn = {1940-1183},
  doi = {10.1080/00224545.2020.1749020},
  abstract = {Trust is a key determinant of whether people rely on automated systems in the military and the public. However, there is currently no standard for measuring trust in automated systems. In the present studies, we propose a scale to measure trust in automated systems that is grounded in current research and theory on trust formation, which we refer to as the Trust in Automated Systems Test (TOAST). We evaluated both the reliability of the scale structure and criterion validity using independent, military-affiliated and civilian samples. In both studies we found that the TOAST exhibited a two-factor structure, measuring system understanding and performance (respectively), and that factor scores significantly predicted scores on theoretically related constructs demonstrating clear criterion validity. We discuss the implications of our findings for advancing the empirical literature and in improving interface design.},
  langid = {english},
  keywords = {Adult,Automation,Confirmatory Factor Analysis,Female,Human-Machine Trust,Humans,Male,Man-Machine Systems,Military Personnel,Reproducibility of Results,Surveys and Questionnaires,Trust,Trust in Automation,Trust Scale},
  annotation = {https://testscience.org/wp-content/uploads/formidable/20/TOAST.pdf\\
\\
https://osf.io/swkny/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/DDT344YB/Wojton et al. - 2020 - Initial validation of the trust of automated systems test (TOAST).pdf}
}

@misc{xiongCanLLMsExpress2024,
  title = {Can {{LLMs Express Their Uncertainty}}? {{An Empirical Evaluation}} of {{Confidence Elicitation}} in {{LLMs}}},
  shorttitle = {Can {{LLMs Express Their Uncertainty}}?},
  author = {Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and Li, Yifei and Fu, Jie and He, Junxian and Hooi, Bryan},
  year = {2024},
  month = mar,
  number = {arXiv:2306.13063},
  eprint = {2306.13063},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.13063},
  urldate = {2025-03-31},
  abstract = {Empowering large language models to accurately express confidence in their answers is essential for trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/MiaoXiong2320/llm-uncertainty},
  file = {/Users/thomasgorman/Zotero/storage/6S4AEFC6/Xiong et al. - 2024 - Can LLMs Express Their Uncertainty An Empirical Evaluation of Confidence Elicitation in LLMs.pdf;/Users/thomasgorman/Zotero/storage/JWSLXJ28/2306.html}
}

@article{xuConfrontingVerbalizedUncertainty2025,
  title = {Confronting Verbalized Uncertainty: {{Understanding}} How {{LLM}}'s Verbalized Uncertainty Influences Users in {{AI-assisted}} Decision-Making},
  shorttitle = {Confronting Verbalized Uncertainty},
  author = {Xu, Zhengtao and Song, Tianqi and Lee, Yi-Chieh},
  year = {2025},
  month = mar,
  journal = {International Journal of Human-Computer Studies},
  volume = {197},
  pages = {103455},
  issn = {1071-5819},
  doi = {10.1016/j.ijhcs.2025.103455},
  urldate = {2025-02-25},
  abstract = {Due to the human-like nature, large language models (LLMs) often express uncertainty in their outputs. This expression, known as ''verbalized uncertainty'', can appear in phrases such as ''I'm sure that [...]'' or ''It could be [...]''. However, few studies have explored how this expression impacts human users' feelings towards AI, including their trust, satisfaction and task performance. Our research aims to fill this gap by exploring how different levels of verbalized uncertainty from the LLM's outputs affect users' perceptions and behaviors in AI-assisted decision-making scenarios. To this end, we conducted a between-condition study (N = 156), dividing participants into six groups based on two accuracy conditions and three conditions of verbalized uncertainty. We also used the widely played word guessing game Codenames to simulate the role of LLMs in assisting human decision-making. Our results show that medium verbalized uncertainty in the LLM's expressions consistently leads to higher user trust, satisfaction, and task performance compared to high and low verbalized uncertainty. Our results also show that participants experience verbalized uncertainty differently based on the accuracy of the LLM. This study offers important implications for the future design of LLMs, suggesting adaptive strategies to express verbalized uncertainty based on the LLM's accuracy.},
  keywords = {AI-assisted decision-making,Large language model,Performance,Satisfaction,Trust,Verbalized uncertainty},
  annotation = {https://ars-els-cdn-com.proxyiub.uits.iu.edu/content/image/1-s2.0-S1071581925000126-mmc1.pdf},
  file = {/Users/thomasgorman/Zotero/storage/UN8UX4GI/Xu et al. - 2025 - Confronting verbalized uncertainty Understanding how LLM’s verbalized uncertainty influences users.pdf;/Users/thomasgorman/Zotero/storage/92GIBE8W/S1071581925000126.html}
}

@misc{xuSaySelfTeachingLLMs2024,
  title = {{{SaySelf}}: {{Teaching LLMs}} to {{Express Confidence}} with {{Self-Reflective Rationales}}},
  shorttitle = {{{SaySelf}}},
  author = {Xu, Tianyang and Wu, Shujin and Diao, Shizhe and Liu, Xiaoze and Wang, Xingyao and Chen, Yangyi and Gao, Jing},
  year = {2024},
  month = oct,
  number = {arXiv:2405.20974},
  eprint = {2405.20974},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.20974},
  urldate = {2025-03-24},
  abstract = {Large language models (LLMs) often generate inaccurate or fabricated information and generally fail to indicate their confidence, which limits their broader applications. Previous work elicits confidence from LLMs by direct or self-consistency prompting, or constructing specific datasets for supervised finetuning. The prompting-based approaches have inferior performance, and the training-based approaches are limited to binary or inaccurate group-level confidence estimates. In this work, we present the advanced SaySelf, a training framework that teaches LLMs to express more accurate fine-grained confidence estimates. In addition, beyond the confidence scores, SaySelf initiates the process of directing LLMs to produce self-reflective rationales that clearly identify gaps in their parametric knowledge and explain their uncertainty. This is achieved by using an LLM to automatically summarize the uncertainties in specific knowledge via natural language. The summarization is based on the analysis of the inconsistency in multiple sampled reasoning chains, and the resulting data is utilized for supervised fine-tuning. Moreover, we utilize reinforcement learning with a meticulously crafted reward function to calibrate the confidence estimates, motivating LLMs to deliver accurate, high-confidence predictions and to penalize overconfidence in erroneous outputs. Experimental results in both in-distribution and out-of-distribution datasets demonstrate the effectiveness of SaySelf in reducing the confidence calibration error and maintaining the task performance. We show that the generated self-reflective rationales are reasonable and can further contribute to the calibration. The code is made public at https://github.com/xu1868/SaySelf.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://github.com/tianyang-x/SaySelf\\
\\
https://www.youtube.com/watch?v=9dBYBnE8rZk\&ab\_channel=PaperWithVideo},
  file = {/Users/thomasgorman/Zotero/storage/G9ACZB6D/Xu et al. - 2024 - SaySelf Teaching LLMs to Express Confidence with Self-Reflective Rationales.pdf;/Users/thomasgorman/Zotero/storage/P7DN9WLF/2405.html}
}

@misc{yingUnderstandingEpistemicLanguage2025,
  title = {Understanding {{Epistemic Language}} with a {{Language-augmented Bayesian Theory}} of {{Mind}}},
  author = {Ying, Lance and {Zhi-Xuan}, Tan and Wong, Lionel and Mansinghka, Vikash and Tenenbaum, Joshua B.},
  year = {2025},
  month = apr,
  number = {arXiv:2408.12022},
  eprint = {2408.12022},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.12022},
  urldate = {2025-05-13},
  abstract = {How do people understand and evaluate claims about others' beliefs, even though these beliefs cannot be directly observed? In this paper, we introduce a cognitive model of epistemic language interpretation, grounded in Bayesian inferences about other agents' goals, beliefs, and intentions: a language-augmented Bayesian theory-of-mind (LaBToM). By translating natural language into an epistemic ``language-of-thought'' with grammar-constrained LLM decoding, then evaluating these translations against the inferences produced by inverting a generative model of rational action and perception, LaBToM captures graded plausibility judgments of epistemic claims. We validate our model in an experiment where participants watch an agent navigate a maze to find keys hidden in boxes needed to reach their goal, then rate sentences about the agent's beliefs. In contrast with multimodal LLMs (GPT-4o, Gemini Pro) and ablated models, our model correlates highly with human judgments for a wide range of expressions, including modal language, uncertainty expressions, knowledge claims, likelihood comparisons, and attributions of false belief.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {https://github.com/cosilab/LaBToM.jl\\
\\
https://osf.io/xq9c4/files/osfstorage},
  file = {/Users/thomasgorman/Zotero/storage/X8BD88WV/Ying et al. - 2025 - Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind.pdf;/Users/thomasgorman/Zotero/storage/AYEDV4YS/2408.html}
}

@article{zhaiEffectsOverrelianceAI2024,
  title = {The Effects of Over-Reliance on {{AI}} Dialogue Systems on Students' Cognitive Abilities: A Systematic Review},
  shorttitle = {The Effects of Over-Reliance on {{AI}} Dialogue Systems on Students' Cognitive Abilities},
  author = {Zhai, Chunpeng and Wibowo, Santoso and Li, Lily D.},
  year = {2024},
  month = jun,
  journal = {Smart Learning Environments},
  volume = {11},
  number = {1},
  pages = {28},
  issn = {2196-7091},
  doi = {10.1186/s40561-024-00316-7},
  urldate = {2025-05-02},
  abstract = {The growing integration of artificial intelligence (AI) dialogue systems within educational and research settings highlights the importance of learning aids. Despite examination of the ethical concerns associated with these technologies, there is a noticeable gap in investigations on how these ethical issues of AI contribute to students' over-reliance on AI dialogue systems, and how such over-reliance affects students' cognitive abilities. Overreliance on AI occurs when users accept AI-generated recommendations without question, leading to errors in task performance in the context of decision-making. This typically arises when individuals struggle to assess the reliability of AI or how much trust to place in its suggestions. This systematic review investigates how students' over-reliance on AI dialogue systems, particularly those embedded with generative models for academic research and learning, affects their critical cognitive capabilities including decision-making, critical thinking, and analytical reasoning. By using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, our systematic review evaluated a body of literature addressing the contributing factors and effects of such over-reliance within educational and research contexts. The comprehensive literature review spanned 14 articles retrieved from four distinguished databases: ProQuest, IEEE Xplore, ScienceDirect, and Web of Science. Our findings indicate that over-reliance stemming from ethical issues of AI impacts cognitive abilities, as individuals increasingly favor fast and optimal solutions over slow ones constrained by practicality. This tendency explains why users prefer efficient cognitive shortcuts, or heuristics, even amidst the ethical issues presented by AI technologies.},
  keywords = {Analytical thinking,Cognitive abilities,Critical thinking,Decision-making,Ethical issues of AI,Generative AI},
  file = {/Users/thomasgorman/Zotero/storage/C6I7AWZ3/Zhai et al. - 2024 - The effects of over-reliance on AI dialogue systems on students' cognitive abilities a systematic r.pdf;/Users/thomasgorman/Zotero/storage/2VTDXKQW/s40561-024-00316-7.html}
}

@article{zhangArtificialIntelligenceAmerican2019,
  title = {Artificial {{Intelligence}}: {{American Attitudes}} and {{Trends}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Zhang, Baobao and Dafoe, Allan},
  year = {2019},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.3312874},
  urldate = {2025-05-02},
  abstract = {This report presents a broad look at the American public's attitudes toward artificial intelligence (AI) and AI governance, based on findings from a nationally representative survey of 2,000 American adults. As the study of the public opinion toward AI is relatively new, we aimed for breadth over depth, with our questions touching on: workplace automation; attitudes regarding international cooperation; the public's trust in various actors to develop and regulate AI; views about the importance and likely impact of different AI governance challenges; and historical and cross-national trends in public opinion regarding AI. Our results provide preliminary insights into the character of US public opinion regarding AI.},
  langid = {english},
  annotation = {https://isps.yale.edu/sites/default/files/files/Zhang\_us\_public\_opinion\_report\_jan\_2019.pdf},
  file = {/Users/thomasgorman/Zotero/storage/FE6REEW5/Zhang and Dafoe - 2019 - Artificial Intelligence American Attitudes and Trends.pdf}
}

@inproceedings{zhangCalibratingConfidenceLarge2024,
  title = {Calibrating the {{Confidence}} of {{Large Language Models}} by {{Eliciting Fidelity}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Mozhi and Huang, Mianqiu and Shi, Rundong and Guo, Linsen and Peng, Chong and Yan, Peng and Zhou, Yaqian and Qiu, Xipeng},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {2959--2979},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.173},
  urldate = {2025-05-12},
  abstract = {Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the Uncertainty about the question and the Fidelity to the answer generated by language models. Then, we propose a plug-and-play method, UF Calibration, to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on Truly Well-Calibrated Confidence for large language models. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.},
  file = {/Users/thomasgorman/Zotero/storage/7Q256UY4/Zhang et al. - 2024 - Calibrating the Confidence of Large Language Models by Eliciting Fidelity.pdf}
}

@article{zhuAutocorrelatedBayesianSampler2023,
  title = {The {{Autocorrelated Bayesian Sampler}}: {{A Rational Process}} for {{Probability Judgments}}, {{Estimates}}, {{Confidence Intervals}}, {{Choices}}, {{Confidence Judgments}}, and {{Response Times}}},
  shorttitle = {The Autocorrelated {{Bayesian}} Sampler},
  author = {Zhu, Jian-Qiao and Sundh, Joakim and Spicer, Jake and Chater, Nick and Sanborn, Adam N.},
  year = {2023},
  month = jun,
  journal = {Psychological Review},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/rev0000427},
  urldate = {2023-11-08},
  abstract = {Normative models of decision-making that optimally transform noisy (sensory) information into categorical decisions qualitatively mismatch human behavior. Indeed, leading computational models have only achieved high empirical corroboration by adding task-specific assumptions that deviate from normative principles. In response, we offer a Bayesian approach that implicitly produces a posterior distribution of possible answers (hypotheses) in response to sensory information. But we assume that the brain has no direct access to this posterior, but can only sample hypotheses according to their posterior probabilities. Accordingly, we argue that the primary problem of normative concern in decision-making is integrating stochastic hypotheses, rather than stochastic sensory information, to make categorical decisions. This implies that human response variability arises mainly from posterior sampling rather than sensory noise. Because human hypothesis generation is serially correlated, hypothesis samples will be autocorrelated. Guided by this new problem formulation, we develop a new process, the Autocorrelated Bayesian Sampler (ABS), which grounds autocorrelated hypothesis generation in a sophisticated sampling algorithm. The ABS provides a single mechanism that qualitatively explains many empirical effects of probability judgments, estimates, confidence intervals, choice, confidence judgments, response times, and their relationships. Our analysis demonstrates the unifying power of a perspective shift in the exploration of normative models. It also exemplifies the proposal that the ``Bayesian brain'' operates using samples not probabilities, and that variability in human behavior may primarily reflect computational rather than sensory noise.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhu et al_2023_The Autocorrelated Bayesian Sampler3.pdf}
}
