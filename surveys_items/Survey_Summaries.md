





***

## Summary Table of Surveys and Scales

| Survey/Scale Name                              | Primary Focus/Construct(s) Measured                                                                                                         | Author(s) & Year                           | # Items (Approx.) | Response Format                                                                                                           | Source File             |
| :--------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------ | :----------------------------------------- | :---------------- | :------------------------------------------------------------------------------------------------------------------------ | :---------------------- |
| **AI-Related Surveys/Scales** |                                                                                                                                             |                                            |                   |                                                                                                                           |                         |
| Scale of Trust in Automated Systems (TPA)      | Trust in Automation (Deception, Confidence, Reliability, Familiarity)                                                                       | Jian, Bisantz, & Drury (2000)              | 12                | 7-point Likert (Not at all - Extremely)                                                                                   | AI_Survey_Concat.md     |
| Complacency-Potential Rating Scale             | Attitudes towards automation, reliability, safety, trust in specific systems (ATM, medical, VCR)                                              | Singh, Molloy, & Parasuraman (1993)        | 20                | 5-point Likert (Strongly agree - Strongly disagree)                                                                       | AI_Survey_Concat.md     |
| Human-Computer Trust Instrument              | Trust Dimensions (Perceived Reliability, Technical Competence, Understandability, Faith, Personal Attachment)                                 | Madsen & Gregor (2000)                     | 25 (5 per factor) | Likert (Scale not specified in snippet)                                                                                   | AI_Survey_Concat.md     |
| SHAPE Automation Trust Index (SATI)            | Trust/Confidence in Simulated Systems (Pre/Post assessment)                                                                                 | Goillau, Kelly, Boardman, & Jeannot (2001) | ~6 + Open-ended | Visual Analog Scale (Bad-Good, None-Full Confidence), Yes/No, Open-ended Reasons                                          | AI_Survey_Concat.md     |
| MAILS (Meta-Artificial Intelligence Literacy)  | AI Literacy (Use, Know, Detect, Ethics), Create AI, AI Self-Efficacy (Problem-Solving, Learning), AI Self-Competency (Persuasion, Emotion) | Carolus et al. (2023)                      | 34                | 11-point scale (0=Not at all - 10=Perfectly pronounced)                                                                 | AI_Survey_Concat.md     |
| MAILS - Short Version                          | AI Literacy (Detect, Apply, Understand, Ethics), Create AI, AI Self-Efficacy (Learning, Problem-Solving), AI Self-Competency (Persuasion, Emotion) | Koch et al. (2024)                         | 10                | 11-point scale (0=Not at all - 10=Perfectly pronounced)                                                                 | AI_Survey_Concat.md     |
| Mental Capacity Attribution Scale            | Perceived Mental Capacities of LLMs (Body, Heart, Mind dimensions)                                                                        | Chen et al. (2025)                         | 40                | 7-point Likert (Not at all capable - Highly capable)                                                                      | AI_Survey_Concat.md     |
| Chen et al. Misc Scales                        | Anthropomorphism, Self-Efficacy, Trust, General Attitudes, LLM Familiarity                                                                  | Chen et al. (2025)                         | ~12 + Open-ended  | 7-point Likert (various anchors), Open-ended, Multi-select Familiarity scale                                              | AI_Survey_Concat.md     |
| TILLMI (Trust-In-LLMs Index)                   | Trust in LLMs (Closeness, Reliance)                                                                                                       | Duro et al. (2025)                         | 6 (final), 8 (initial) | 5-point Likert (Frequency/Agreement anchors)                                                                              | AI_Survey_Concat.md     |
| Fang et al. Psychosocial Scales                | Loneliness (ULS-8), Socialization (LSNS-6), Emotional Dependence (ADS-9), Problematic Use (PCUS)                                              | Fang et al. (2025)                         | Varies per scale  | 4-point Likert (Loneliness), 6-point Frequency (Socialization), 5-point Likert (Dependence, Problematic Use)            | AI_Survey_Concat.md     |
| Fang et al. Exploratory Measures               | Humanness, Perceived Intelligence, Chatbot Usage Frequency                                                                                  | Fang et al. (2025)                         | 9 + Usage Qs      | 5-point Semantic Differential (Humanness/Intelligence), 5-point Likert (Frequency)                                      | AI_Survey_Concat.md     |
| Gerlich AI Usage & Cognition Survey            | AI Tool Usage Frequency/Reliance, Trust, Cognitive Offloading, Critical Thinking                                                            | Gerlich (2025)                             | 18                | 6-point Likert/Frequency/Likelihood/Confidence/Dependence scales                                                          | AI_Survey_Concat.md     |
| Gnambs et al. AI Attitudes                     | Attitudes towards AI in specific domains (Work, Healthcare, Education) - Cognitive, Affective, Behavioral facets                             | Gnambs, Griese et al. (2025)               | 9                 | 5-point Likert (0=strongly disagree - 4=strongly agree)                                                                   | AI_Survey_Concat.md     |
| AIEASM (AI Experience & Attitude Survey)       | Experience, Attitude, Future Use Intention towards specific AI scenarios (Virtual assistants, Recommenders, Robots, etc.)                   | Gnambs, Griese et al. (2025)               | 4 per scenario    | 5-point Frequency, 11-point Attitude (Negative-Positive), 5-point Experience/Intention scales                           | AI_Survey_Concat.md     |
| AIAS-4 (AI Attitude Scale)                   | General Attitude Towards AI                                                                                                                 | Grassini (2023)                            | 4                 | 10-point Likert (1=Not at all - 10=Completely agree)                                                                      | AI_Survey_Concat.md     |
| Kim et al. Trust & Perception Scales           | Confidence (in AI answer/own answer), Reliance Source, Transparency, Anthropomorphism, Trust (Belief/Intention), LLM Familiarity/Use/Attitude | Kim et al. (2024)                          | ~30 + Open-ended  | 5-point Likert (Confidence, Trust, Transparency), 5-point Semantic Differential (Anthropomorphism), Categorical (LLM) | AI_Survey_Concat.md     |
| TiA (Trust in Automation) Questionnaire        | Trust Dimensions (Reliability/Competence, Understanding/Predictability, Familiarity, Intentions, Propensity, Overall Trust)                 | Körber (2019)                              | 19                | 5-point Likert (Strongly disagree - Strongly agree)                                                                       | AI_Survey_Concat.md     |
| SNAIL (Scale for Non-experts' AI Literacy)     | AI Literacy (Understanding concepts, Societal/Ethical implications, Application awareness)                                                  | Laupichler et al. (2023)                   | 31 (final), 39 (initial) | 7-point Likert (Strongly disagree - Strongly agree)                                                                       | AI_Survey_Concat.md     |
| Lee et al. AI Prediction Belief Scales       | Prediction Believability (Convincing, Personalization, Reliability, Usefulness) for AI, Astrology, Personality sources                | Lee et al. (2024)                          | 8 per source      | 7-point Likert (Strongly disagree - Strongly agree)                                                                       | AI_Survey_Concat.md     |
| Lee et al. Misc Scales                         | CRT-2, Need for Cognition (NCS-6), AI Attitude (AIAS-4), Gullibility, Paranormal Beliefs (R-PBS), Familiarity, AI Experience, Perception | Lee et al. (2024)                          | Varies            | Multiple Choice (CRT), Likert (NCS, AIAS, Gullibility, R-PBS, Perception), Categorical/Likert (Familiarity, Experience) | AI_Survey_Concat.md     |
| GSE-6AI (General Self-Efficacy - AI version) | Perceived Self-Efficacy in using AI                                                                                                         | Morales-García et al. (2024)               | 6                 | 4-point Likert (Not at all true - Exactly true)                                                                           | AI_Survey_Concat.md     |
| Morrill & Noetel AI Literacy Scales          | Subjective & Objective AI Literacy                                                                                                          | Morrill & Noetel (2023)                    | 6 (Subj) + 5 (Obj) | 7-point Likert (Subjective), Multiple Choice (Objective)                                                                  | AI_Survey_Concat.md     |
| Morrill & Noetel Trust & Reliance Measures     | Trust in AI, Reliance on AI (Fake review task), Confidence                                                                                  | Morrill & Noetel (2023)                    | 6 + Task + Conf.  | 7-point Likert (Trust), Binary Choice (Reliance Task), Slider (Confidence)                                                | AI_Survey_Concat.md     |
| AILQ (AI Literacy Questionnaire)               | AI Literacy (Affective, Behavioral, Cognitive, Ethical dimensions)                                                                          | Ng et al. (2024)                           | ~40 (final)       | Likert (Scale not specified in snippet, likely 5 or 7-point)                                                              | AI_Survey_Concat.md     |
| Ovsyannikova Compassion/Responsiveness         | Perceived Compassion, Response Preference, Perceived Responsiveness (Understanding, Validation, Caring)                                     | Ovsyannikova et al. (2025)               | ~12 + Preference  | 5-point Likert (Compassion, Responsiveness), Binary Choice (Preference)                                                   | AI_Survey_Concat.md     |
| Puppart & Aru AI Lit & Reliance              | AI Literacy Self-Assessment, Reliance on ChatGPT (Math puzzles), Confidence                                                                 | Puppart & Aru (2025)                       | 3 + Task + Conf.  | 4-point Likert (Self-Assess), Binary Choice (Reliance), 5-point Likert (Confidence)                                       | AI_Survey_Concat.md     |
| Rheu & Cho AI Lit, Heuristics, Fact-Checking | AI Literacy (Self-Efficacy, Cognitive, Tech Knowledge, Steps Knowledge), Machine Heuristics, Fact-Checking Behavior, Credibility             | Rheu & Cho (2025)                          | ~40               | 7-point Likert (AI Lit, Heuristics, Fact-Check), 7-point Semantic Differential (Credibility)                              | AI_Survey_Concat.md     |
| PAICE (Perceptions on AI - Europe)           | AI Competency, Attitude, Impact Awareness, Knowledge (Applications, Sectors), Trust (Measures, Entities), Comfort (Scenarios)             | Scantamburlo et al. (2025)               | ~50 + Ranking     | 5-point Likert (various anchors), Yes/No, Multiple Select, Ranking                                                        | AI_Survey_Concat.md     |
| Scharowski Trust Measures                      | Risk/Unfavorableness Perception, Single-item Trust/Use, TPA, TXAI (adapted), STS-AD/Chatbot, PANAS (Affect)                               | Scharowski et al. (2025)                   | Varies            | Sliders (0-100), 7-point Likert (Single items, TPA, STS), 5-point Likert (TXAI, PANAS)                                     | AI_Survey_Concat.md     |
| PTT-A (Propensity to Trust - Automation)       | Propensity to Trust Automation (Trusting Stance, Competence, Benevolence, Integrity)                                                        | Schulz et al. (2025)                       | 14 (full), 6 (short) | 5-point Likert (Strongly disagree - Strongly agree)                                                                       | AI_Survey_Concat.md     |
| PTT-H (Propensity to Trust - Humans)           | Propensity to Trust Humans (Trusting Stance, Competence, Benevolence, Integrity)                                                            | Schulz et al. (2025)                       | 14 (full), 6 (short) | 5-point Likert (Strongly disagree - Strongly agree)                                                                       | AI_Survey_Concat.md     |
| Affective & Cognitive Trust Scale (AI)       | Affective and Cognitive Dimensions of Trust in AI                                                                                         | Shang et al. (2025)                        | 27                | 7-point Semantic Differential (-3 to +3)                                                                                  | AI_Survey_Concat.md     |
| ATAI (Attitude Towards AI)                     | General Attitude Towards AI (Fear, Trust, Benefit/Harm, Job Loss)                                                                           | Sindermann et al. (2021)                   | 5                 | 11-point Likert (0=strongly disagree - 10=strongly agree)                                                                 | AI_Survey_Concat.md     |
| Sindermann et al. Specific AI Measures       | Trust, Willingness to Use, Actual Use (for specific AI products: Self-driving cars, Siri, Alexa, Pepper, Erica)                              | Sindermann et al. (2021)                   | 3 per product     | 11-point Likert (Trust, Willingness), Yes/No (Actual Use)                                                                 | AI_Survey_Concat.md     |
| Song et al. Social Influence Scales            | User Opinion (pre/post), Informational Influence, Normative Influence, Agent Impressions, AI Acceptance, Compliance, Conformity             | Song et al. (2024)                         | Varies            | 6-point Likert (Opinion), 7-point Likert (Influence, Impressions, Acceptance, Compliance, Conformity), Open-ended        | AI_Survey_Concat.md     |
| SAIL4ALL (Scale of AI Literacy for All)      | AI Literacy (Recognizing AI, Capabilities, How it Works, Ethics)                                                                            | Soto-Sanfiel et al. (2024)                 | 58                | True/False OR 5-point Confidence Scale                                                                                    | AI_Survey_Concat.md     |
| Tully et al. AI Literacy (25-item)             | Objective AI Literacy                                                                                                                       | Tully, Longoni, & Appel (2023)             | 25                | Multiple Choice                                                                                                           | AI_Survey_Concat.md     |
| Tully et al. AI Literacy (17-item)             | Objective AI Literacy (Aligned with Long & Magerko competencies)                                                                            | Tully, Longoni, & Appel (2023)             | 17                | Multiple Choice                                                                                                           | AI_Survey_Concat.md     |
| Tully et al. AI Receptivity & Perception     | AI Receptivity (Adoption Readiness, Propensity to Use, Relative Preference), Perceptions (Magic, Awe, Fear, Capability)                      | Tully, Longoni, & Appel (2023)             | Varies            | Yes/No, 5-point Likert (Propensity), 7-point Likert (Preference, Magic, Awe, Fear, Capability), 4-point Likert (Readiness) | AI_Survey_Concat.md     |
| Weber et al. Objective AI Literacy             | Objective AI Literacy (Socio User, Socio Creator/Evaluator, Technical User, Technical Creator/Evaluator)                                    | Weber et al. (2023)                        | 16                | Multiple Choice                                                                                                           | AI_Survey_Concat.md     |
| Zhang & Dafoe AI Attitudes & Trends          | Perceived Global Risks (incl. AI), AI Definition Perception, CS Knowledge, Support for AI Dev., Trust (Actors), Governance Challenges, Arms Race | Zhang & Dafoe (2019)                       | Varies            | Likelihood/Impact Scales, Multiple Select, Likert (Support, Agreement), Confidence Scales                                 | AI_Survey_Concat.md     |
| Zhu et al. Empathic Mental Inference Tasks   | User Goal Interpretation, Fundamental Psychological Needs Attribution (User self-report & Designer inference)                               | Zhu et al. (2024)                          | Open-ended + 13   | Open-ended Goal Elicitation, 5-point Likert (Needs Attribution)                                                           | AI_Survey_Concat.md     |
| **Energy-Related Surveys/Scales** |                                                                                                                                             |                                            |                   |                                                                                                                           |                         |
| Attari et al. Energy Perceptions Survey      | Energy Saving Behaviors, Perceived Consumption (Household/Appliance), Perceived Savings (Household/Transport), Transport/Recycling Knowledge, Ease, Attitudes (NEP subset), Climate Attitudes, Numeracy | Attari et al. (2010)                       | ~60 + Open-ended  | Open-ended, Percentage Estimation, Ranking, 8-point Ease Scale, 7-point Likert (Attitudes)                              | energy_survey_concat.md |
| Attari, Mishra, Motz Tech Survey             | Familiarity/Interest/Barriers (EV/HP), Knowledge (True/False), Attitudes, Exposure, Proportions, Climate Change Perceptions                | Attari, Mishra, Motz (2025)                | ~45 + Open-ended  | Categorical Familiarity, Yes/No, Open-ended, True/False + Confidence Slider, Likert (Attitudes)                             | energy_survey_concat.md |
| Canfield Electricity Info Perceptions        | Understanding (Historical, Neighbor, Appliance info), Preferences (Clarity, Ease, Usefulness, Trust), Intention to Reduce, Energy Literacy | Canfield et al. (2017)                     | ~30 + 8 Lit. Qs   | True/False (Understanding), 7-point Likert (Preferences, Intentions), Multiple Choice (Literacy)                            | energy_survey_concat.md |
| Chiu & DeWaters EIAQ                           | Energy Issue Attitudes (Saving/Carbon Knowledge, Lifestyle, Authority, Tech/Nature Approaches, Future/Present Goals)                        | Chiu & DeWaters (2018)                     | 38                | 5-point Likert (Agree strongly - Disagree strongly)                                                                       | energy_survey_concat.md |
| Cotton et al. Energy Demand Survey           | Perceived Knowledge, Knowledge Test, Attitudes (incl. NEP), Behaviors, Energy Knowledge Source, Issue Importance                              | Cotton et al. (2021)                       | 40 (approx)       | 4-point Knowledge Perception, Multiple Choice (Knowledge), 5-point Likert (Attitudes), 4-point Frequency (Behaviors)      | energy_survey_concat.md |
| DeWaters & Powers ELQ (Original/Adapted)     | Energy Literacy (Cognitive/Knowledge, Affective/Attitude, Behavioral)                                                                       | DeWaters & Powers (2011)                   | 38/17/10 (HS orig) | Multiple Choice (Cognitive), 5-point Likert (Affective, Behavioral)                                                         | energy_survey_concat.md |
| DeWaters et al. ELQ (Revised)                  | Energy Literacy (Cognitive/Knowledge, Affective/Attitude, Behavioral)                                                                       | DeWaters et al. (2013)                     | ~45/17/10         | Multiple Choice (Cognitive), 5-point Likert (Affective, Behavioral)                                                         | energy_survey_concat.md |
| He et al. Energy Decision Experiment         | Norm-Following, Purchase Decisions (Modified Dictator Game), Climate Beliefs/Concerns, Competitiveness, Risk/Time Pref., Loss Aversion   | He et al. (2024)                           | Tasks + ~25       | Allocation Task, Purchase Amount Input, 7-point Likert (Beliefs, Concerns, Compete, Loss Aversion), 11-point Scale (Risk/Time) | energy_survey_concat.md |
| Kantenbacher & Attari Expert Energy Decision | Open-ended Energy Thoughts, Choice Task (Lowest energy use), Estimation Task (Wh), Heuristic Accuracy Rating                              | Kantenbacher & Attari (2021)               | ~4 + Tasks + 35   | Open-ended, Binary Choice, Numeric Estimation, 4-point Likert (Heuristic Accuracy)                                      | energy_survey_concat.md |
| Marghetis et al. Energy Misperceptions       | Estimation Task (Wh - 36 appliances), Behavioral Choice Task (BCT - 20 pairs), Energy/Power Knowledge, Perceptions, Policy/Climate Beliefs, NEP, Numeracy/CRT | Marghetis et al. (2019)                    | ~36 + 20 + ~35    | Numeric Estimation, Binary Choice, Yes/No/DK, Open-ended, Likert Scales (Policy, NEP, Climate), Multiple Choice (Numeracy) | energy_survey_concat.md |

***




--------


# https://aistudio.google.com/prompts/1HJFkZ42YnDAaTQWm4taj3cyPJ4EGtzIQ

This review examines research on human knowledge and attitudes towards Artificial Intelligence (AI) and energy, commonly referred to as AI literacy and energy literacy, respectively. It explores how these literacies relate to various factors such as trust, reliance, behavior, and perceptions, and summarizes the diverse measurement instruments used in the field based on the provided documents.

## AI Literacy Research Summary

AI literacy encompasses understanding AI concepts, capabilities, limitations, and societal implications. Research indicates a complex relationship between AI literacy and user perceptions and behaviors.

*   **Trust, Attitudes, and Receptivity:** Studies show varied links between AI literacy and trust or attitudes. While some might expect higher literacy to lead to more nuanced trust, research like Tully et al. (2023) found that *lower* AI literacy predicted *greater* receptivity to AI, potentially because less knowledgeable individuals perceive AI as more magical or capable. [Tully25_AI_Survey] Scales like the AI Attitude Scale (AIAS-4) and Attitudes Towards Artificial Intelligence (ATTARI-12) are used to measure general sentiment [Grassini23_AI_Scale, Stein24_AI_Survey]. Studies also examine attitudes within specific domains (work, healthcare, education) or towards specific AI types (virtual assistants, robots) [Gnabs_AI_Survey]. Trust is multifaceted, with distinct cognitive (reliability, competence) and affective (empathy, sensitivity) components, which can be measured separately [Shang25_AI_Trust_Scale, TPA]. Scenario-based studies manipulate AI behavior to elicit trust or distrust responses [Scharowski_AI_Survey].
*   **Reliance, Behavior, and Bias:** AI literacy influences how people interact with AI. Rheu & Cho (2025) found a paradoxical relationship where higher self-perceived AI literacy was associated with *less* fact-checking of Large Language Model (LLM) outputs, possibly mediated by increased machine heuristics (perceiving AI as expert, objective, accurate). [Rheu25_AI_Survey] This highlights the potential for overreliance even among those who feel knowledgeable. Research also explores how literacy relates to problematic use, emotional dependence [Fang25_AI_Survey], and beliefs about AI predictions compared to other sources like astrology [Lee24_AI_Surveys].
*   **Perception and Mental Capacity Attribution:** How AI is framed (e.g., as a machine, tool, or companion) affects the mental capacities people attribute to it, an effect potentially moderated by literacy [Chen25_Survey]. Literacy may also shape perceptions of AI's empathy or compassion compared to humans [Ovsyannikova_Survey].

## Energy Literacy Research Summary

Energy literacy involves understanding energy concepts, consumption patterns, and conservation strategies. Research consistently shows gaps in public knowledge and perception.

*   **Perception Gaps:** Studies reveal significant misperceptions about household energy consumption and the effectiveness of different energy-saving behaviors [Attari2010_Survey, Marghetis2019_materials]. People often underestimate the energy used by high-consumption activities (like heating/cooling) and overestimate savings from low-impact actions (like turning off lights) [Attari2010_Survey].
*   **Expertise and Heuristics:** Experts and novices rely on different rules or heuristics to judge energy use. Experts often focus on factors like heating/cooling functions, while novices might use simpler cues like device size [Katenbacher_Attari_2021_sup].
*   **Communication and Interventions:** The format and content of energy information affect understanding and behavioral intentions. Providing appliance breakdowns or neighbor comparisons can improve comprehension, though effectiveness varies based on individual differences like numeracy and energy literacy [Canfield2017_Supp]. Simple interventions, like providing scale-use information or explicit heuristics, can help correct misperceptions about energy use [Marghetis2019_materials]. Social comparison feedback can also influence behavior, though its impact may depend on factors like environmental externality levels [He_Blasch24_energy_survey].
*   **Specific Technologies:** Research is also focusing on public knowledge and attitudes towards specific low-carbon technologies like electric vehicles (EVs) and heat pumps, identifying barriers and motivations for adoption [Attari_2025_TechSurvey].
*   **Comprehensive Literacy Models:** Frameworks like those developed by DeWaters and colleagues assess energy literacy across cognitive (knowledge), affective (attitudes/values), and behavioral (actions) domains [DeWaters_Powers2011_Supp, DeWaters2013_Energy].

## Measurement Instruments

Researchers employ a variety of instruments, including questionnaires, scenario evaluations, and estimation tasks.

### AI Literacy Scales

*   **MAILS (Meta-Artificial Intelligence Literacy Scale) & MAILS-Short:** Measure self-perceived abilities across dimensions like Use & Apply, Know & Understand, Detect AI, AI Ethics, Create AI, AI Self-Efficacy (Problem-Solving, Learning), and AI Self-Competency (Persuasion Literacy, Emotion Regulation). Uses a 0-10 rating scale for perceived ability. [Carolus23_MAILS, Koch24_MAILS_Short]
*   **Tully et al. (2023) AI Literacy Measures:** Developed 25-item and 17-item multiple-choice questionnaires assessing objective knowledge about AI concepts, applications, data, algorithms, and ethics. [Tully25_AI_Survey]
*   **Weber et al. (2023) Objective Measurement Scale:** A 16-item multiple-choice test covering Socio-User, Socio-Creator/Evaluator, Technical-User, and Technical-Creator/Evaluator dimensions of AI literacy. [Weber23_AI_Survey]
*   **Rheu & Cho (2025) AI Literacy Measure:** Combines scales measuring Self-Efficacy, Cognitive Learning, AI Technology Knowledge, and AI-Steps Knowledge using 7-point Likert scales. [Rheu25_AI_Survey]
*   **Wang et al. (2022) (as used by Shang et al., 2025):** Likert-scale items assessing self-perceived ability to identify and use AI. [Shang25_AI_Trust_Scale]

### AI Attitude, Trust, and Perception Scales

*   **ATTARI-12 (Attitudes Towards Artificial Intelligence):** A 12-item scale assessing cognitive, affective, and behavioral components of general AI attitude using a 5-point Likert scale. [Stein24_AI_Survey]
*   **AIAS-4 (AI Attitude Scale):** A brief 4-item measure of general positive attitude towards AI's impact and future use, using a 10-point Likert scale. [Grassini23_AI_Scale, Lee24_AI_Surveys]
*   **AIEASM (Artificial Intelligence Experience and Attitude Survey):** Measures attitudes (cognitive, affective, behavioral) within specific domains (Work, Healthcare, Education) and assesses experience/attitudes towards various AI scenarios (virtual assistants, recommender systems, etc.). Uses Likert and frequency scales. [Gnabs_AI_Survey]
*   **PAICE (Perceptions on AI by the Citizens of Europe):** Assesses self-rated competency, general attitude, perceived impact, awareness of initiatives/applications, comfort with scenarios, and trust in different entities regarding AI. Uses Likert scales and multiple-choice formats. [Scantamburlo_AI_Survey]
*   **TPA (Trust Perception Scale - AI):** A 12-item scale measuring trust and distrust dimensions (e.g., deceptive, suspicious, wary vs. confident, dependable, reliable) using a 7-point "Not at all" to "Extremely" scale. Originally for automated systems, adapted for AI. [TPA, Scharowski_AI_Survey]
*   **Shang et al. (2025) Affective and Cognitive Trust Scale:** A 27-item semantic differential scale measuring trust along cognitive (e.g., Reliable, Competent, Honest) and affective (e.g., Empathetic, Caring, Patient) dimensions using bipolar adjective pairs rated on a 5-point or 7-point scale. [Shang25_AI_Trust_Scale]
*   **Scharowski et al. (2025) Trust/Distrust Scenarios:** Uses videos depicting AI (self-driving car, chatbot) in high/low trust scenarios, followed by questionnaires including TPA, single trust/use items, PANAS (affect), and situation-specific trust scales (STS-AD, STS-Chatbot). [Scharowski_AI_Survey]
*   **Ovsyannikova et al. (2025) Compassion/Responsiveness:** Participants evaluate AI vs. human responses to emotional prompts, rating perceived compassion (using Likert scales) and preference (binary choice). Also measures perceived responsiveness (understanding, validation, caring). [Ovsyannikova_Survey]
*   **Lee et al. (2024) Prediction Believability:** Rates the perceived validity, personalization, reliability, and usefulness of predictions from different sources (AI, astrology, personality) using a 7-point Likert scale. [Lee24_AI_Surveys]
*   **Chen et al. (2025) Mental Capacity Attribution:** Measures the extent to which people attribute various mental capacities (related to body, heart, mind) to LLMs using a 7-point capability scale. [Chen25_Survey]
*   **Song et al. (2024) Opinion/Influence/Impressions:** Measures user opinions on topics before/after discussion with AI agents, perceived informational/normative influence, and impressions of agents (understanding, expertise, intelligence, likability) using Likert scales. [Song24_AI_Survey]
*   **Zhu et al. (2024) Empathic Mental Inference:** Users or designers interpret user goals (specific tasks, sub-actions, overarching goals) from product comments and attribute these goals to fundamental psychological needs (e.g., Autonomy, Competence, Relatedness, Security) using Likert scales. [Zhu2024_AI_Survey]

### Energy Literacy & Knowledge Scales

*   **Attari et al. (2010) Survey on Energy:** Assesses perceived effectiveness of energy-saving behaviors (open-ended), estimates of energy use breakdown (%), relative energy use of devices (numeric estimate vs. 100W bulb), relative energy savings of actions (numeric estimate), attitudes (NEP scale), and demographics/behaviors. [Attari2010_Survey]
*   **DeWaters & Powers (2011) / DeWaters et al. (2013) Energy Literacy Questionnaire:** Comprehensive measures for secondary students covering Cognitive (multiple-choice questions on concepts, resources, usage), Affective (Likert scale agreement with attitude/value statements, including self-efficacy), and Behavioral (Likert scale frequency of conservation actions) domains. [DeWaters_Powers2011_Supp, DeWaters2013_Energy]
*   **Canfield et al. (2017) Electricity-Use Communications:** Measures understanding of different energy report formats (historical use, neighbor comparison, appliance breakdown) via True/False questions, preferences for formats (Likert scales on clarity, usefulness, trust), and intentions to reduce use (Likert scale). Also includes an energy literacy subscale adapted from DeWaters. [Canfield2017_Supp]
*   **Kantenbacher & Attari (2021) Expert Heuristics:** Uses talk-aloud protocols during choice tasks (selecting lower energy appliance) and estimation tasks (vs. 100W bulb) to identify heuristics. Also includes a survey rating the accuracy of predefined heuristic rules. [Katenbacher_Attari_2021_sup]
*   **Marghetis et al. (2019) Misperceptions Survey:** Includes an estimation task (36 appliances vs. 100W bulb), a behavioral choice task (20 pairs, choose lower energy option), numeracy/CRT items, climate/policy questions, and NEP scale, testing the effect of simple interventions (scale-use info, heuristic). [marghetis2019_materials]
*   **Attari et al. (2025) Low Carbon Tech Survey:** Assesses familiarity, ownership, consideration, barriers, and motivations related to EVs and heat pumps (Likert, open-ended). Includes True/False knowledge statements about these technologies and general climate/demographic questions. [Attari_2025_TechSurvey]
*   **He et al. (2024) Social Comparison Feedback:** Uses a multi-round purchase decision task where purchasing emits externalities (simulated CO2). Measures choices and uses survey items for climate change beliefs/concerns, competitiveness, risk/time preferences, loss aversion, and demographics. Tests effects of different feedback types (self, social comparison, tangible emissions). [He_Blasch24_energy_survey]

### Numeracy & Cognitive Scales

*   **Berlin Numeracy Test:** Assesses statistical numeracy and risk literacy through probability problems (e.g., coin flips, lottery chances). Often adapted or used in short forms. [nsf_instrument_codebook (Attari items), marghetis2019_materials, Cokely2012_BerlinNumeracy]
*   **Cognitive Reflection Test (CRT):** Measures the tendency to override incorrect intuitive responses with deliberate correct answers (e.g., bat and ball problem). [marghetis2019_materials, Lee24_AI_Surveys]
*   **Need for Cognition (NCS-6):** Short scale measuring the tendency to engage in and enjoy effortful cognitive activities. [Lee24_AI_Surveys]

### Other Relevant Scales

*   **New Ecological Paradigm (NEP):** Widely used scale measuring pro-environmental orientation by assessing agreement with statements about the human-nature relationship. [Attari2010_Survey, marghetis2019_materials, nsf_instrument_codebook]
*   **Personality Traits:** Big Five Inventory (BFI-44, TIPI), Short Dark Triad (SD3). [Stein24_AI_Survey, Lee24_AI_Surveys]
*   **Beliefs:** Paranormal Belief Scale (R-PBS), Conspiracy Mentality Questionnaire (CMQ), Gullibility Scale. [Lee24_AI_Surveys, Stein24_AI_Survey]
*   **Affect/Emotion:** Positive and Negative Affect Schedule (PANAS). [Scharowski_AI_Survey]
*   **Social/Relational:** Loneliness (ULS-8), Socialization (LSNS-6), Emotional Dependence (ADS-9), Problematic Use (PCUS) - often used in chatbot interaction studies. [Fang25_AI_Survey]











--------


https://chatgpt.com/c/6812fe81-fee0-8006-90b0-443210c7394c
https://aistudio.google.com/prompts/1epaIbD_rD7_ljoSoXueh-0V5YL0JTV1Q

# AI Literacy and Trust in AI: Empirical Studies Review

## Puppert & Aru (2025) – AI Literacy Intervention and Overreliance on AI
**Study Overview:** Puppart and Aru (2025) conducted a controlled experiment with high school seniors to investigate whether a short AI literacy intervention could reduce **overreliance** on AI (ChatGPT) in problem-solving. Participants solved math puzzles with assistance from ChatGPT, which deliberately provided incorrect suggestions on half the trials. One group received a brief educational text about how ChatGPT works, its limitations, and proper use (an AI literacy primer) before the task, while a control group did not. The study’s goal was to see if improving participants’ understanding of AI would decrease their tendency to **over-trust** or rely on wrong AI recommendations.

-   **AI Literacy Operationalization:** The literacy intervention consisted of a one-page text (see *Appendix B* of the paper) explaining ChatGPT’s mechanism, limitations, and appropriate usage. AI literacy was not measured via a score; instead, it was manipulated by providing or withholding this explanatory primer. To gauge its effect, after the task participants rated their *self-perceived knowledge* in three domains: *Usage* of ChatGPT, its *Working mechanism*, and its *Limitations*. Each was measured by a single-item self-report (on a 5-point scale) about how well they felt they understood that aspect. For example, in the **Usage** category participants indicated agreement with statements reflecting their ability to use ChatGPT properly (exact item text is provided in the paper’s Appendix B). The appendix contains the full text of the literacy primer and related materials.

-   **Trust/Overreliance Measures:** Overreliance was behaviorally measured as the frequency of accepting ChatGPT’s incorrect suggestions. If a participant used the AI’s wrong answer instead of solving the puzzle themselves, it counted as **overreliance**. Conversely, instances where they ignored incorrect AI advice (even if they still got the puzzle wrong) were counted as resisting overreliance (labeled “human error” if their own answer was wrong). The study also measured **confidence in AI** by asking participants to rate how confident they were when relying on ChatGPT’s recommendations (via a post-task questionnaire). This confidence rating was compared between groups to see if the literacy primer affected how much trust/confidence students placed in ChatGPT.

-   **Key Findings:** **Overreliance** on the AI was very common – students followed ChatGPT’s incorrect recommendations **52.1%** of the time in the control group. Surprisingly, the AI literacy intervention **did not significantly reduce** overreliance: the intervention group was just as likely to adopt wrong answers as the control group. In fact, the only notable difference was that the intervention group became slightly more cautious with correct AI answers – they were more likely to **ignore even correct suggestions** from ChatGPT (leading to a small increase in missed opportunities). The self-report ratings confirmed the primer did increase participants’ perceived knowledge in “Working mechanism” and “Usage” of ChatGPT (medium effect sizes, *p* < .05), indicating the text successfully taught them about how ChatGPT works and how to use it. However, this greater knowledge did **not translate into better trust calibration**: students with the primer were not better at discerning when to trust or distrust the AI. The confidence ratings when using ChatGPT were slightly lower in the educated group (mean 3.58 vs. 3.67 on a 5-point scale) but not significantly different. Overall, even a short literacy boost did not overcome automation bias in this context – participants still **overrelied on AI**, suggesting more robust or practical training may be needed to reduce inappropriate trust. *(The supplementary materials of this preprint provide the full text of the ChatGPT explanatory intervention and the puzzle questions used, for replication.)*

## Marmolejo-Ramos et al. (2024) – Statistical Literacy and Trust in Algorithmic Decisions
**Study Overview:** Marmolejo-Ramos et al. (2025) conducted a **large-scale survey experiment** (≈2,000 participants across 20 countries) to examine how people’s understanding of statistics and algorithms relates to their **trust in AI-driven decisions**. Rather than a self-perceived AI knowledge measure, this study assessed objective **data/statistical literacy** as a component of AI literacy, positing that understanding how algorithms work (pattern-based predictions, data limitations) would influence trust. Participants were presented with hypothetical decision-making scenarios where an AI system could be used (ranging from low-stakes, like music recommendations, to high-stakes, like medical decisions) and reported how likely they would be to trust, use, or recommend the AI in each scenario. The researchers measured individual differences in *statistical literacy* and *general trust propensity* to see how these affected trust in AI across scenarios.

-   **AI Literacy Operationalization:** They operationalized “AI literacy” in terms of **statistical literacy**, reasoning that understanding statistics and algorithms’ working principles is key to AI literacy. To measure this, they administered 14 items from the **Basic Literacy in Statistics (BLIS)** scale (Ziegler & Garfield, 2018), a multiple-choice test covering core statistical concepts. These items (selected from a larger 37-item item bank) cover topics like data interpretation, probability, and bias – knowledge areas relevant to understanding how AI models function (e.g., recognizing that algorithms may err or have uncertainty). Participants’ **statistical literacy scores** were the total correct out of 14. (The specific BLIS questions used – items #1, 6, 7, 9, 10, 17, 19, 27, 31, 34–37 from the BLIS – are documented in the article’s supplementary material, ensuring transparency of the exact quiz questions.) In addition, they measured *familiarity with AI* by asking participants to rate how familiar they were with “algorithms, data and AI (ADA)” on a 0–5 visual analog scale. This served as a self-report gauge of one’s exposure to or experience with AI technology (an aspect of AI literacy).

-   **Trust/Outcome Measures:** **Trust in AI** was measured in two ways: (1) a generalized *propensity to trust automation* scale, and (2) scenario-specific trust/use intentions. First, participants completed a 6-item **Propensity to Trust (PtT) scale** adapted from Merritt et al. (2013). This is a standard psychometric scale assessing one’s general tendency to trust machines/automation (e.g., rating agreement with statements like “I usually trust automated systems once they have proven to be reliable”). Marmolejo-Ramos et al. used a continuous 0–5 slider format for these items for precision. Second, in each scenario participants indicated how likely they were to *trust the AI’s decision, use the AI’s recommendation, or recommend the AI to others*. These scenario responses were on a 0–5 scale (from “not at all likely” to “very likely”). By varying the scenario context (low-stakes vs. high-stakes decisions, with or without an explanation of the AI’s method), the researchers could observe how trust changes with context and whether literacy moderates those changes. (All scenario texts and trust questions are provided in the article’s appendix/supplement.)

-   **Key Instruments:** The **BLIS** test is a *performance-based instrument* for statistical/AI literacy – here it yielded a score for each participant (α ≈ 0.78 in this sample). The **propensity to trust scale** (6 items) is originally from automation trust literature; sample item: *“I tend to trust technology until it gives me a reason not to”*. In this study it showed good reliability (reported in text). These established instruments lend validity to the measurement approach. The **exact BLIS items and Merritt’s trust items used can be found in the supplementary materials** of the publication, which the authors note for transparency.

-   **Findings:** The study found that domain context critically affects trust in AI, and that **statistical literacy modulates these effects**. In general, people were *less willing to trust AI in high-stakes scenarios* (like medical or hiring decisions) than in low-stakes scenarios (e.g. entertainment). Importantly, those with **higher AI/statistical literacy were more discerning**: they showed *healthy skepticism* in high-stakes situations (lower trust if the stakes were life/health critical) but relatively higher trust in benign scenarios. In contrast, participants with **low statistical literacy or little AI familiarity** did not differentiate – they were “just as likely to trust algorithms for trivial choices as for critical decisions”. This suggests that understanding how AI works (and its limitations) leads to *calibrated trust*: knowledgeable users trusted AI *appropriately* (lower trust when potential risks were high). The analysis confirmed that statistical literacy had a significant interaction with scenario stakes in predicting trust/use intentions (p < .01). Another finding was demographic: older people and men were generally more cautious about AI, and respondents from highly industrialized countries had slightly lower baseline trust in AI. Overall, this study underscores that **AI literacy (in the form of statistical knowledge)** can shape how people balance the benefits and risks of AI, potentially reducing **automation bias** in critical contexts. *(Appendices include the list of BLIS questions and trust scale items for those interested in the specific content of these measures.)*

## Zhang et al. (2024) – AI Literacy, Trust, and AI Dependency among Preservice Teachers
**Study Overview:** Zhang et al. (2024) surveyed 469 preservice mathematics teachers in China to explore how their self-rated **AI literacy** and **trust in AI** relate to their **dependency** on AI and their development of 21st-century skills. This is a domain-general study in the sense that it treats AI literacy and trust as broad constructs, though within a teacher education context. The authors were interested in whether improving teachers’ AI literacy and trust in AI might inadvertently increase “AI dependency” – an overreliance on AI tools – and how that dependency could affect skills like problem-solving and critical thinking. They developed a **self-report questionnaire** covering: (1) AI Literacy, (2) Trust in AI, (3) AI Dependency, and (4) 21st-century skills (e.g. critical thinking). The survey instrument was custom-built based on prior literature, and the exact item wordings were refined via pilot testing and factor analysis.

-   **AI Literacy Measurement:** *AI literacy* was defined as teachers’ **ability to understand AI’s fundamental concepts and technologies** (including ethical and critical thinking aspects). The authors adapted items from existing AI literacy scales – notably the **Meta AI Literacy Scale (MAILS)** by Ng et al. and colleagues (2023) and an AI literacy concept inventory for students. The final AI literacy scale in this study included multiple Likert-type items asking teachers to rate their knowledge and skills related to AI. For example, items assessed understanding of AI principles, proficiency with AI tools, and awareness of AI’s capabilities/limitations. Though exact item text is not quoted in the paper, sample content included statements like *“I understand the basic working principles of AI applications”* or *“I know how to apply AI tools in my teaching practice”* (as inferred from descriptions). All AI literacy items were rated on a 5-point agreement scale. After exploratory factor analysis (EFA), the AI literacy items loaded well on a single factor (or closely related sub-factors), with high internal consistency (Cronbach’s α ≈ 0.91). *Importantly, the survey instrument drew on prior validated scales:* for instance, references indicate it was informed by Carl et al. (2023) – who developed the “Scale for Non-Experts’ AI Literacy (SNAIL)” – and by Zhang & Lee (2024) – who created an AI Literacy Concept Inventory. (These sources contain specific items and competency facets; e.g., SNAIL includes technical understanding, critical appraisal, and practical application items. Zhang et al.’s adaptation tailored these to the teacher context. The exact questionnaire items are available in the original Chinese and English upon request to the authors, as noted in their appendix.)

-   **Trust in AI Measurement:** *Trust in AI* was measured with a set of Likert items divided into two subdimensions: **accepting AI** and **fearing AI**. This approach was adapted from prior research by Sindermann et al. (2021) and others, which distinguish positive acceptance vs. anxiety about AI. For example, an “acceptance” item might be *“I am comfortable relying on AI systems in my work”*, while a “fear” item might be *“I worry about AI making harmful decisions”*. Participants rated agreement on 5-point scales, with some items reverse-coded (the fear-based items). This scale showed good reliability (α = 0.86) and was grounded in existing attitude scales (e.g., the ATAI scale with similar content). Zhang et al. effectively combined these into one overall “trust attitude” construct in their analysis (higher scores = more positive trust/acceptance of AI).

-   **AI Dependency and Outcome Measures:** A unique aspect is the **AI Dependency** scale the authors developed to capture *overreliance or compulsive use* of AI tools. They drew from psychological criteria of dependency (inspired by DSM-5 addiction criteria). The scale had items under three themes: **Feeling of Vulnerability without AI**, **Concern about AI’s Relevance to Performance**, and **Seeking AI for Validation**. In practice, teachers rated statements like *“I feel uneasy if I can’t use AI tools for important tasks”* or *“I depend on AI recommendations to feel confident in my decisions”*. This scale was reliable (α ≈ 0.95) and captured how emotionally and habitually teachers rely on AI. Finally, **21st-century skills** (critical thinking, problem-solving, creativity, collaboration, self-confidence) were each measured by a few self-report items (adapted from educational psychology scales). For instance, critical thinking might be assessed by agreement with *“I often analyze problems from multiple perspectives before deciding, even with AI support”*. These served as outcome variables to see if AI dependency correlated with lower skills. (Sources for these items are given in refs. 110–113 of the paper, and the questionnaire was bilingual in English/Chinese.)

-   **Key Findings:** Both **AI literacy and trust in AI were found to encourage greater AI use/dependency**, which in turn had downsides for certain skills. Specifically, teachers with higher AI literacy tended to have higher trust in AI, and **both factors independently predicted greater AI dependency** (overreliance) on generative AI tools. In other words, those who knew more about AI and had positive attitudes were more likely to lean on AI in their work. However, this **dependency was associated with lower self-rated problem-solving and critical thinking skills** (standardized negative effect). The structural equation model suggested a cautionary effect: while improving AI literacy and trust is important, it needs to be paired with guidance to avoid overdependence, as overreliance might undermine teachers’ development of essential skills. The authors call for balance – enhancing AI literacy **with** training on when *not* to use AI, to maintain one’s own abilities. They also performed a latent profile analysis (in a related study by Wijaya et al., 2024) which identified distinct groups of teachers – from low literacy/low trust to high literacy/high trust – and found those with both high literacy and trust had the **highest dependency** but also slightly better attitudes toward 21st-century skills (suggesting complexity in the relationship). In summary, this study highlights that **AI literacy** (as measured by a multi-facet questionnaire) is positively correlated with **trust and usage of AI**, but without interventions it could lead to **automation overreliance**. *(The survey items were self-designed based on established scales; the paper notes that all items loaded strongly on intended factors and provides references to full instruments like MAILS and ATAI for item-level detail.)*

## Kox & Beretta (2024) – AI Literacy, Attitudes, and Trust in Generative AI Incidents
**Study Overview:** Kox and Beretta (2024) conducted an **exploratory vignette study** to see how **AI literacy** influences people’s **trust in AI chatbots, attitudes toward AI, and reliance** after exposure to real-world AI failure scenarios. They recruited 139 participants from the general public and presented them with five vignettes describing incidents involving generative AI (e.g., cases of AI misinformation, bias, or error in various domains). Participants’ trust and attitude towards AI were measured *before and after* reading the incident vignettes, allowing the researchers to observe changes. The key question: Would those with higher AI literacy respond differently (e.g., be less surprised or upset by AI failures, or show smaller drops in trust)? They also examined whether AI literacy correlates with baseline **trust, usage, and attitude** toward AI.

-   **AI Literacy Measurement:** This study measured AI literacy using the **Meta AI Literacy Scale (MAILS)** – a comprehensive domain-general scale. MAILS is a self-report instrument with 34 items (on an 11-point scale) covering four facets of AI competency: **(1) Use & Apply AI**, **(2) Know & Understand AI**, **(3) Detect AI**, and **(4) AI Ethics**. Kox & Beretta used an 18-item shortened version of MAILS (as indicated in their method) covering those four subscales evenly. Each item asks participants to rate their ability or knowledge in a specific aspect. For example: an item from *Use & Apply AI* is *“I can use AI applications to make my everyday life easier.”*, and an item from *Know & Understand AI* is *“I can assess what the limitations and opportunities of using AI are.”*. Other sample items include *“I can tell if I am dealing with an application based on AI”* (Detect AI) and *“I incorporate ethical considerations when deciding whether to use data provided by an AI.”* (AI Ethics). Participants rated these on a 0 to 10 scale (0 = not at all true of me, 10 = completely true). The MAILS subscales showed strong reliability in this sample (α = 0.86–0.96 for subscales, α = 0.94 overall). *(For full item lists, see Baum et al. (2023) who developed MAILS – the scale items and their facet structure are documented in that open-access source.)*

-   **Trust and Attitude Measures:** **Trust in AI chatbots** was measured via a **12-item human-computer trust scale** (originally developed by Jian et al., 2000) tailored to AI assistants. Example item: *“I think that AI chatbots perform their role as personal assistant very well.”*. Participants rated agreement on a 5-point Likert scale, yielding a trust score before and after seeing the vignettes. The internal consistency was acceptable (α = 0.78 pre-vignette, improving to 0.95 post-vignette, as responses became more polarized). **Attitude toward AI** was measured with a brief scale focusing on emotional reactions to AI. Before the vignettes, participants answered how they *weigh AI’s risks vs. benefits*, and rated three statements: “AI makes me feel ... **worried** / **hopeful** / **angry**,” on a 6-point scale. These captured the affective attitude: higher scores (after reversing “worried” and “angry”) mean a more positive, optimistic attitude toward AI. After the vignettes, the same attitude items were administered to detect changes. This simple affective scale (adapted from Zhang & Dafoe, 2019) has participants reflect on their emotional stance toward AI in general. Additionally, they measured **continuance intention** to use AI with two items (e.g., *“I plan to keep using AI chatbots”*) on a 5-point scale.

-   **Reactions to AI Incidents:** To gauge how participants perceived each AI failure scenario, the study asked them to rate each vignette on several 5-point semantic scales: how **surprising, harmful, morally wrong,** and **distressing** they found the AI’s action in the story. These ratings were aggregated (except “surprising”) into a composite “Perceived severity” score per participant. This served as a measure of **automation bias awareness** – did those with high literacy perceive the incidents as less severe (perhaps because they expected such failures) or equally severe?

-   **Key Findings:** *Before* reading any incidents, participants with **higher AI literacy showed significantly higher trust in AI and more positive attitudes**. AI literacy was positively correlated with initial trust in chatbots and with a hopeful (less worried) attitude (correlation ~0.3–0.4, *p* < .01). They also reported more frequent use of AI chatbots in general. This suggests a baseline trend that people who understand AI better tend to *embrace and use it more*. However, when it came to **evaluating AI failures**, AI literacy did *not* buffer the negative impact of seeing those incidents. Across all participants, trust in AI and attitude toward AI dropped significantly after reading the vignettes (people became more cautious and less positive, as expected). But the size of this drop was **unrelated to AI literacy** – high-literacy individuals were just as likely to lose some trust or become concerned as low-literacy individuals upon seeing the AI mistakes. Moreover, AI literacy did not correlate with how harmful or immoral they rated the incidents; it only correlated with perceiving them as slightly *less surprising* (suggesting knowledgeable people were not shocked that AI could fail) – the surprisingness item was included as a check and indeed those high in literacy were less often caught off guard. Crucially, **higher AI literacy was associated with greater overall chatbot use and trust**, but it **did not lead to complacency or tolerance of AI errors**: everyone, regardless of literacy, found severe incidents unacceptable. The authors conclude that while AI literacy correlates with *general attitudes and usage* of AI (more literacy -> more trust and use), it **does not automatically translate to trust resilience** in the face of actual AI failures. They discuss that training users in AI literacy needs to include not just how to use AI, but also realistic expectations and coping strategies for when AI goes wrong, to achieve proper trust calibration. *(The study used established instruments: MAILS for literacy and a known trust scale; example items are given in-text as noted above, and full questionnaires can be obtained via the cited sources. This research was published open-access, with the PDF available online for detailed appendices.)*

## Grassini (2023) – AIAS-4: Attitudes Toward AI Scale Development
**Study Overview:** Grassini (2023) developed and validated the **Artificial Intelligence Attitude Scale (AIAS-4)** – a brief questionnaire to measure general attitudes and *confidence in* AI. While not a study of literacy per se, this instrument was created in response to the need for domain-general measures of **trust and outlook toward AI**, which often correlate with AI literacy. The AIAS-4 is relevant as an outcome measure: other studies (like Kox & Beretta 2024 above) have used attitude scales to see how AI literacy relates to acceptance of AI. Grassini’s work provides a standardized 4-item scale capturing positive versus wary attitudes toward AI. The scale went through two empirical studies for validation, including exploratory and confirmatory factor analysis on diverse samples.

-   **Scale Structure and Items:** The initial AIAS item pool had 5 candidate items covering key themes: AI’s benefit to one’s life, benefit to one’s career, intention to use AI, AI as a threat (negative item), and AI’s overall benefit to humanity. After testing, one item with weak correlations was dropped, resulting in a unidimensional **4-item scale**. The final **AIAS-4** asks respondents to rate agreement (on a 1–10 Likert scale) with statements: **(1)** *“I believe that AI will improve my life.”*, **(2)** *“I believe that AI will improve my work.”*, **(3)** *“I think I will use AI technology in the future.”*, and **(4)** *“I think AI technology is a threat to humans.”* (reverse-scored). (The fifth original item, *“AI is positive for humanity,”* was removed during refinement – the remaining items already cover that sentiment indirectly.) These items capture personal and societal attitudes: perceived usefulness (life/work improvement), **behavioral intention to use AI**, and a risk perception aspect (threat vs. benefit). The scale balances optimism with caution by including the “threat” item as a reverse indicator of trust.

-   **What It Captures:** The AIAS-4 is essentially a **general trust/attitude index** for AI. High scores indicate a person sees AI as beneficial and intends to embrace it, with low concern about existential threats. Low scores would indicate skepticism or fear about AI’s impact. Grassini validated that the scale has solid internal consistency (α ≈ 0.85) and one-factor structure. Convergent validity was shown via correlations with related measures like technology acceptance and AI anxiety scales. Notably, AIAS-4 distills attitude into a quick measure useful for large surveys or as a covariate. In the context of AI literacy research, AIAS-4 (or its predecessors) have been used as **outcome variables** or to segment users by attitude. For example, Sindermann et al.’s earlier 5-item scale (ATAI) had “AI will destroy humankind” as an extreme item; Grassini’s AIAS-4 opts for milder wording to capture general sentiment without focusing on science-fiction extremes. Overall, AIAS-4 provides a concise gauge of **trust and confidence in AI**. The **full item wording is available in the publication**, and researchers can use those items (subject to the article’s open access terms) in new studies. Grassini’s recommendation is that AIAS-4 can be **paired with AI literacy scales** to explore how knowledge relates to attitude – indeed, he cites that public perception of AI often mixes hopes (items 1–3) and fears (item 4). Having a standard scale for attitude allows for more consistent findings across studies of trust in AI.

-   **Findings and Usage:** While developing the AIAS-4, Grassini noted that public attitudes are generally positive but not without concern. In his samples (which included students and general public), agreement was high that *“AI will improve my life/work”*, and most people indicated they plan to use AI. However, a nontrivial minority also agreed with the reverse item about AI being a threat, illustrating the ambivalence in attitudes. The scale scores correlated positively with general trust in technology and with perceived usefulness of AI. This scale has since been used in subsequent research as a control or dependent measure. For instance, in teacher studies (Wijaya et al. 2024), an adaptation of “accepting AI vs. fearing AI” draws on similar item wordings. In summary, the AIAS-4 is a **validated 4-item questionnaire** that captures one’s overall **confidence in and acceptance of AI**, making it a handy tool for studies linking **AI literacy to attitudes**. *(For reference, the exact AIAS-4 items and a discussion of their theoretical grounding are provided in the Frontiers in Psychology article. Researchers are encouraged to see that source for the full scale and its psychometric properties.)*

---

## Additional Instruments for AI Literacy and Related Attitudes

Beyond the studies above, several **scales and questionnaires** have been developed recently to measure AI literacy and related user attitudes/behaviors. These instruments can complement the ones already mentioned (e.g., MAILS, AIAS-4) in assessing how people understand and engage with AI:

-   **Perceived AI Literacy Questionnaire (PAILQ-6):** A **6-item self-report scale** that assesses one’s own perception of their AI knowledge and skills. Grassini (2024) introduced PAILQ-6 as a concise measure for general populations (e.g., *“I understand how artificial intelligence works”*). It yields a single score of perceived AI literacy. Initial validation (NordiCHI 2024) shows good reliability and correlation with factual AI knowledge. The full item list is available via Grassini’s open access materials (Zenodo).

-   **Scale for Non-Experts’ AI Literacy (SNAIL):** A comprehensive **31-item scale** by Carl et al. (2023) to measure AI literacy among general users. SNAIL covers three factors: **Technical Understanding**, **Critical Appraisal**, and **Practical Application** of AI. Example items include technical knowledge statements and critical thinking prompts about AI outputs. SNAIL was developed through a Delphi method and is documented in Carl’s thesis (“Are They Lit?…” on ResearchGate). It provides a more granular literacy profile than a single score.

-   **Attitude Toward AI Scale (ATAI):** Sindermann et al. (2021) proposed a **5-item attitude scale** with two subscales: Acceptance vs. Fear of AI. It includes stark items (e.g., *“Artificial intelligence will destroy humankind.”* on the fear side) and positive ones. It’s an earlier brief scale that informed later instruments. Researchers interested in explicit fear-based attitudes might use ATAI or its items as a supplement to more neutral attitude measures.

-   **General Attitudes toward Artificial Intelligence Scale (GAIAS/GAISS):** Schepman and Rodway (2020) developed a **20-item scale** (GAIUS or GAISS) capturing positive and negative attitudes toward AI. It has a two-factor structure (e.g., items on societal benefit of AI vs. items on discomfort with AI). This longer scale can be useful for in-depth attitude assessment and has been used in human–robot interaction studies.

-   **Basic Literacy in Statistics (BLIS):** While not solely an “AI” literacy scale, the **BLIS test** (Ziegler & Garfield, 2018) of statistical literacy has been effectively used as a proxy for AI literacy (as in Marmolejo-Ramos et al. 2024). It’s a **37-item multiple-choice test** covering data interpretation, probability, and statistical thinking. Higher scores reflect better understanding of data and algorithms, which often correlates with more calibrated trust in AI. BLIS can be useful in studies focusing on the *knowledge* aspect of AI literacy.

-   **Propensity to Trust Automation Scale:** Originally by Merritt et al. (2013), this is a **6-item scale** measuring a person’s general tendency to trust machines and AI. For example: *“I usually trust a machine until it proves untrustworthy.”* It’s often included in studies to account for individual differences in trust bias (as a control or moderating variable). In AI contexts, it helps separate one’s baseline trustfulness from informed trust. The scale items are published in Merritt (2013) and have been reused in multiple human factors studies.

-   **Human-Computer Trust Scale (Jian et al., 2000):** A **12-item semantic differential scale** (e.g., rating an AI system on bipolar adjectives like reliable–unreliable, dependable–undependable). This was adapted in Kox & Beretta (2024) to specifically assess trust in AI chatbots. It provides a nuanced view of trust and has been widely adopted in automation trust research. The original items are available in Jian et al.’s publication (2000) and measure dimensions such as reliability, faith, and trust in system performance.

-   **Meta AI Literacy Scale (MAILS):** Mentioned earlier, MAILS by Baum et al. (2023) is a **34-item scale** with four core literacy facets and additional subskills. It’s one of the most comprehensive **domain-general AI literacy instruments** to date. MAILS not only assesses knowledge but also ethical understanding and application skills. Its items (published in the original article) can be useful for researchers who need to measure improvements in AI literacy after an intervention or to profile users’ strengths/weaknesses across different AI competencies.

-   **AI Literacy Concept Inventory (AI-CI):** Zhang, Perry & Lee (2024) created an **AI Literacy Concept Inventory** for middle school students. It’s a test-based instrument (multiple-choice) evaluating conceptual understanding of AI (e.g., what AI can/can’t do, basic principles of machine learning). While aimed at younger learners, its questions can inspire domain-general knowledge quizzes. It complements self-assessments by checking actual understanding.

-   **Measures of Automation Bias/Overreliance:** Although not standard “scales” (since automation bias is often behaviorally measured), some studies have formalized questionnaires to capture **overreliance tendencies**. For instance, Ji et al. (2023) developed items to assess how likely a person is to follow AI advice blindly (e.g., *“I often accept AI recommendations without double-checking”* – answered on a Likert scale). These kinds of items are sometimes included in trust questionnaires or post-task surveys to quantify **automation bias** inclination. They are less common than behavioral metrics but can be found in human factors research (Sheridan, 2019 discusses survey approaches to trust and reliance).

Each of these instruments provides a different lens on **AI literacy or its correlates (trust, attitude, reliance)**. Researchers often use a combination – for example, a literacy quiz (like BLIS or a concept inventory) together with an attitude scale (like AIAS-4 or trust scale) – to get a full picture of how knowledge and attitudes intertwine. The choice of instrument depends on the study’s focus: **knowledge vs. self-efficacy vs. ethical judgment** (for AI literacy dimensions), and **trust vs. fear vs. usage behavior** (for outcomes). All the scales listed above are published in peer-reviewed venues or open preprints, and many have their full items available in appendices or supplementary files, ensuring that future studies can readily adopt them to build on the current understanding of AI literacy and its impacts.

**Sources:** The descriptions above draw on the cited studies and scale development papers, which provide detailed item examples and psychometric properties for each instrument. Researchers are encouraged to consult the original publications for complete questionnaires and further validation evidence.






-------



https://chatgpt.com/c/68130513-0b38-8006-ae20-35f8fbf43bd7
https://aistudio.google.com/prompts/1epaIbD_rD7_ljoSoXueh-0V5YL0JTV1Q


# AI Literacy and Energy Literacy: Survey Instruments Review

## AI Literacy Scales

### Meta-Artificial Intelligence Literacy Scale (MAILS)
- **Constructs Measured:** Broad AI literacy competencies, including practical **skills in using AI**, conceptual **knowledge of AI**, **ethical understanding**, and **self-efficacy** in dealing with AI (Carolus23_MAILS.md) (Carolus23_MAILS.md). It assesses how well individuals can operate, apply, understand, create, and critically engage with AI.
- **Format:** 34 self-report ability statements rated on an 11-point scale (0 = “not at all” to 10 = “perfectly”) (Carolus23_MAILS.md). Items are phrased as **“I can…”** statements (e.g., *“I can operate AI applications”*, *“I can incorporate ethical considerations when deciding whether to use data provided by an AI”*) (Carolus23_MAILS.md) (Carolus23_MAILS.md).
- **Subscales:** Multiple sub-dimensions grouped into higher-order categories (Carolus23_MAILS.md) (Carolus23_MAILS.md). Key facets include **Use & Apply AI**, **Know & Understand AI**, **Detect AI**, **AI Ethics**, **Create AI**, **AI Problem-Solving**, **Learning (keeping up-to-date)**, **Persuasion Literacy** (resisting undue AI influence), and **Emotion Regulation** when using AI (Carolus23_MAILS.md) (Carolus23_MAILS.md).
- **Scoring:** Mean scores are calculated for each subscale by averaging their item ratings (Carolus23_MAILS.md) (Carolus23_MAILS.md). Higher scores indicate greater self-perceived competency in that aspect of AI literacy.
- **Notable Features:** Developed based on competency models, MAILS captures **cognitive, technical, and socio-emotional aspects** of AI literacy in one instrument (Carolus23_MAILS.md). It uniquely integrates **ethical considerations and self-regulation** (e.g. emotional control, resisting AI persuasion) into AI literacy (Carolus23_MAILS.md) (Carolus23_MAILS.md), going beyond factual knowledge to include attitudes and confidence in using AI.

### MAILS – Short Version
- **Constructs Measured:** A shortened 10-item measure of **overall AI literacy** covering key competencies: identifying AI, creating AI, staying up-to-date (learning), emotional regulation, ethical judgment, applying AI, problem-solving with AI, resisting AI influence, etc. (Koch24_MAILS_Short.md) (Koch24_MAILS_Short.md). It preserves the breadth of the full MAILS but with fewer items.
- **Format:** 10 self-assessment statements rated 0–10, same scaling as the full MAILS (Koch24_MAILS_Short.md). Each item represents a representative ability from a MAILS sub-domain (e.g., *“I can tell if I am dealing with an application based on AI”*, *“I can handle it when interactions with AI frustrate or frighten me”*) (Koch24_MAILS_Short.md) (Koch24_MAILS_Short.md).
- **Subscales:** Retains four composite sections: **AI Literacy** (comprised of items on detecting AI, applying AI, understanding AI, and AI ethics) (Koch24_MAILS_Short.md), **Create AI** (items on programming/designing AI) (Koch24_MAILS_Short.md), **AI Self-Efficacy** (learning and problem-solving with AI) (Koch24_MAILS_Short.md), and **AI Self-Competency** (persuasion literacy and emotion regulation) (Koch24_MAILS_Short.md).
- **Scoring:** Mean scores for each of the four areas are computed (e.g. average of two items for Create AI) (Koch24_MAILS_Short.md), and an overall literacy score can be obtained by combining all items. The short scale was validated to ensure it correlates well with the full scale (Koch24_MAILS_Short.md).
- **Notable Features:** Much **briefer administration** while still capturing multiple facets of AI literacy. Emphasizes core capabilities (e.g. recognizing AI, keeping up with AI advances) and personal agency in using AI (self-efficacy), making it practical for large surveys or pre/post assessments (Koch24_MAILS_Short.md).

### AI Literacy Test (Tully et al., 2023)
- **Constructs Measured:** **Objective knowledge of AI concepts and algorithms**, termed an AI Literacy Test. It gauges how well individuals understand facts and principles about AI (as opposed to self-perception). The test’s purpose is to assess factual AI knowledge and its relation to attitudes (like AI receptivity).
- **Format:** A set of **multiple-choice questions** (initially 25, refined to 17) covering key **AI competency areas**. Each question has one correct answer. For example, questions might address definitions (e.g., what “machine learning” means), capabilities of AI, or limitations, with plausible distractors. The items span different domains of AI knowledge, one question per identified competency.
- **Subscales:** The test primarily yields a single overall knowledge score. Exploratory factor analysis suggested a **3-factor structure** underlying the items – related to ability to **interact with AI products**, understanding **AI’s capabilities**, and understanding **AI’s limitations**. However, in practice the instrument is often treated unidimensionally (as a total score of AI knowledge).
- **Scoring:** **Objective scoring** – each multiple-choice item is scored as correct or incorrect, and scores are the total number (or percentage) correct. Higher scores indicate greater factual AI literacy. In studies, the sum of correct answers served as the AI literacy index.
- **Notable Features:** This instrument is **knowledge-based** rather than attitudinal, analogous to a quiz. It was validated by finding that lower objective AI knowledge correlates with higher unwarranted **AI receptivity** (people with low literacy were more “awed” by AI). It is scenario-like in that it often uses real-world examples in questions, but ultimately tests **conceptual understanding** rather than self-reported confidence.

### Rheu & Cho (2025) – AI Literacy and LLM Usage Survey
- **Constructs Measured:** Investigates the relationship between **AI literacy, usage of AI (LLMs), and critical evaluation behaviors**. In particular, it measures students’ self-reported **AI literacy level**, their **use of generative AI tools**, and their **fact-checking or critical thinking behaviors** when using AI. (Referred to as the “Trap of AI Literacy” study.)
- **Format:** Survey questionnaire with scenario-based self-report items. Participants (college students) report how frequently they use large language models (LLMs) or other AI tools for schoolwork, how they perceive their own AI understanding, and how often they **verify or fact-check AI-provided information**. Likert-type scales (e.g., frequency from “never” to “always” for behaviors) are used for these measures.
- **Subscales:** Likely includes distinct measures for **AI tool use frequency**, **perceived AI literacy** (confidence/knowledge in AI), and **fact-checking diligence** (tendency to double-check AI outputs). These could be analyzed separately to see how they interrelate (e.g., does high self-rated literacy associate with lower fact-checking).
- **Scoring:** Each construct is scored by averaging relevant items (e.g., mean frequency of AI use, mean agreement with literacy self-assessments). Higher scores on the fact-checking scale indicate more careful verification behavior. The study examines correlations/“paradoxical” patterns between these scores.
- **Notable Features:** The survey is framed around a potential paradox: those with higher AI familiarity might develop **overconfidence** leading to less critical checking (“trap of AI literacy”). It thus touches on **overreliance** on AI vs. healthy skepticism. While not a formal published scale, it provides insight into **automation bias**: measuring whether knowledge reduces or increases complacency. *(Citation: Rheu & Cho (2025) – as referenced in academic discussions).*

### AI Attitude Scale (AIAS-4)
- **Constructs Measured:** **General attitude toward AI** – specifically optimistic beliefs about AI’s personal and societal benefits (Grassini23_AI_Scale.md). It captures one’s overall positive or negative stance on AI’s impact on life and humanity.
- **Format:** 4 succinct statements assessed on a **10-point Likert agreement scale** (1 = “Not at all” to 10 = “Completely agree”) (Grassini23_AI_Scale.md). Items include beliefs such as *“AI will improve my life”*, *“AI will improve my work”*, *“I will use AI in the future”*, *“AI is positive for humanity”* (Grassini23_AI_Scale.md). All items are phrased positively.
- **Subscales:** None – it is unidimensional. The four items collectively reflect a single **pro-attitude toward AI** factor (Grassini23_AI_Scale.md). (It was validated as a one-factor scale of positive AI attitudes.)
- **Scoring:** Typically by computing the **average of the four item ratings** (Grassini23_AI_Scale.md) (or sum, since all are on the same scale). A higher AIAS-4 score indicates a more favorable attitude toward AI.
- **Notable Features:** **Brevity and focus** – with only four items, it provides a quick gauge of optimism about AI. It was developed and validated in 2023 to be **cross-culturally applicable** (Grassini23_AI_Scale.md). Its focus is on expected **benefits and usage** of AI, not addressing fears or ethical concerns (thus complementing other scales that measure AI anxiety or skepticism).

### Attitude Towards AI (ATAI) Scale
- **Constructs Measured:** **Attitudes toward AI with both positive and negative dimensions**, specifically feelings of **trust/acceptance** vs. **fear/distrust** of AI. It probes general expectations of AI’s impact (utopian or apocalyptic) (Sindermann21_AI.md).
- **Format:** 5 statements rated on an 11-point Likert scale (0 = “strongly disagree” to 10 = “strongly agree”) (Sindermann21_AI.md). The items are phrased in a mix of positive and negative directions: e.g., *“I trust artificial intelligence”*, *“AI will benefit humankind”* versus *“I fear artificial intelligence”*, *“AI will destroy humankind”*, *“AI will cause many job losses.”* (Sindermann21_AI.md).
- **Subscales:** Yields two subcomponents identified as **Acceptance** and **Fear** of AI. In validation, items split into a **Trust/Benefit factor** (positive attitude: trust and benefit items) and a **Fear factor** (negative attitude: fear, existential risk, job loss items). Each subscale has 2–3 items (e.g., trust and benefit comprise “Acceptance”).
- **Scoring:** Typically computes separate mean scores for the **Acceptance** subscale and the **Fear** subscale. Negative items are reverse-scored when computing an overall attitude, or the two dimensions are reported separately. The internal consistency (reliability) for each subscale is modest due to few items (α ~0.65) but acceptable given brevity.
- **Notable Features:** This scale captures the **ambivalence in public opinion** about AI. It is multilingual (developed in German, Chinese, English) and demonstrated cross-cultural measurement invariance for its two-factor structure. Notably, it balances **techno-optimism and techno-pessimism** in one instrument. It has been used to link attitude profiles with demographic and personality factors.

### Trust Perception Scale for AI (Jian et al. TPA)
- **Constructs Measured:** **Trust in a specific AI system** – i.e. the user’s immediate trust perception of an AI agent or tool. It measures both **distrust (negative expectations)** and **trust (positive beliefs)** regarding the AI’s behavior (TPA.md) (TPA.md). Originally developed for automation, it has been adapted to AI contexts as the TPA (Trust Perception in Automation/AI) scale.
- **Format:** 12 adjective or statement items rated on a 7-point semantic differential/Likert (1 = Not at all, 4 = Neutral, 7 = Extremely) (TPA.md). Items include negative trust indicators (*“The AI is deceptive,” “I am suspicious of the AI’s intent”*, *“The AI’s actions will have a harmful outcome”*) and positive trust indicators (*“The AI is dependable,” “The AI is reliable,” “I can trust the AI”*) (TPA.md) (TPA.md). One item gauges **familiarity** with the AI (TPA.md).
- **Subscales:** Often treated as one scale after reversing negative items to yield an overall trust score. Some analyses separate it into **Distrust vs. Trust** factors (the original development found two factors that are highly correlated) – e.g., the first 5 items often load on a “distrust” factor and items 6–11 on a “trust” factor, with the familiarity item considered auxiliary (TPA.md) (TPA.md).
- **Scoring:** If a single trust score is desired, negative-worded items are reverse-scored and then all trust items are averaged (TPA.md) (TPA.md). Alternatively, separate mean scores for **Trust** and **Distrust** can be reported. Higher overall score means greater trust. The **familiarity** item may be analyzed separately or used to check its influence on trust.
- **Notable Features:** This is a **standardized trust measure** widely used in human–automation interaction studies. Adapted for AI, it allows comparisons of trust levels across different systems or experimental conditions (TPA.md) (TPA.md). It covers a spectrum from outright suspicion to confidence. Notably, it has been recently validated specifically for AI contexts, confirming it effectively measures user trust in AI systems (TPA.md).

### AI Use and Cognitive Offloading Survey (Gerlich, 2025)
- **Constructs Measured:** **Frequency of AI tool use and reliance**, and **cognitive offloading behaviors** (delegating tasks to technology), along with aspects of **critical thinking** in the context of AI use. It assesses how often people use AI assistants, how much they trust and depend on them, and whether they still apply critical evaluation when using AI (Gerlich25_Survey.md) (Gerlich25_Survey.md).
- **Format:** A multi-section questionnaire. Key sections:
  - **AI Tool Usage:** Likert items on how often one uses AI tools and how much one relies on or trusts AI recommendations (e.g., *“I trust the recommendations provided by AI tools”*) (Gerlich25_Survey.md). Responses on a 6-point scale (Never to Always / Strongly Disagree to Strongly Agree).
  - **Cognitive Offloading:** Items about using technology to offload memory or problem-solving (e.g., frequency of using search engines or devices instead of recalling information) (Gerlich25_Survey.md).
  - **Critical Thinking:** Items about information vetting and skepticism (e.g., *“I often cross-check information provided by AI tools with other sources”* and confidence in spotting misinformation) (Gerlich25_Survey.md) (Gerlich25_Survey.md).
- **Subscales:** The survey can be grouped into indices: **AI Usage & Trust** (items 6–10) capturing usage frequency, perceived time-saving, trust, and verification habits (Gerlich25_Survey.md) (Gerlich25_Survey.md); **Digital Offloading** (items 11–15) about reliance on devices and internet for information (Gerlich25_Survey.md); and **Critical Thinking Self-Assessment** (items 16–23) about source evaluation and reflection (Gerlich25_Survey.md). Each set serves as a subscale.
- **Scoring:** Mean or sum scores for each subscale (with some items reverse-scored if higher means less desirable behavior). For instance, a high score on AI Trust/Use might indicate frequent use and high trust, while a high score on Critical Thinking indicates frequent fact-checking and skepticism. These can be correlated to see if heavy AI users also maintain critical habits.
- **Notable Features:** This instrument blends behavior and attitude: it measures **actual practices (usage frequency)** and **mindsets (trust vs. verification)**. Notably, it includes an item explicitly about **cross-checking AI-provided information (Gerlich25_Survey.md)**, directly addressing **automation bias/overreliance**. It situates AI use within broader cognitive habits (offloading and critical reasoning), reflecting concerns about AI’s impact on critical thinking (Gerlich25_Survey.md).

## Energy Literacy Scales

### DeWaters & Powers (2011) Energy Literacy Questionnaire
- **Constructs Measured:** **Energy literacy** in secondary students, conceived as a combination of **knowledge**, **attitudes**, and **behavior** regarding energy (DeWaters_Powers2011_Supp.md). Specifically: understanding of energy concepts, concern for energy issues, and engagement in energy-saving behaviors.
- **Format:** Three parts:
  - **Cognitive (Knowledge) Subscale:** ~30–38 **multiple-choice questions** (differences for middle school vs high school) on energy topics (DeWaters_Powers2011_Supp.md). Topics include energy sources, consumption, conservation strategies, and basic principles (e.g., units like kWh, energy conversion in appliances) (DeWaters_Powers2011_Supp.md) (DeWaters_Powers2011_Supp.md). Each item has one correct answer (factual).
  - **Affective (Attitude) Subscale:** 17 **Likert-scale statements** about attitudes, values, and self-efficacy related to energy (DeWaters_Powers2011_Supp.md). Students rate agreement (Strongly Disagree to Strongly Agree). Items cover importance of energy education, importance of conserving, perceived efficacy (e.g., *“I believe I can contribute to solving energy problems”*) and beliefs about technology or policy (some are reverse-worded to capture dismissive attitudes) (DeWaters_Powers2011_Supp.md) (DeWaters_Powers2011_Supp.md).
  - **Behavioral Subscale:** 10 **Likert-frequency items** on personal energy-saving behaviors (DeWaters_Powers2011_Supp.md). Students report how often they perform actions like turning off lights, saving water, using alternative transportation, etc. (Never to Almost Always) (DeWaters_Powers2011_Supp.md).
- **Subscales:** Explicitly three subscales – **Cognitive**, **Affective**, and **Behavior** – which can be analyzed separately (DeWaters_Powers2011_Supp.md). The affective subscale itself includes a few **self-efficacy items** identified within it (belief in one’s own action making a difference) (DeWaters_Powers2011_Supp.md) (DeWaters_Powers2011_Supp.md).
- **Scoring:** For knowledge, typically the percentage of correct answers or a knowledge score is calculated. For attitudes and behaviors, average Likert scores are computed (with reverse-scoring for negatively phrased items) (DeWaters_Powers2011_Supp.md) (DeWaters_Powers2011_Supp.md). An overall “energy literacy” score might be reported by combining domains, but often the domains are kept separate to diagnose specific strengths or weaknesses (DeWaters_Powers2011_Supp.md).
- **Notable Features:** Comprehensive in scope – it doesn’t treat energy literacy as just knowledge, but also what students feel and do. The **inclusion of self-efficacy** statements is notable, recognizing confidence in influencing energy issues as part of literacy (DeWaters_Powers2011_Supp.md) (DeWaters_Powers2011_Supp.md). This instrument was among the first to systematically measure youth energy literacy and was used to identify gaps (e.g. strong pro-conservation attitudes but weaker factual knowledge in students) (DeWaters_Powers2011_Supp.md).

### DeWaters et al. (2013) Youth Energy Literacy Survey
- **Constructs Measured:** Largely the same domains as the 2011 survey – **energy-related knowledge, attitudes, and behaviors** – but refined for broad use with middle and high school students. The 2013 work focused on **design and validation** of the questionnaire (DeWaters2013_Energy.md).
- **Format:**
  - **Knowledge Section:** Multiple-choice questions (on the order of 40 items) covering fundamental energy concepts (definition of energy, sources, renewables vs. non-renewables, consumption facts) (DeWaters2013_Energy.md) (DeWaters2013_Energy.md). Many items were similar to the 2011 version (e.g., *“Energy is defined as… the ability to do work.”* (DeWaters2013_Energy.md)) with adjustments for age level (some labeled for HS or MS only).
  - **Attitude Section:** Likert-scale statements (5-point scale from Strongly Disagree to Strongly Agree) about the importance of energy issues, willingness to save energy, and perceived responsibility (DeWaters2013_Energy.md). These included items on curriculum importance, willingness to act (*“I would do more to save energy if I knew how”*), and belief statements about technology and environment (mirroring 2011 attitude items).
  - **Behavior Section:** Likert-frequency items on personal and family energy-saving actions (similar to 2011, e.g., conserving water, turning off devices) (DeWaters2013_Energy.md).
- **Subscales:** Like the 2011 instrument, divided into **Cognitive**, **Attitudinal**, and **Behavioral** components. The 2013 design process confirmed these components and adjusted question wording for clarity and age appropriateness.
- **Scoring:** Each section is scored separately: knowledge as number correct, and attitudes/behaviors as mean agreement or frequency. In analysis, researchers looked at overall scores and also how knowledge correlates with attitudes/behaviors (DeWaters2013_Energy.md).
- **Notable Features:** This version underwent thorough psychometric testing (item difficulty, discrimination, factor analysis) to ensure it is a reliable **evaluation tool for energy education programs** (DeWaters2013_Energy.md). It explicitly distinguishes between middle school and high school versions for certain items (DeWaters2013_Energy.md). The instrument assumes that improving knowledge and attitudes will ultimately influence behavior, aligning with the framework that energy literacy is multi-dimensional.

### Attari et al. (2010) Energy Perceptions Survey
- **Constructs Measured:** **Public perceptions and knowledge of household energy consumption and savings** (Attari2010_Survey.md). It evaluates how accurately people understand the energy use of various activities and appliances, and what conservation actions they believe are most effective.
- **Format:** A mixed-format questionnaire:
  - **Open-ended question:** Participants first list *“the most effective thing you could do to conserve energy in your life”* (qualitative response) (Attari2010_Survey.md). This gauges beliefs about top conservation actions.
  - **Quantitative estimation tasks:** Respondents estimate energy usage and savings. For example:
    - **Energy Use Breakdown:** Estimate what percentage of an average household’s energy is used by 1) household operations, 2) transportation, 3) food production (Attari2010_Survey.md). (This tests understanding of major consumption sectors.)
    - **Appliance Hourly Usage:** Using a reference point that a 100-Watt incandescent bulb uses 100 units in one hour, estimate how many units various devices use in one hour (Attari2010_Survey.md). A table is provided with items like a CFL bulb, a laptop, an AC unit, etc., where respondents fill in numbers (Attari2010_Survey.md) (Attari2010_Survey.md).
    - **Energy Savings:** Similarly, estimate energy saved by certain actions (e.g., replacing bulbs, adjusting thermostats) relative to that 100-Watt bulb baseline (Attari2010_Survey.md) (Attari2010_Survey.md).
- **Subscales:** Not organized into subscales per se, but covers **different knowledge categories**: qualitative beliefs about conservation, quantitative understanding of consumption, and quantitative understanding of savings. Each can be analyzed (e.g., accuracy of appliance usage estimates, ranking of actions).
- **Scoring:** There are correct or benchmark answers (derived from actual energy data) for the quantitative questions, so accuracy can be scored by error from the true value. Attari et al. analyzed **median estimates and error patterns** – for instance, finding consistent underestimation or overestimation biases. The open-ended responses were coded to see which actions are most frequently believed effective.
- **Notable Features:** This survey famously revealed **systematic misperceptions** – e.g., people tend to **underestimate high-energy uses and overestimate low-energy ones**. It highlighted “**energy use perception bias**,” such as people thinking turning off lights saves more energy than it actually does, while underestimating the impact of driving less. The instrument’s use of an intuitive baseline (100-W bulb) and requiring numeric guesses was novel in exposing gaps between perceived and actual energy use (Attari2010_Survey.md). It’s not a traditional Likert scale; rather, it’s a knowledge/perception test via scenarios.

### Marghetis et al. (2019) Energy Use Misperception Study
- **Constructs Measured:** **Misperceptions of home energy use and the effectiveness of simple interventions**. This study assessed how people estimate energy consumption of household activities (similar to Attari 2010) and tested methods to improve those estimates. Key constructs include baseline **estimation accuracy** and changes in understanding after interventions.
- **Format:** A randomized online experiment with survey components:
  - Participants first make **numerical estimates** of energy usage for various household appliances/usages (e.g., how many watt-hours for using an appliance, or relative energy use ranking). This is akin to the Attari tasks, quantifying mental models of energy use.
  - Then, participants receive one of two simple educational interventions: either **numerical information about extremes** (providing the energy use of a very low-energy vs. high-energy appliance to anchor the scale) or an **explicit heuristic** (a rule of thumb addressing a common misbelief).
  - After intervention, participants re-evaluate or answer additional questions to see if their **judgments improved**.
- **Subscales:** Not a scale with sub-components, but rather measures **two outcomes**: improvement in **numerical estimation accuracy** and improvement in **qualitative understanding of relative usage**. These correspond to the two interventions: one aimed at using the given unit (watt-hour) scale correctly, the other at internalizing a better heuristic for comparing usage.
- **Scoring:** Estimation errors (difference between estimated and actual energy use) are calculated pre- and post-intervention. The effectiveness is measured by reduction in error or better ranking of high vs low energy uses. Also, specific judgment about energy-saving behaviors (e.g., which actions save more energy) are scored for correctness to see if the heuristic helped.
- **Notable Features:** While not a deployable “survey scale,” this study’s instrument underscores **cognitive biases** in energy judgments. It demonstrated that providing a simple heuristic (e.g., highlighting a high-consumption appliance to correct underestimation) significantly improved people’s understanding of what actions matter most for conservation. Marghetis et al. built on Attari’s findings, showing that targeted information can shift perceptions closer to reality. In summary, it measured energy literacy in a **dynamic way (before vs. after teaching)** to explore how to fix misconceptions.

### He et al. (2024) Environmental Decision-Making Survey
- **Constructs Measured:** **Environmental attitudes, personal traits, and experiences affecting decision-making**, particularly around energy-related externalities. It covers beliefs about **climate change**, general psychological traits (competitiveness, risk aversion), and personal involvement in household energy management.
- **Format:** Part of a three-phase experiment (Phases 1 & 2 were decision tasks; Phase 3 was a survey) (He_Blasch24_energy_survey.md) (He_Blasch24_energy_survey.md). The **Phase 3 survey** included:
  - **Climate Change Beliefs & Concern:** Questions on conviction that climate change is human-caused and level of concern about its impacts on society and personally (He_Blasch24_energy_survey.md). Rated on a 7-point scale from not at all convinced/concerned to fully convinced/strongly concerned.
  - **Environmental Threat Perceptions:** A checklist of concern about specific threats (floods, heat waves, etc.) affecting the respondent or their community (He_Blasch24_energy_survey.md) (also 7-point concern scale).
  - **Competitiveness Scale:** A brief personality scale about enjoyment of competition (several statements like *“I enjoy testing myself in competitive situations”*, with some reverse items) on a 7-point agreement scale (He_Blasch24_energy_survey.md).
  - **Risk/Time Preferences:** Single-item measures of willingness to take risks and to delay gratification (0–10 scale) (He_Blasch24_energy_survey.md).
  - **Loss Aversion Tendencies:** Multiple statements about attachment to possessions and dislike of losses (7-point agree/disagree, with reverse-coded items) (He_Blasch24_energy_survey.md).
  - **Experience with Energy Bills (Social Comparison):** Yes/No questions about whether the person handles their **household energy bills** and if they have ever seen **social comparison feedback** (like energy use comparisons on a bill) (He_Blasch24_energy_survey.md) (He_Blasch24_energy_survey.md).
- **Subscales:** The survey combined several constructs. Relevant to energy literacy is the **climate belief/concern scale** (assessing awareness and worry about energy-related environmental issues) and the **experience with energy bills** item (a proxy for engagement in energy usage knowledge) (He_Blasch24_energy_survey.md). Competitiveness and loss aversion are separate subscales included for research on decision-making behavior.
- **Scoring:** Each set of items is scored individually: e.g., a mean score for **competitive orientation**, a composite for **loss aversion**, etc., and single item scores for risk tolerance. Climate concern can be averaged across the multiple items (societal and personal concern). The energy bill question is categorical (yes/no) and was likely used to segment participants or as a control variable (He_Blasch24_energy_survey.md).
- **Notable Features:** This instrument is not an “energy literacy” test per se, but it embeds energy-related questions in a broader context of economic decision-making. It uniquely includes whether participants have **practical experience with energy consumption feedback** (like comparing energy use to others) (He_Blasch24_energy_survey.md). The presence of climate change attitude questions ties into energy literacy by measuring the respondent’s **perceived importance of energy issues**. In summary, it illustrates how **psychological and attitudinal factors** (beyond pure knowledge) are surveyed in relation to energy decisions.

## Comparison and Discussion

Both AI literacy and energy literacy domains recognize that **literacy is multi-dimensional**, involving **knowledge, attitudes, and behaviors**. In energy literacy instruments, this is very explicit: for example, the DeWaters surveys have separate cognitive, affective, and behavioral sections (DeWaters_Powers2011_Supp.md) (DeWaters_Powers2011_Supp.md). AI literacy efforts likewise span factual knowledge (e.g. Tully’s objective test) and self-efficacy or ethical attitude components (e.g. MAILS includes ethics and emotional regulation) (Carolus23_MAILS.md) (Carolus23_MAILS.md). In both fields, **self-efficacy** is a notable construct – energy surveys ask if one feels their actions matter (DeWaters_Powers2011_Supp.md), and AI scales like MAILS gauge confidence in using AI and staying up-to-date (Carolus23_MAILS.md). This shows a similarity in emphasizing a person’s confidence to engage with the subject matter (be it saving energy or using AI).

However, there are clear **differences in emphasis** between the domains. AI literacy scales often focus on **trust and acceptance of a new technology** – for instance, specialized AI scales measure trust vs. fear of AI (Sindermann21_AI.md), or overreliance (Gerlich’s survey checks if users blindly trust AI or cross-check information (Gerlich25_Survey.md)). Energy literacy instruments, on the other hand, put more weight on **environmental concern and conservation values** (e.g., attitudes about the importance of saving energy and support for renewable policies) rather than trust in technology (DeWaters_Powers2011_Supp.md) (DeWaters_Powers2011_Supp.md). The outcome behaviors differ: energy literacy is concerned with **personal conservation actions** (turning off lights, reducing usage (DeWaters_Powers2011_Supp.md)), whereas AI literacy-related behavior is about **technology usage patterns** (how often one uses or relies on AI tools (Gerlich25_Survey.md)).

In terms of **construct dimensionality**, energy literacy is sometimes treated holistically (overall literacy score) but often separated into knowledge/attitude/behavior components for diagnostic purposes. AI literacy is still emerging as a construct – some efforts produce broad multi-factor scales like MAILS with a rich competency structure (Carolus23_MAILS.md), while others use a single factor (AIAS-4’s unidimensional attitude (Grassini23_AI_Scale.md) or Tully’s one-score knowledge test). Both domains see a mix of **broad multi-factor instruments** and **targeted short scales** depending on the use case. Notably, AI literacy measurement is incorporating dimensions like **ethics and emotional regulation**, which have less parallel in energy literacy (energy surveys do include ethical stances on environment, but not personal emotion-regulation). This reflects the different nature of the content: AI interactions might induce trust or fear responses, whereas energy decisions invoke ethical/utilitarian judgments.

Regarding **psychometric formats**, energy surveys historically use more **objective and scenario-based questions** (e.g., numeric estimation tasks in Attari’s survey (Attari2010_Survey.md)) to test knowledge, along with traditional Likerts for attitudes. AI surveys, until recently, relied predominantly on **self-report Likert scales** for both knowledge self-assessment and attitudes (e.g., people rating how much they think they know or how they feel about AI) (Sindermann21_AI.md). This is changing as AI literacy researchers adopt knowledge tests similar to those in energy domain (e.g., multiple-choice quizzes like the AI Literacy Test). Another difference is that **overreliance and bias** in AI is sometimes measured through interactive scenarios (for example, observing if users follow a flawed AI recommendation), whereas in energy literacy the “bias” is in mental estimates captured through survey questions (no direct device interaction, but hypothetical scenarios like Attari’s). Both domains use scenario questions, but energy scenarios are about factual reality (estimating consumption), while AI scenarios often concern human–AI interaction outcomes (trusting AI in a scenario).

Finally, the two domains differ in their ultimate emphasis: **behavior vs. cognition**. Energy literacy instruments explicitly aim to link knowledge and attitude to behavior change (conservation actions), so behavioral self-report is a substantial part (DeWaters_Powers2011_Supp.md). AI literacy measures are often aiming to predict **usage behavior or acceptance** of AI (for instance, whether literacy level affects willingness to use AI or susceptibility to AI’s errors). Thus, AI instruments like ATAI or Gerlich’s survey are used to understand or predict user decisions around AI (adoption, trust) rather than direct physical behaviors. In summary, both domains share a commitment to multidimensional assessment, but **energy literacy tools lean toward assessing factual understanding and pro-environment behavior, whereas AI literacy tools place somewhat more emphasis on attitudes like trust and on one’s capacity to use technology wisely**. Each domain’s instruments evolved in context: energy literacy in the context of public understanding of science and conservation, and AI literacy in the context of emerging technology adoption and responsible use. The result is complementary approaches that nonetheless echo the core idea that literacy is not just knowledge, but a combination of knowing, feeling, and doing. (Sindermann21_AI.md)
```