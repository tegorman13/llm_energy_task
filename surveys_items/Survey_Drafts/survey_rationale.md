


# https://notebooklm.google.com/notebook/a04e8d62-e334-44e2-86ee-f1bf4b02a21f?pli=1

**I. Energy Literacy and the Cognitive Science of Energy Decision-Making**

Energy literacy is widely conceptualized as a multidimensional construct encompassing cognitive knowledge, affective attitudes, and behavioral intentions or actions related to energy (DeWaters & Powers, 2011; DeWaters & Powers, 2013; Santillán & Cedano, 2023; Andolfi et al., 2024). Cognitive energy literacy involves understanding fundamental concepts such as energy generation, transportation, storage, distribution, and use, as well as awareness of environmental and social impacts. Affective components include attitudes, values, and concerns regarding energy issues, while behavioral dimensions relate to engaging in energy-saving practices or efficiency investments. Research consistently demonstrates that while cognitive knowledge is a necessary foundation for informed choices, it is often insufficient alone to drive desired behaviors. Affective factors like values and attitudes, self-efficacy, and normative influences frequently play a more significant role in translating literacy into action. Measuring energy literacy often involves surveys assessing knowledge via multiple-choice questions, attitudes via Likert scales, and self-reported behaviors. Instruments like the Energy Literacy Questionnaire (ELQ) developed by DeWaters and Powers (2011, 2013) are foundational in this area, providing a comprehensive framework and validated measures.

Decision-making in the energy domain is often characterized by systematic cognitive biases and the use of simplifying heuristics rather than purely rational calculation. Studies by Attari et al. (2010) and Marghetis et al. (2019) have empirically demonstrated that individuals exhibit significant misperceptions regarding the energy consumption of common appliances and the effectiveness of various conservation actions. These biases include a tendency to overestimate the energy savings from low-impact curtailment activities (e.g., turning off lights) and underestimate the impact of high-consumption activities or efficiency investments (e.g., heating/cooling, upgrading appliances). These misperceptions are often rooted in cognitive heuristics, such as relying on the perceived *effort* of an action rather than its actual *energy impact*. Numeracy, the ability to understand and use quantitative information, has been shown to correlate with more accurate energy perceptions. Individuals with higher numeracy skills may be better equipped to evaluate numerical information related to energy use or savings. Measurement approaches in this area include estimation tasks (e.g., relative to a 100W bulb), ranking tasks, open-ended questions about conservation strategies, and behavioral choice tasks. Numeracy is typically assessed using validated quantitative reasoning items.

The survey's inclusion of adapted ELQ items addresses the multidimensional nature of energy literacy, allowing for an examination of how cognitive knowledge, attitudes, and behaviors relate to perceptions of LLMs for energy tasks (RQ2, H2b). The inclusion of numeracy items aligns with research demonstrating its role in quantitative understanding and energy perception (RQ2, H2a). While the exclusion of behavioral estimation tasks in the *current survey format* limits the ability to measure individual energy perception biases directly *within* this study, the design of custom items probing perceived LLM accuracy for tasks where these biases are known to occur in humans allows for a comparison of perceived LLM patterns to documented human biases (H1), drawing inspiration from the methodologies of Attari et al. (2010) and Marghetis et al. (2019).

**II. Human-AI Interaction, Trust, and Reliance**

Research on trust in automated systems and artificial intelligence is a growing, though in some areas still immature and not fully integrated, field. Trust is a central construct in human-AI interaction, influencing user adoption, reliance, and satisfaction. Trust is often defined in terms of perceived reliability, competence, and predictability of the automated system. However, recent work highlights the importance of distinguishing between trust and distrust, proposing that they may function as separate constructs rather than opposite ends of a single continuum. Measuring both allows for a more nuanced understanding of user perceptions. Furthermore, trust can have cognitive components (based on beliefs about competence and reliability) and affective components (based on feelings like comfort or empathy), with the latter being particularly relevant for human-like systems such as Large Language Models (LLMs). Established scales like the Trust Perception Scale - AI (TPA), when used with a validated two-factor structure distinguishing trust and distrust, and semantic differential scales capturing cognitive and affective dimensions, provide robust options for measuring general AI trust.

In the context of LLMs, users form mental models about how these systems work, their capabilities, and their limitations. These mental models, which may be incomplete or inaccurate, influence user interaction strategies and expectations. Crucially, LLMs are known to sometimes generate factually incorrect information or "hallucinations," posing risks for domains requiring accuracy. Understanding user perceptions of LLM accuracy and reliability is therefore paramount. Task-specific trust, referring to the degree of confidence in an AI system for a particular function or task, is also critical, as trust may vary depending on the domain and the perceived stakes. Measures of trust and perceived accuracy/utility are key predictors of users' willingness to use or rely on AI systems. However, the relationship between literacy, trust, and reliance is complex. Higher literacy does not always prevent over-reliance, a phenomenon sometimes referred to as the "trap of AI literacy," potentially due to factors like increased confidence or automation bias (a tendency to favor AI output over human judgment). Calibrated trust, where reliance aligns appropriately with the system's actual capabilities and limitations, is a desirable outcome of effective human-AI interaction.

The survey's inclusion of measures for general AI trust and distrust (using TPA-Revised facets) allows for an investigation of the mediating or moderating role of dispositional trust in the relationship between task-specific perceptions and usage intentions (RQ4, H4). The development of custom items for perceived LLM accuracy, utility, and task-specific trust for energy management directly addresses the need to measure these novel, context-specific constructs for which no standard scales exist (RQ1, RQ3). By linking these perceptions and trust measures to self-reported willingness and frequency of LLM use for energy tasks, the survey directly tests hypotheses about the drivers of potential AI adoption in this domain (RQ3, H3).

**III. AI Literacy and LLM Mental Models**

AI literacy, the ability to understand, use, evaluate, and communicate about AI technologies, is a rapidly evolving concept with ongoing efforts to refine definitions and establish comprehensive frameworks. Similar to energy literacy, AI literacy is viewed as a multidimensional construct, often encompassing cognitive (understanding AI concepts), operational (using AI tools), critical (evaluating AI outputs and implications), and ethical dimensions. Critical evaluation is consistently highlighted as a crucial facet, especially concerning generative AI, due to the potential for biased or inaccurate outputs. Developing robust and validated instruments to measure AI literacy is an active area of research. While self-report scales capturing perceived knowledge or competence are common (e.g., MAILS), there is a recognized need for more objective measures assessing actual understanding and skills. Frameworks for AI literacy often draw on established educational taxonomies like Bloom's Taxonomy. Understanding AI limitations, such as the propensity for "hallucinations" in LLMs, is a key component of AI literacy relevant to critical evaluation and calibrated trust.

AI literacy is hypothesized to influence users' mental models of AI systems, leading to more accurate expectations and a greater capacity for critical evaluation. A higher level of AI literacy, particularly concerning the capabilities and limitations of LLMs, is expected to correlate with more cautious or nuanced beliefs about the system's performance, especially in domains requiring factual accuracy like energy management (H2c). Measures such as the MAILS-Short scale capture self-perceived AI literacy across various facets, providing a practical tool for survey research.

The survey's inclusion of the MAILS-Short scale allows for the assessment of self-perceived AI literacy as an individual difference variable predicting beliefs about LLM capabilities for energy tasks (RQ2, H2c). This aligns with the theoretical expectation that a better understanding of AI influences perceptions of its competence. However, the reliance on a self-report measure necessitates acknowledging its limitations regarding potential biases compared to objective assessments.

**IV. Construct Validity and Measurement Approach in the Survey**

The survey instrument endeavors to achieve construct validity by operationalizing the key theoretical constructs – energy literacy, numeracy, AI literacy, perceived LLM capability, trust, and usage intention – using methods justified by the literature and the specific research questions.

Established and validated instruments serve as the foundation for several constructs:
*   **Energy Literacy:** Adapted items from the ELQ leverage a comprehensive, multidimensional framework supported by extensive development and validation processes involving literature reviews, expert panels, pilot testing, and psychometric evaluation. Selecting items representing cognitive, affective, and behavioral dimensions aligns with the current understanding of energy literacy.
*   **Numeracy:** The inclusion of items adapted from standard numeracy assessments used in prior energy perception research provides a measure of quantitative reasoning with established relevance in this domain.
*   **AI Literacy:** The MAILS-Short form is a recently developed self-report scale with documented validation efforts, offering a concise measure covering multiple key facets of perceived AI literacy.
*   **General Trust in AI:** The use of facets from the TPA-Revised scale aligns with recent psychometric evidence supporting a two-factor structure distinguishing trust and distrust as distinct constructs, offering a more nuanced measure than single-factor approaches.

For constructs where established, context-specific measures do not exist, such as perceived LLM accuracy/utility for energy tasks and task-specific trust/usage, custom items were developed. The design of these custom items is informed by the structure and content of existing measures and theoretical frameworks. For instance, the items assessing perceived LLM accuracy for energy tasks are structured to probe judgments about activities where human biases are known to occur, drawing conceptually from the estimation tasks used by Attari et al. (2010) and Marghetis et al. (2019). Similarly, items for task-specific trust and usage are framed explicitly within the energy management context to ensure relevance. While custom measure development requires subsequent psychometric validation, this approach is necessary to address the novel intersection of AI and energy decisions.

**V. Critique and Suggestions for Improvement**

Based on the review of the literature and the survey design, several areas present opportunities for refinement in the research questions, hypotheses, or theoretical grounding for future work:

1.  **Measuring Human Energy Perception Biases (RQ1/H1):** While the custom items probe perceived LLM accuracy for tasks susceptible to human bias, the exclusion of behavioral energy estimation tasks (e.g., relative estimation from Attari et al., 2010) from the *current survey format* means human biases are not directly measured within the same sample. H1 is currently tested by comparing perceived LLM accuracy patterns to *documented* human biases from prior literature. Including these tasks in future studies would enable a stronger within-subjects analysis, directly correlating individual human biases with their perceptions of LLM accuracy for similar tasks.
2.  **Objective vs. Subjective AI Literacy (RQ2/H2c):** The reliance primarily on a self-report measure (MAILS-Short) for AI literacy is a practical choice for a survey but may not fully capture objective understanding or critical evaluation skills. The literature highlights the distinction and potential divergence between perceived and objective literacy. Future research could benefit from incorporating objective AI literacy assessment items, particularly focusing on understanding LLM mechanisms (e.g., statistical models vs. knowledge bases), sources of potential errors (e.g., hallucinations), and appropriate use cases, as suggested by frameworks aiming for more robust AI literacy assessment. Measuring prompt literacy, relevant for effective LLM interaction, could also be a valuable addition.
3.  **Operationalizing Reliance (RQ3/H3):** While self-reported willingness and frequency are standard measures of behavioral intention and past behavior, incorporating behavioral tasks could provide a more direct assessment of *reliance calibration*. For example, presenting participants with potentially flawed LLM outputs related to energy and offering options to verify the information (e.g., fact-checking as in Rheu & Cho, 2025) or choose between LLM advice and another source would better capture actual reliance tendencies.
4.  **Affective Trust in LLMs (RQ4/H4):** Given the interactive and sometimes anthropomorphic nature of LLMs, the affective dimension of trust (e.g., perceived empathy, care, responsiveness) is likely relevant. While the TPA-Revised focuses on cognitive trust/distrust facets, incorporating a measure specifically capturing affective trust (e.g., items inspired by the Semantic Differential AI Trust scale) could provide additional insights into how emotional responses to LLMs influence trust and usage for energy tasks.
5.  **Deepening Theoretical Grounding:** The theoretical grounding could be further enriched by explicitly discussing how general cognitive biases (e.g., automation bias, confirmation bias) might interact with energy-specific biases when users process LLM-generated energy information. Exploring how LLM mental models are formed and potentially updated through interaction, drawing on cognitive theories of learning and belief revision, would also strengthen the framework. While behavioral economics is mentioned in the energy context, explicitly linking it to the AI interaction (e.g., how LLM framing of information might act as a nudge, and how literacy or trust influences susceptibility to such nudges) presents a rich avenue for theoretical exploration.

In conclusion, the survey design represents a methodologically sound approach to investigating the complex interplay between individual differences, cognitive biases, AI literacy, and trust in the novel context of LLMs for energy management. It appropriately leverages established measures where available and develops conceptually grounded custom items for context-specific constructs. The literature review highlights both the theoretical foundations supporting this approach and potential avenues for future refinement, particularly concerning the measurement of objective literacy, behavioral reliance, and the intricate cognitive processes involved in evaluating AI-generated information in a decision-making context. Rigorous validation of custom measures remains essential for ensuring the psychometric quality of the instrument.






---


# https://aistudio.google.com/prompts/1ytvkbZ2_jIETchshS4ra0KmF03RARiB5

This literature review synthesizes established findings from cognitive psychology, human-AI interaction (HAI), and energy-related decision-making research to establish the construct validity and psychometric rationale for the measurement approach employed in the proposed survey. The study investigates lay perceptions of Large Language Model (LLM) accuracy and utility for household energy management tasks, examining the roles of individual differences (numeracy, energy literacy, AI literacy) and trust in shaping these perceptions and subsequent behavioral intentions. A rigorous measurement strategy is paramount for understanding these complex, latent constructs (Nunnally & Bernstein, 1994).

**1. Cognitive and Motivational Underpinnings of Energy-Decision Accuracy**

Decades of research have consistently demonstrated systematic biases in lay estimations of household energy consumption (Attari et al., 2010; Kempton et al., 1985). Individuals tend to compress the range of energy use, underestimating high-consumption activities (e.g., heating) while overestimating low-consumption ones (e.g., turning off lights), a pattern potentially explained by reliance on availability or affect heuristics rather than quantitative reasoning (Larrick & Soll, 2008; Tversky & Kahneman, 1974). These biases persist despite informational campaigns, suggesting deep-seated cognitive limitations or the use of simplifying, often inaccurate, mental models (van den Broek & Walker, 2019). Research by Kantenbacher and Attari (2021) further elucidated that experts employ qualitatively different, more valid heuristics (e.g., associating motors and heat with high energy use) compared to novices.

Importantly, the accuracy of energy estimates correlates with domain-general numerical competence (Marghetis et al., 2019). This aligns with psychophysical scaling principles (Stevens, 1946) and judgment and decision-making research (Peters et al., 2006), suggesting that objective numeracy provides a foundational cognitive skill for accurately representing and comparing magnitudes. Therefore, numeracy is hypothesized to moderate the accuracy of beliefs about LLM-provided energy estimates (Hypothesis H1b).

**Measurement Approach:**
*   **Objective Numeracy:** Measured using the Berlin Numeracy Test (4-item version; Cokely et al., 2012). This widely used instrument predicts comprehension of risk and quantitative information across various domains, including health and finance, and has been linked to energy estimate accuracy (Marghetis et al., 2019). The open-ended format reduces guessing and captures computational skill alongside understanding.

**2. Human–AI Interaction: Trust, Calibration, and Literacy**

In human-AI interaction, a common finding is *automation bias*, where users tend to over-rely on automated suggestions, even when flawed (Mosier et al., 1998; Parasuraman & Riley, 1997). This tendency may be exacerbated with LLMs due to their linguistic fluency, which can inflate perceived competence independent of factual accuracy (Kim et al., 2024). Steyvers et al. (2025) identified a significant *calibration gap* for LLMs, where user confidence exceeded objective model performance, particularly for longer, more elaborate outputs. Interventions requiring analytical engagement (cognitive forcing) can mitigate over-reliance, though potentially at the cost of user satisfaction (Bucinca et al., 2021).

Trust in AI is a multidimensional construct, evolving from generic automation trust (Lee & See, 2004) to LLM-specific measures. The **Trust-In-LLMs Index (TILLMI-6)** differentiates cognitive reliance (belief in capability) from affective closeness (emotional connection), demonstrating good psychometric properties and construct validity (Duro et al., 2025). Measuring general LLM trust provides a dispositional baseline complementing task-specific trust assessments.

AI literacy, encompassing knowledge, skills, and critical evaluation abilities (Long & Magerko, 2020), is theorized to influence trust calibration and reliance decisions. While objective AI literacy tests exist (e.g., Jin et al., 2024; Weber et al., 2023), *perceived* AI literacy is often more proximal to self-reported confidence and interaction choices in consumer contexts. The **Perceived Artificial Intelligence Literacy Questionnaire (PAILQ-6)** offers a concise, validated measure of self-assessed AI competence with good internal consistency (α ≈ .83) and invariance across demographic groups (Grassini, 2024). Its brevity suits survey constraints, and its focus on perceived competence aligns with the study's interest in subjective calibration judgments.

**Measurement Approach:**
*   **General LLM Trust:** Measured using the TILLMI-6 (Duro et al., 2025), capturing both cognitive reliance and affective closeness dimensions. Minor wording adaptation ("an LLM") ensures generalizability.
*   **Perceived AI Literacy:** Measured using the PAILQ-6 (Grassini, 2024) via 7-point Likert scales to capture self-assessed competence relevant to trust and reliance judgments.

**3. Energy and AI Literacies as Distinct Predictors**

Energy literacy comprises cognitive (knowledge), affective (attitudes/values), and behavioral components (DeWaters & Powers, 2011). Meta-analyses indicate the cognitive dimension is most strongly associated with accurate energy consumption estimates, while affective components correlate more with conservation intentions (Shipley et al., 2022). This study focuses on the cognitive knowledge aspect as a potential moderator or predictor of belief accuracy when evaluating LLM outputs.

**Measurement Approach:**
*   **Energy Knowledge:** Measured using a short form of the cognitive subscale of the Energy Literacy Questionnaire (ELQ; DeWaters & Powers, 2011, 2013). Six items with high item-total correlations (r > .35) and test-retest reliability (r = .78) representing core household energy concepts are selected to provide an objective measure of domain-specific knowledge.

**4. Task-Specific Beliefs and Intentions**

Evaluating LLMs for specific energy tasks (estimation vs. planning) requires measures tailored to those tasks.

**Measurement Approach:**
*   **Task-specific Accuracy & Trust:** Measured using a custom 7-item battery. Items are designed to assess perceived accuracy and trustworthiness for the distinct tasks of appliance energy estimation and personalized plan generation, drawing conceptual framing from Bucinca et al. (2021) and Kim et al. (2024). Bipolar 1–7 anchors are used, including a reverse-coded item to mitigate acquiescence bias.
*   **Intention to Use:** Measured using 3 bespoke items framed according to the Theory of Planned Behavior (Ajzen, 1991), assessing likelihood of using LLMs for energy management within a specified future timeframe (e.g., 6 months).

**5. Measurement Choices and Psychometric Rationale Summary**

The selection of instruments balances established validity with contextual relevance and brevity.

| Construct                      | Instrument (items)       | Key Validity Evidence (from Literature)                                                  | Adaptations for this Survey                             | Anticipated Role in Model                       |
| :----------------------------- | :----------------------- | :--------------------------------------------------------------------------------------- | :------------------------------------------------------ | :---------------------------------------------- |
| Objective Numeracy             | Berlin 4-item (open)¹    | Predicts risk comprehension, decision quality, energy estimate accuracy (Cokely et al., 2012; Marghetis et al., 2019) | Retain open-ended format; adaptive scoring            | Moderator of belief accuracy (H1b)              |
| Perceived AI Literacy          | PAILQ-6²                 | Validated factor structure (CFI=.96), strict invariance, good internal consistency (α≈.83) (Grassini, 2024) | Wording unchanged; 7-pt Likert                          | Predictor of trust-calibration (H2c)            |
| Energy Knowledge               | ELQ short cognitive pool³| Subscale predicts energy estimates; items show good discrimination & reliability (DeWaters & Powers, 2011) | Six U.S.–context items with highest discrimination      | Predictor of belief accuracy (H2b)              |
| General LLM Trust              | TILLMI-6⁴                | Validated two-factor structure (CFI=.995), reliability (ω=.89), convergent validity (Duro et al., 2025) | Minor stem tweak: “When interacting with **an LLM**…” | Mediator between perceived accuracy & intention (H4) |
| Task-specific Accuracy & Trust | Custom 7-item battery    | Items map onto trust/accuracy constructs studied in HAI (Bucinca et al., 2021; Kim et al., 2024) | 1–7 bipolar anchors; reverse-coded scepticism item | Dependent variables (RQ1) / Predictors (RQ3)    |
| Intention to Use               | 3 bespoke items          | Framed per Theory of Planned Behaviour (Ajzen, 1991); face-valid                         | 6-month horizon for behavioural prediction              | Outcome variable (RQ3, H3)                      |

*References: ¹Cokely et al. (2012); ²Grassini (2024); ³DeWaters & Powers (2011); ⁴Duro et al. (2025).*

Instruments are presented logically (knowledge → beliefs → trust → intention) to minimize priming effects. Pilot testing will assess internal consistency (α, ω) and item-total correlations for custom scales.

**6. Construct Validity Strategy**

Following Campbell and Fiske's (1959) multitrait-multimethod logic, the design incorporates multiple methods and levels of specificity to strengthen construct validity:
*   **Objective vs. Self-Report:** Contrasting objective numeracy (Berlin Test) and energy knowledge (ELQ cognitive) with self-reported AI literacy (PAILQ-6) guards against mono-method bias.
*   **General vs. Task-Specific Trust:** Comparing general LLM trust (TILLMI-6) with task-specific trust ratings allows for assessing trust calibration and generalization.
*   **Behavioral Intention Criterion:** Including intention to use as a distinct outcome enables testing the predictive validity of the belief and trust measures (H3).

Furthermore, exploring interactions, such as Numeracy × Energy Literacy, allows testing theoretical predictions (e.g., from Fuzzy-Trace Theory, which suggests numeracy aids verbatim processing while knowledge supports gist-based reasoning; Brainerd & Reyna, 2002) regarding synergistic effects on evaluating LLM outputs.

**7. Conclusion**

This measurement approach provides a robust foundation for investigating lay perceptions of LLMs in the household energy domain. By employing validated instruments where available (BNT, PAILQ-6, ELQ cognitive, TILLMI-6), developing theoretically grounded custom items for task-specific beliefs and intentions, and adhering to principles of construct validity, the survey is well-positioned to yield reliable and meaningful insights. This allows the study to rigorously test hypotheses concerning the roles of cognitive abilities, domain knowledge, AI literacy, and trust in shaping user understanding and potential adoption of LLMs as energy management aids, ultimately contributing to the understanding of human-AI collaboration in complex decision contexts.




------

# https://chatgpt.com/g/g-p-680c9643408c8191a128bbb19854ccee-ai-brainstorm/c/68155dbb-6b84-8006-bea5-9f68d07b0828


## Literature Review: Construct Validity and Measurement Approach

### 1 Cognitive & Motivational Roots of Energy-Decision Accuracy

Across four decades, behavioural research has demonstrated pervasive, systematic error in lay estimates of household‐energy use. Participants compress the true numerical range, dramatically under-rating high-consumption activities and over-rating trivial ones . These biases persist despite repeated public-information efforts and reflect reliance on salient but uninformative heuristics—e.g., “anything that is turned **on** must be costly” (Kempton et al., 1985) rather than watt-hour integration. More recent experiments document that concise, expert-derived heuristics or scalar anchors can partially repair these misperceptions, thereby improving behavioural intentions . Qualitative work with energy specialists further clarifies which cognitively “fast-and-frugal” rules are diagnostically valid (e.g., motor-heat dual-use appliances ≈ high kWh) and which novice rules are not .

Notably, the magnitude of error covaries with domain-general numerical competence: higher Berlin-Test scores predict smaller absolute deviations from ground-truth consumption , suggesting that **objective numeracy** functions as an enabling resource for accurate magnitude representation (cf. psychophysical scaling theory; Stevens, 1946). Hence, numeracy is theorised to moderate accuracy beliefs (H1b).

### 2 Human–AI Interaction: Trust, Calibration & Literacy

Laboratory- and field-based studies of algorithmic decision-support show that people often “over-trust” default AI explanations, a phenomenon labelled *automation bias* (Mosier et al., 1998). Buçinca et al. (2021) demonstrated that requiring analytic engagement (cognitive forcing) reduces such overreliance, albeit at the cost of lower subjective system ratings . With Large Language Models specifically, Steyvers et al. (2025) uncovered a pronounced **calibration gap**: users’ confidence exceeded ground-truth model accuracy, and longer, more verbose answers further inflated confidence without improving correctness .

Trust research has matured from generic automation to LLM-specific constructs. The TILLMI-6 differentiates *affective closeness* from *cognitive reliance*; psychometrics on 1 000 U.S. adults show a robust two-factor structure (CFI =.995) and meaningful personality correlates . Such evidence supports its inclusion as a criterion-valid measure of **general LLM trust**, complementary to task-specific trust items crafted for energy estimation.

Parallel work on **AI literacy** argues that self-perceived competence shapes reliance decisions. The PAILQ-6 offers a concise assessment with good internal consistency (α ≈ .83) and strict factorial invariance across gender and age groups . Although multiple, longer instruments exist (MAILS-72, AILQ-32), brevity is required for survey feasibility; moreover, perceived literacy—not objective coding skill—is theoretically proximal to calibration judgements in consumer contexts.

### 3 Energy & AI Literacies as Distinct Yet Synergistic Predictors

Energy literacy encompasses factual knowledge, affect, and behavioural dispositions (DeWaters & Powers, 2011). Meta-analytic evidence links the *cognitive* subscale most strongly to accurate consumption estimates, whereas the *affective* subscale aligns with pro-environmental action . For the present study, a six-item factual subset targets the knowledge facet that is hypothesised to interact with numeracy and AI literacy when participants evaluate LLM outputs.

### 4 Measurement Choices and Psychometric Rationale

| Construct                      | Instrument (items)       | Key validity evidence                                                          | Adaptations for this survey                                     | Anticipated role in model                       |
| ------------------------------ | ------------------------ | ------------------------------------------------------------------------------ | --------------------------------------------------------------- | ----------------------------------------------- |
| Objective Numeracy             | Berlin 4-item (open)     | Predicts risk comprehension and energy‐estimate accuracy                       | Retain open-ended format; adaptive scoring to maximise variance | Moderator of belief accuracy                    |
| Perceived AI Literacy          | PAILQ-6                  | Two-level CFA fit: CFI =.96; strict invariance                                 | Wording unchanged; 7-pt Likert                                  | Predictor of trust-calibration                  |
| Energy Knowledge               | ELQ short cognitive pool | Item–total r >.35; test–retest r =.78                                          | Six U.S.–context items with highest discrimination              | Predictor of belief accuracy                    |
| General LLM Trust              | TILLMI-6                 | Two-factor, ω =.89; convergent with Big-Five traits                            | Minor stem tweak: “When interacting with **an LLM** …”          | Mediator between perceived accuracy & intention |
| Task-specific Accuracy & Trust | Custom 7-item battery    | Items aligned with constructs in Buçinca et al. (2021) and Kim et al. (2024)  | 1–7 bipolar anchors; reverse-coded scepticism item              | Dependent variables                             |
| Intention to Use               | 3 bespoke items          | Framed per Theory of Planned Behaviour; face-valid                             | 6-month horizon for behavioural prediction                      | Outcome variable                                |

Each scale appears in a logically staged order (knowledge → beliefs → trust → intention) to minimise consistency artefacts. Pilot pre-testing will compute Cronbach’s α and McDonald’s ω; items dropping <.30 in item-total correlations will be culled.

### 5 Construct Validity Strategy

Following the *multitrait–multimethod* logic (Campbell & Fiske, 1959), the design triangulates:

* **Objective vs. self-report** (Berlin vs. PAILQ) to guard against mono-method bias.
* **General vs. task-specific trust** (TILLMI vs. custom items) to detect mis-calibration.
* **Behavioural intention** as a separate criterion, enabling tests of predictive validity (H2c).

Cognitive psychology highlights that numeracy primarily enhances *verbatim* representations, whereas domain knowledge enriches *gist* reasoning (Fuzzy-Trace Theory). Thus the interaction term Numeracy × Energy Literacy will be probed for synergistic effects on accuracy beliefs.

### 6 Conclusion

By grounding each latent construct in well-validated instruments, aligning item wording to the household-energy/LLM context, and balancing brevity with psychometric rigour, the survey achieves strong **content**, **convergent**, and **predictive** validity. This foundation positions the project to illuminate whether literacies and numeracy jointly foster better-calibrated trust in emerging AI advisors—or whether, as sceptics fear, they merely shift the locus of misplaced confidence from the self to the machine.









----

# nblm

Drawing upon established principles in cognitive psychology, psychometrics, and the burgeoning literatures on human-AI interaction and energy-related decision-making, this review provides the necessary background to understand the construct validity and measurement approach employed in the proposed survey. A rigorous measurement strategy is paramount for empirical inquiry, particularly when examining complex, latent constructs that are not directly observable, such as various forms of literacy, trust, and cognitive biases. The design of a valid and reliable measurement instrument is a foundational step in psychological and educational research, ensuring that the collected data accurately reflect the theoretical constructs under investigation.

**Construct Validity and Psychometric Soundness**

Central to the development and evaluation of any psychological or educational measurement tool is the concept of construct validity. Construct validity refers to the degree to which a test or instrument measures the theoretical construct it is intended to measure. Unlike directly measurable physical properties, constructs like intelligence, personality, trust, or literacy are theoretical abstractions. Establishing that an instrument validly measures such a construct requires accumulating evidence from various sources.

Multiple facets contribute to the overall construct validity of an instrument. Content validity, for example, assesses whether the items on a survey or test adequately cover the entire domain or scope of the construct being measured. This often involves defining the construct clearly, identifying its key dimensions and characteristics, and consulting subject matter experts (SMEs) to evaluate the relevance and comprehensiveness of the items. Criterion validity examines the extent to which an instrument's scores correlate with other measures (criteria) that are theoretically related to the construct. This includes both concurrent validity (correlation with a criterion measured at the same time) and predictive validity (correlation with a criterion measured in the future). Convergent and discriminant validity, often evaluated using techniques like factor analysis or correlation analysis, demonstrate whether the instrument correlates highly with measures of similar constructs (convergent) and poorly with measures of dissimilar constructs (discriminant). Factor analysis, in particular, is a statistical technique used to explore or confirm the underlying dimensional structure of a set of items, verifying whether the items group together as expected based on the theoretical construct and its proposed facets. Reliability, while distinct from validity, is a necessary prerequisite for it, indicating the consistency and stability of the measurement. Common methods for assessing reliability include internal consistency (e.g., Cronbach's alpha) and test-retest reliability.

The literature emphasizes that validity is not a fixed property of an instrument but rather an ongoing process of accumulating evidence for the appropriateness of score interpretations for a specific purpose and population. Therefore, when adapting or developing measures, particularly for novel contexts like human-LLM interaction in energy decision-making, meticulous attention to these psychometric principles is essential.

**Measurement Approaches for Key Constructs**

The proposed survey integrates several key constructs drawn from cognitive science, human-AI interaction, and energy research to understand how individuals perceive and utilize LLMs for household energy management. The measurement approach for each construct draws upon established methodologies, sometimes requiring adaptation to the specific context.

**AI Literacy:** Conceptualizing and measuring AI literacy remains a challenge due to the rapid evolution of AI and the diverse fields in which it is applied. There is no single, universally adopted definition or measurement tool. However, the literature offers several influential frameworks that delineate key dimensions of AI literacy, often including cognitive (knowledge and understanding), operational (use and application), critical (evaluation), and ethical aspects. Prominent conceptualizations by Long and Magerko (2020) and Ng et al. (2021) are frequently referenced as foundational.

Measurement approaches for AI literacy typically fall into two categories: self-report scales and objective tests. Self-report instruments, such as the Meta AI Literacy Scale (MAILS; Carolus et al., 2023) and the Scale for the assessment of non-experts’ AI literacy (SNAIL; Laupichler et al., 2023), assess individuals' perceived knowledge, skills, attitudes, and confidence regarding AI. These scales have undergone validation processes, including exploratory and confirmatory factor analyses to establish their factorial structure and assessment of internal consistency and convergent validity. MAILS, for instance, measures dimensions like Know & Understand AI, Use & Apply AI, Evaluate & Create AI, and AI ethics, and also includes psychological meta-competencies believed to support AI interaction. The development of shorter versions, such as the MAILS Short Form (MAILS-SF), aims to improve efficiency for survey administration.

Objective tests, such as the Scale of Artificial Intelligence Literacy for all (SAIL4ALL; Soto-Sanfiel et al., 2024) and instruments developed by Weber et al. (2023) and Tully et al. (2023/2024), aim to measure factual knowledge about AI using formats like multiple-choice questions. This approach is often favored to mitigate potential biases inherent in self-assessment, such as overconfidence (e.g., the Dunning-Kruger effect). However, objective tests may not fully capture the affective, attitudinal, or practical application dimensions covered by self-report scales. A multi-method approach, combining subjective and objective measures, is often recommended for a comprehensive understanding of AI literacy. For the proposed survey, utilizing a validated self-report scale like MAILS-SF, which encompasses cognitive, critical, and ethical dimensions relevant to understanding and evaluating AI, is a well-supported approach.

**Energy Literacy:** Energy literacy is recognized as a multidisciplinary construct encompassing knowledge, attitudes, and behaviors related to energy. It involves understanding energy concepts, the impacts of energy use, and the ability to make informed energy choices. Measurement typically involves surveys and questionnaires designed to assess cognitive knowledge (e.g., how energy works, sources, consumption patterns), affective dimensions (attitudes, values, concerns about energy issues), and behavioral aspects (self-reported energy-saving actions, intentions).

The development of energy literacy instruments, such as the Energy Literacy Questionnaire (ELQ; DeWaters & Powers, 2011), has followed established psychometric principles. This includes defining the construct based on literature reviews and expert input, developing item pools, conducting pilot tests, and assessing reliability (e.g., Cronbach's alpha) and validity (content validity via expert panels, construct validity via factor analysis and contrasted group comparisons). While some instruments were developed for specific populations or contexts (e.g., secondary students in New York State), they often provide a useful starting point for adaptation. Given the survey's focus on household energy management, measuring cognitive knowledge related to household energy use, along with relevant attitudes and reported behaviors, is crucial. The self-report nature of behavioral and affective measures in energy literacy scales can introduce limitations.

**Beliefs about LLM Accuracy, Utility, and Trustworthiness for Energy Tasks:** Assessing specific beliefs about LLMs within the narrow domain of household energy management necessitates the development of custom measures, as standardized scales for this precise intersection are not readily available. These measures should operationalize concepts such as perceived accuracy of LLM-provided energy information, perceived utility of LLMs for energy-related tasks (e.g., estimation, planning, advice), and task-specific trust in the LLM's capabilities within this domain.

Drawing on methodologies used in energy perception research (e.g., Attari et al., 2010), which often employ estimation tasks to reveal cognitive biases in understanding energy consumption, can inform the conceptualization of perceived accuracy. Custom Likert-scale items are a standard approach for measuring perceived utility and task-specific trust. When crafting items for task-specific trust, it is beneficial to adapt wording from validated general trust scales, explicitly specifying the AI agent (e.g., "ChatGPT") and the relevant energy task (e.g., "estimating appliance energy use," "creating an energy-saving plan"). Pilot testing these custom items is advisable to ensure clarity, relevance, and initial evidence of psychometric properties. Rigorous development and potential psychometric evaluation in a dedicated study would further enhance the validity of these context-specific measures.

**General Trust in AI:** Trust in automated systems and AI is a critical construct in human-AI interaction literature. Conceptualizations often distinguish between cognitive trust (based on assessments of competence, reliability, predictability, and transparency) and affective trust (based on emotional responses or perceived care/empathy). The distinction between trust and distrust is also increasingly recognized as important, suggesting they may be separate constructs with different influences on behavior. Dispositional trust, or a general propensity to trust, can also influence trust in specific AI systems.

Measurement instruments for general trust in AI include various scales, some of which are rooted in models of interpersonal or organizational trust (e.g., adapting concepts from Mayer et al., 1995). Validated scales, such as those examining a two-factor structure of trust/distrust (e.g., Lai et al.'s, 2024 validation of a TPA-revised scale) or scales measuring cognitive and affective trust using semantic differentials (e.g., Shang et al.'s, 2024 scale), are valuable resources. Using scales that distinguish between trust and distrust and capture affective dimensions is particularly relevant for understanding interactions with LLMs, which can elicit anthropomorphic perceptions and emotional responses. The validation of these scales through scenario-based studies supports their applicability in AI contexts. While general trust in automation research has seen progress in survey instrument development, empirical validation can be inconsistent, and construct proliferation exists. Utilizing validated measures and clearly defining the specific facets of trust being assessed are crucial.

**Willingness to Use / Frequency of Use:** These measures represent behavioral intentions and reported behavior, commonly assessed in technology adoption research. Standard Likert scales are typically used to gauge participants' likelihood or intention to use LLMs for energy-related tasks (willingness). Self-report items are employed to measure the frequency of past LLM use, requiring clear specification of the AI type (e.g., ChatGPT, Gemini), the context (energy-related decisions/information), and the relevant time frame (e.g., "in the past 3 months") to ensure reliable reporting. These measures serve as dependent variables, reflecting the potential impact of cognitive factors, literacy, and trust on the actual or intended adoption of LLMs in this domain.

**Other Relevant Cognitive and Attitudinal Factors:** The survey also incorporates measures of numeracy, environmental values/attitudes, and assesses susceptibility to energy perception biases. Numeracy, the ability to understand and use quantitative information, is a fundamental cognitive skill relevant to interpreting data provided by LLMs and understanding energy consumption information. Standardized tests exist for measuring numeracy, providing validated approaches for this construct. Environmental values and attitudes are known predictors of energy-saving behaviors and intentions and are typically assessed via survey items. Measuring these allows for an examination of their potential role alongside AI-specific factors in influencing energy-related decisions mediated by LLMs. Assessing energy perception biases, drawing on methods from prior research, helps contextualize how pre-existing cognitive tendencies in estimating energy use might interact with LLM-provided information.

In conclusion, the measurement approach for the proposed survey is grounded in established psychometric principles and draws upon methodologies from cognitive psychology, human-AI interaction, and energy research. It involves leveraging validated instruments for established constructs (AI literacy, general AI trust, numeracy, environmental attitudes) where possible, while developing carefully designed custom measures for context-specific beliefs and task-specific trust in LLMs within the energy domain. The emphasis on construct validity, through literature grounding, clear conceptualization, and attention to psychometric properties, is crucial for generating robust and interpretable findings on the complex interplay of cognitive factors, AI literacy, trust, and energy decision-making in the age of LLMs.


# https://notebooklm.google.com/notebook/a04e8d62-e334-44e2-86ee-f1bf4b02a21f?pli=1


**Construct Validity and Psychometric Soundness**

Central to the development and evaluation of any psychological or educational measurement tool is the concept of construct validity. Construct validity refers to the degree to which a test or instrument measures the theoretical construct it is intended to measure. Unlike directly measurable physical properties, constructs like intelligence, personality, trust, or literacy are theoretical abstractions. Establishing that an instrument validly measures such a construct requires accumulating evidence from various sources.

Multiple facets contribute to the overall construct validity of an instrument. Content validity, for example, assesses whether the items on a survey or test adequately cover the entire domain or scope of the construct being measured. This often involves defining the construct clearly, identifying its key dimensions and characteristics, and consulting subject matter experts (SMEs) to evaluate the relevance and comprehensiveness of the items. Criterion validity examines the extent to which an instrument's scores correlate with other measures (criteria) that are theoretically related to the construct. This includes both concurrent validity (correlation with a criterion measured at the same time) and predictive validity (correlation with a criterion measured in the future). Convergent and discriminant validity, often evaluated using techniques like factor analysis or correlation analysis, demonstrate whether the instrument correlates highly with measures of similar constructs (convergent) and poorly with measures of dissimilar constructs (discriminant). Factor analysis, in particular, is a statistical technique used to explore or confirm the underlying dimensional structure of a set of items, verifying whether the items group together as expected based on the theoretical construct and its proposed facets. Reliability, while distinct from validity, is a necessary prerequisite for it, indicating the consistency and stability of the measurement. Common methods for assessing reliability include internal consistency (e.g., Cronbach's alpha) and test-retest reliability.

The literature emphasizes that validity is not a fixed property of an instrument but rather an ongoing process of accumulating evidence for the appropriateness of score interpretations for a specific purpose and population. Therefore, when adapting or developing measures, particularly for novel contexts like human-LLM interaction in energy decision-making, meticulous attention to these psychometric principles is essential.

**Measurement Approaches for Key Constructs**

The proposed survey integrates several key constructs drawn from cognitive science, human-AI interaction, and energy research to understand how individuals perceive and utilize LLMs for household energy management. The measurement approach for each construct draws upon established methodologies, sometimes requiring adaptation to the specific context.

**AI Literacy:** Conceptualizing and measuring AI literacy remains a challenge due to the rapid evolution of AI and the diverse fields in which it is applied. There is no single, universally adopted definition or measurement tool. However, the literature offers several influential frameworks that delineate key dimensions of AI literacy, often including cognitive (knowledge and understanding), operational (use and application), critical (evaluation), and ethical aspects. Prominent conceptualizations by Long and Magerko (2020) and Ng et al. (2021) are frequently referenced as foundational.

Measurement approaches for AI literacy typically fall into two categories: self-report scales and objective tests. Self-report instruments, such as the Meta AI Literacy Scale (MAILS; Carolus et al., 2023) and the Scale for the assessment of non-experts’ AI literacy (SNAIL; Laupichler et al., 2023), assess individuals' perceived knowledge, skills, attitudes, and confidence regarding AI. These scales have undergone validation processes, including exploratory and confirmatory factor analyses to establish their factorial structure and assessment of internal consistency and convergent validity. MAILS, for instance, measures dimensions like Know & Understand AI, Use & Apply AI, Evaluate & Create AI, and AI ethics, and also includes psychological meta-competencies believed to support AI interaction. The development of shorter versions, such as the MAILS Short Form (MAILS-SF), aims to improve efficiency for survey administration.

Objective tests, such as the Scale of Artificial Intelligence Literacy for all (SAIL4ALL; Soto-Sanfiel et al., 2024) and instruments developed by Weber et al. (2023) and Tully et al. (2023/2024), aim to measure factual knowledge about AI using formats like multiple-choice questions. This approach is often favored to mitigate potential biases inherent in self-assessment, such as overconfidence (e.g., the Dunning-Kruger effect). However, objective tests may not fully capture the affective, attitudinal, or practical application dimensions covered by self-report scales. A multi-method approach, combining subjective and objective measures, is often recommended for a comprehensive understanding of AI literacy. For the proposed survey, utilizing a validated self-report scale like MAILS-SF, which encompasses cognitive, critical, and ethical dimensions relevant to understanding and evaluating AI, is a well-supported approach.

**Energy Literacy:** Energy literacy is recognized as a multidisciplinary construct encompassing knowledge, attitudes, and behaviors related to energy. It involves understanding energy concepts, the impacts of energy use, and the ability to make informed energy choices. Measurement typically involves surveys and questionnaires designed to assess cognitive knowledge (e.g., how energy works, sources, consumption patterns), affective dimensions (attitudes, values, concerns about energy issues), and behavioral aspects (self-reported energy-saving actions, intentions).

The development of energy literacy instruments, such as the Energy Literacy Questionnaire (ELQ; DeWaters & Powers, 2011), has followed established psychometric principles. This includes defining the construct based on literature reviews and expert input, developing item pools, conducting pilot tests, and assessing reliability (e.g., Cronbach's alpha) and validity (content validity via expert panels, construct validity via factor analysis and contrasted group comparisons). While some instruments were developed for specific populations or contexts (e.g., secondary students in New York State), they often provide a useful starting point for adaptation. Given the survey's focus on household energy management, measuring cognitive knowledge related to household energy use, along with relevant attitudes and reported behaviors, is crucial. The self-report nature of behavioral and affective measures in energy literacy scales can introduce limitations.

**Beliefs about LLM Accuracy, Utility, and Trustworthiness for Energy Tasks:** Assessing specific beliefs about LLMs within the narrow domain of household energy management necessitates the development of custom measures, as standardized scales for this precise intersection are not readily available. These measures should operationalize concepts such as perceived accuracy of LLM-provided energy information, perceived utility of LLMs for energy-related tasks (e.g., estimation, planning, advice), and task-specific trust in the LLM's capabilities within this domain.

Drawing on methodologies used in energy perception research (e.g., Attari et al., 2010), which often employ estimation tasks to reveal cognitive biases in understanding energy consumption, can inform the conceptualization of perceived accuracy. Custom Likert-scale items are a standard approach for measuring perceived utility and task-specific trust. When crafting items for task-specific trust, it is beneficial to adapt wording from validated general trust scales, explicitly specifying the AI agent (e.g., "ChatGPT") and the relevant energy task (e.g., "estimating appliance energy use," "creating an energy-saving plan"). Pilot testing these custom items is advisable to ensure clarity, relevance, and initial evidence of psychometric properties. Rigorous development and potential psychometric evaluation in a dedicated study would further enhance the validity of these context-specific measures.

**General Trust in AI:** Trust in automated systems and AI is a critical construct in human-AI interaction literature. Conceptualizations often distinguish between cognitive trust (based on assessments of competence, reliability, predictability, and transparency) and affective trust (based on emotional responses or perceived care/empathy). The distinction between trust and distrust is also increasingly recognized as important, suggesting they may be separate constructs with different influences on behavior. Dispositional trust, or a general propensity to trust, can also influence trust in specific AI systems.

Measurement instruments for general trust in AI include various scales, some of which are rooted in models of interpersonal or organizational trust (e.g., adapting concepts from Mayer et al., 1995). Validated scales, such as those examining a two-factor structure of trust/distrust (e.g., Lai et al.'s, 2024 validation of a TPA-revised scale) or scales measuring cognitive and affective trust using semantic differentials (e.g., Shang et al.'s, 2024 scale), are valuable resources. Using scales that distinguish between trust and distrust and capture affective dimensions is particularly relevant for understanding interactions with LLMs, which can elicit anthropomorphic perceptions and emotional responses. The validation of these scales through scenario-based studies supports their applicability in AI contexts. While general trust in automation research has seen progress in survey instrument development, empirical validation can be inconsistent, and construct proliferation exists. Utilizing validated measures and clearly defining the specific facets of trust being assessed are crucial.

**Willingness to Use / Frequency of Use:** These measures represent behavioral intentions and reported behavior, commonly assessed in technology adoption research. Standard Likert scales are typically used to gauge participants' likelihood or intention to use LLMs for energy-related tasks (willingness). Self-report items are employed to measure the frequency of past LLM use, requiring clear specification of the AI type (e.g., ChatGPT, Gemini), the context (energy-related decisions/information), and the relevant time frame (e.g., "in the past 3 months") to ensure reliable reporting. These measures serve as dependent variables, reflecting the potential impact of cognitive factors, literacy, and trust on the actual or intended adoption of LLMs in this domain.

**Other Relevant Cognitive and Attitudinal Factors:** The survey also incorporates measures of numeracy, environmental values/attitudes, and assesses susceptibility to energy perception biases. Numeracy, the ability to understand and use quantitative information, is a fundamental cognitive skill relevant to interpreting data provided by LLMs and understanding energy consumption information. Standardized tests exist for measuring numeracy, providing validated approaches for this construct. Environmental values and attitudes are known predictors of energy-saving behaviors and intentions and are typically assessed via survey items. Measuring these allows for an examination of their potential role alongside AI-specific factors in influencing energy-related decisions mediated by LLMs. Assessing energy perception biases, drawing on methods from prior research, helps contextualize how pre-existing cognitive tendencies in estimating energy use might interact with LLM-provided information.

In conclusion, the measurement approach for the proposed survey is grounded in established psychometric principles and draws upon methodologies from cognitive psychology, human-AI interaction, and energy research. It involves leveraging validated instruments for established constructs (AI literacy, general AI trust, numeracy, environmental attitudes) where possible, while developing carefully designed custom measures for context-specific beliefs and task-specific trust in LLMs within the energy domain. The emphasis on construct validity, through literature grounding, clear conceptualization, and attention to psychometric properties, is crucial for generating robust and interpretable findings on the complex interplay of cognitive factors, AI literacy, trust, and energy decision-making in the age of LLMs.







------




# https://notebooklm.google.com/notebook/a04e8d62-e334-44e2-86ee-f1bf4b02a21f?pli=1


**AI Literacy: Conceptualization and Measurement Approaches**

Defining and conceptualizing AI literacy has been an evolving challenge due to the broad applications of AI and the diverse fields associated with it [Almatrafi et al., 2024; Biagini, 2025]. Scholars have employed various definitions and constructs, leading to high variance in frameworks, often depending on the domain or level of application [Almatrafi et al., 2024]. The field of AI literacy research is relatively young and somewhat fragmented [Biagini, 2025; Pinski and Benlian, 2024; gemDR_Survey Link Collection and Organization_.md]. Early reviews sought to organize this fragmented body of work and propose definitions, but the academic discourse has advanced significantly since [Biagini, 2025]. There is a recognized need to review and assess the literature to document definitions and constructs, enabling the creation of frameworks to link different aspects of AI literacy for future research and practice [Almatrafi et al., 2024].

Several conceptualizations frame AI literacy. Influential frameworks often differentiate multiple dimensions. For instance, Ng et al. (2021b) proposed dimensions including "Know & Understand AI," "Use & Apply AI," "Evaluate & Create AI," and "AI ethics" [Carolus et al., 2023; Jin et al., 2024; Koch et al., 2024]. Similarly, Long and Magerko (2020) developed a conceptual framework based on an exploratory literature review, structured around questions like "What is AI?", "What can AI do?", "How does AI work?", "How should AI be used?", and "How do people perceive AI?" [Long and Magerko, 2020; Pinski and Benlian, 2024; Soto-Sanfiel et al., 2024]. Some frameworks extend these to include cognitive, operational, critical, and ethical dimensions [Biagini et al., Assessing AI Literacy; Biagini et al., Assessing AI Literacy]. The multidimensional nature of AI literacy, encompassing knowledge, abilities, attitudes, and behaviors, is widely recognized [Biagini et al., Assessing AI Literacy; Biagini et al., Assessing AI Literacy; Ng et al., 2024].

Measurement instruments for AI literacy reflect this multidimensionality, but vary in their approach. A notable distinction exists between subjective self-assessment scales and objective knowledge tests [gemDR_AI_Energy_Surveys2.md]. Self-report scales, such as the Meta AI Literacy Scale (MAILS) [Carolus et al., 2023; Koch et al., 2024], the Scale for the assessment of non-experts’ AI literacy (SNAIL) [Laupichler et al., 2023], and Ng et al.'s questionnaire [Ng et al., 2024], capture individuals' self-perceived understanding, attitudes, confidence, and reported behaviors [gemDR_AI_Energy_Surveys2.md; Ng et al., 2024]. These measures are valuable for assessing individuals' subjective beliefs about their capabilities and their affective responses to AI, which can influence their willingness to engage with the technology [Ng et al., 2024; Scantamburlo et al., 2025]. Validation efforts for these scales typically involve literature reviews to define the construct, expert consultations to ensure content validity, pilot testing to refine items, and statistical methods like exploratory and confirmatory factor analysis to examine the underlying factor structure and establish construct validity [Biagini et al., Assessing AI Literacy; Carolus et al., 2023; Koch et al., 2024; Laupichler et al., 2023; Ng et al., 2024]. For example, MAILS was developed and validated based on existing literature and established conceptualizations, utilizing confirmatory factor analysis on samples of German-speaking adults to test its structure and demonstrate psychological psychometric requirements [Carolus et al., 2023; Koch et al., 2024]. The MAILS short form (MAILS-SF) is supported as a validated self-report scale for the general population, capturing facets like knowledge/understanding and critical evaluation/ethics, with established correlations with objective knowledge, prior AI use, and attitudes [gemDR_AI Literacy_Trust_Energy_LLM.md].

In contrast, objective measures of AI literacy aim to assess factual knowledge or demonstrable skills, often using multiple-choice or true/false questions [gemDR_Survey Link Collection and Organization_.md; Soto-Sanfiel et al., 2024; Weber et al., 2023]. Instruments like SAIL4ALL [Soto-Sanfiel et al., 2024] and the measure developed by Weber et al. (2023) fall into this category. This methodological shift towards objective assessment in some recent instruments is driven by concerns about potential biases inherent in self-perception, such as overconfidence or social desirability, which can lead self-report measures to potentially reflect perceived rather than actual competence [gemDR_Survey Link Collection and Organization_.md]. The development of objective measures often follows systematic procedures, adapting established scale development methods [Weber et al., 2023]. For instance, SAIL4ALL's development involved a three-phase methodology including literature review, expert feedback, pilot testing, and quantitative analysis to establish psychometric properties [Soto-Sanfiel et al., 2024].

The divergence between self-report and objective measures highlights different facets of AI literacy; self-report may capture confidence and attitudes more strongly, while objective tests focus on factual knowledge [gemDR_AI_Energy_Surveys2.md]. Cognitive science theory suggests that self-assessments of knowledge or skill can be influenced by metacognitive biases (e.g., the Dunning-Kruger effect) [gemDR_Survey Link Collection and Organization_.md]. Therefore, a comprehensive understanding might necessitate a multi-method approach assessing both subjective and objective aspects of AI literacy [gemDR_AI_Energy_Surveys2.md]. Despite advancements, the lack of a universal consensus on definition and standard measurement tool leads to fragmentation, making direct comparisons across studies using different instruments challenging [Almatrafi et al., 2024; Lintner, 2024; gemDR_AI_Energy_Surveys2.md; gemDR_Survey Link Collection and Organization_.md]. Researchers must carefully define which aspects of AI literacy are being measured and select instruments accordingly [gemDR_AI_Energy_Surveys2.md]. Further validation, including convergent validity with other literacy measures and testing across diverse populations and cultural contexts, is needed for many instruments [Carolus et al., 2023; Grassini, 2024; Koch et al., 2024; Laupichler et al., 2023; Soto-Sanfiel et al., 2024].

**Trust in AI/Automation: Conceptualization and Measurement**

Trust in automated systems is a critical area of research, though the scientific understanding, particularly within specific domains like military applications, is noted as still relatively immature and not well integrated [Adams, TRUST IN AUTOMATED SYSTEMS]. Nonetheless, a substantial body of research and theory from related contexts can inform the understanding of trust in automation [Adams, TRUST IN AUTOMATED SYSTEMS].

Trust is often conceptualized as a multifaceted psychological construct influencing human-technology interaction [Scantamburlo et al., 2025; Zhai et al., 2024]. Multi-dimensional models of trust, often adapted from models of human-human trust, are commonly applied [Kohn et al., 2021; Razin and Feigh, 2024]. Key components frequently discussed include perceived trustworthiness, encompassing factors like competence, reliability, and predictability, as well as risk-taking or reliance on the automated system and affective responses [Kohn et al., 2021; Razin and Feigh, 2024; gemDR_AI Literacy_Trust_Energy_LLM.md]. Cognitive aspects of trust relate to the evaluation of the system's capabilities and dependability, while affective aspects relate to feelings or emotional responses towards the system [gemDR_AI Literacy_Trust_Energy_LLM.md; Razin and Feigh, 2024].

Measurement approaches for trust in AI and automation primarily rely on questionnaires and behavioral measures, though empirical validation of questionnaires has been noted as spotty, contributing to construct proliferation [Kohn et al., 2021; Razin and Feigh, 2024]. Questionnaires are a common approach, but different instruments may capture distinct components of the trust process, which, if not distinguished, can lead to seemingly contradictory findings [Kohn et al., 2021]. Meta-analyses are employed to integrate findings and provide a more comprehensive understanding of the field and the validity/reliability of existing instruments [Kohn et al., 2021; Razin and Feigh, 2024].

Relevant scales for measuring trust in AI include the TPA-Revised, which has been validated in AI contexts and measures general trust and distrust as distinct constructs [gemDR_AI Literacy_Trust_Energy_LLM.md]. This two-factor structure (Trust/Distrust) aligns with psychometric evidence suggesting the importance of measuring these as separate dimensions, as they may exert different influences on outcomes [Scholz et al., 2025; gemDR_AI Literacy_Trust_Energy_LLM.md]. The Semantic-Differential AI-Trust Scale, measuring cognitive and affective trust using bipolar adjectives, is also relevant, particularly for capturing the affective component of trust which is likely important in interactions with AI like LLMs that can elicit anthropomorphic responses [gemDR_AI Literacy_Trust_Energy_LLM.md]. Its validation through scenario-based studies supports its applicability [gemDR_AI Literacy_Trust_Energy_LLM.md].

For construct validity in measuring trust, instrument designers must carefully consider dimensionality, clear language, and the type of scale used [Razin and Feigh, 2024]. Content validity is also crucial, ensuring items adequately cover all relevant aspects of the intended construct, often leveraging subject matter experts in the review process [Razin and Feigh, 2024]. It is also important to clearly define the specific object of trust (e.g., AI in general, a specific LLM, or an LLM performing a particular task) and the dimension of trust being assessed to ensure the measure aligns with the theoretical construct [gemDR_AI Literacy_Trust_Energy_LLM.md].

**Energy Literacy: Conceptualization and Measurement**

Energy literacy is conceptualized as encompassing cognitive, affective, and behavioral dimensions, enabling individuals to make informed energy choices [Andolfi et al., FROM AWARENESS TO ACTION ENERGY LITERACY AND HOUSEHOLD ENERGY USE; DeWaters and Powers, 2011; DeWaters and Powers, 2013; DeWaters et al., 2013; Santillán and Cedano, 2023]. The cognitive dimension includes knowledge and understanding of energy concepts, as well as cognitive skills like critical analysis and problem-solving [DeWaters and Powers, 2011; DeWaters et al., 2013]. The affective dimension pertains to attitudes, values, and concerns related to energy issues, while the behavioral dimension covers predispositions and actual actions related to energy use [DeWaters and Powers, 2011; DeWaters et al., 2013].

The development of energy literacy questionnaires, such as the Energy Literacy Questionnaire (ELQ), has followed established psychometric principles [DeWaters and Powers, 2011; DeWaters and Powers, 2013; DeWaters et al., 2013]. The process typically involves defining the construct and establishing measurement criteria based on literature reviews and expert input [DeWaters and Powers, 2011; DeWaters and Powers, 2013; DeWaters et al., 2013; Andolfi et al., 2024]. This is followed by developing a pool of survey items, often adapted from existing instruments, and subjecting them to review by validity panels of experts to ensure content validity [DeWaters and Powers, 2011; DeWaters and Powers, 2013; DeWaters et al., 2013]. Pilot testing is then conducted to evaluate items and the overall instrument performance, followed by psychometric evaluation using methods like reliability analysis (e.g., Cronbach's alpha) and factor analysis to support construct validity [DeWaters and Powers, 2011; DeWaters et al., 2013].

While comprehensive, energy literacy measures can be developed with a specific context or population in mind, such as secondary students in a particular geographic area [DeWaters and Powers, 2011; DeWaters and Powers, 2013; DeWaters et al., 2013]. Such instruments may require adaptation for use with different populations or in different domains [DeWaters and Powers, 2013; DeWaters et al., 2013]. The content validity of these measures relies on ensuring the criteria and items adequately represent the different dimensions of energy literacy [DeWaters et al., 2013; DeWaters and Powers, 2013]. While quantitative surveys are commonly used, a mixed-methods approach incorporating qualitative data can provide a more thorough examination of the impact of various factors on energy consumption [Andolfi et al., FROM AWARENESS TO ACTION ENERGY LITERACY AND HOUSEHOLD ENERGY USE; DeWaters et al., 2013].

**LLM-Specific Beliefs and Task-Specific Trust**

Measuring constructs specific to the interaction with LLMs in a particular domain, such as perceived accuracy and utility for household energy management tasks, and trust in the LLM for these specific tasks, often requires the development of custom items [gemDR_AI Literacy_Trust_Energy_LLM.md]. Standardized scales designed for general AI literacy or trust may not fully capture the nuances of beliefs and trust specific to a particular AI type (LLM) used in a concrete context (household energy decisions).

Developing custom measures necessitates a rigorous process aligned with psychometric principles [DeVellis, 2016; Soto-Sanfiel et al., 2024]. This involves clearly defining and operationalizing the specific constructs (e.g., distinguishing accuracy as quantitative estimation correctness vs. utility as helpfulness for planning) [gemDR_AI Literacy_Trust_Energy_LLM.md]. Drawing upon the language and wording used in validated general scales can inform item construction for domain-specific measures, ensuring that the items conceptually align with the dimensions they are intended to assess (e.g., adapting wording for trust items to specify the LLM as the agent and the energy task as the context) [gemDR_AI Literacy_Trust_Energy_LLM.md]. Once initial items are generated, content validity should be established through expert review, and the items should undergo pilot testing to assess their clarity, comprehensibility, and preliminary psychometric properties within the target population [Biagini et al., Assessing AI Literacy; DeWaters and Powers, 2011; DeWaters et al., 2013; gemDR_AI Literacy_Trust_Energy_LLM.md; Ng et al., 2024].

**Measurement of Willingness and Frequency of Use**

Assessing willingness to use an AI system and the frequency of its use are common methods in technology adoption research to measure behavioral intention and actual behavior [gemDR_AI Literacy_Trust_Energy_LLM.md]. These are typically measured using standard self-report items. Willingness or behavioral intention can be assessed using Likert-scale items asking about likelihood or intention to use the system in the future [gemDR_AI Literacy_Trust_Energy_LLM.md]. Frequency of use is typically measured by asking participants to report how often they have used the system within a specified timeframe and context [gemDR_AI Literacy_Trust_Energy_LLM.md]. For clarity and consistency, it is important to clearly specify the type of AI (LLMs), the context (energy-related decisions), and the timeframe when measuring frequency of use [gemDR_AI Literacy_Trust_Energy_LLM.md].

**Measurement Approach for the Proposed Survey: Synthesis**

Based on the reviewed literature, a comprehensive survey battery to investigate the proposed research questions requires measuring AI literacy, energy literacy, general trust/distrust in AI, specific beliefs about LLM capabilities for energy tasks, task-specific trust in LLMs for energy tasks, and willingness/frequency of LLM use for energy tasks [gemDR_AI_Energy_Surveys2.md; survey_eval.md].

The use of validated scales for foundational constructs, such as a short form of MAILS for self-perceived AI literacy, validated measures for general trust/distrust (e.g., TPA-Revised), and components of energy literacy scales (e.g., ELQ cognitive and behavioral sections), is supported by the literature [gemDR_AI Literacy_Trust_Energy_LLM.md; gemDR_AI_Energy_Surveys2.md; survey_eval.md]. These scales have undergone some level of psychometric evaluation to establish reliability and validity.

However, given the specific focus on LLMs and household energy management, custom measures are essential for capturing beliefs about LLM accuracy and utility in this domain, and trust specifically directed at LLMs for energy tasks [gemDR_AI Literacy_Trust_Energy_LLM.md; survey_eval.md]. The process for developing these custom items must adhere to established psychometric standards, including clear construct definition, item generation informed by theory and related scales, expert review for content validity, and pilot testing [Biagini et al., Assessing AI Literacy; DeVellis, 2016; gemDR_AI Literacy_Trust_Energy_LLM.md]. Pilot testing the combined survey instrument is crucial to ensure its overall clarity, flow, and preliminary psychometric properties within the target lay population assessing LLMs for household energy decisions [Biagini et al., Assessing AI Literacy; DeWaters and Powers, 2011; DeWaters et al., 2013; gemDR_AI Literacy_Trust_Energy_LLM.md; Ng et al., 2024; survey_eval.md]. While self-report measures are practical for large-scale surveys, acknowledging their limitations and potential biases is important, as is considering multi-method approaches where feasible [DeWaters et al., 2013; gemDR_AI_Energy_Surveys2.md].

In conclusion, building a survey with robust construct validity for examining the relationship between literacy, trust, and AI use in a domain like household energy management requires a multifaceted measurement strategy. This strategy involves selecting validated scales for general psychological constructs and broad literacies while also developing and validating custom measures specifically tailored to the unique aspects of the AI system (LLM) and the application domain (energy management). This approach aligns with psychometric theory and the evolving research landscape in human-AI interaction and domain-specific literacies.


----











# https://aistudio.google.com/prompts/1M9tcpP-PiC9X3QmK3RCK9lIiGlJr7nLU

---

## Literature Review: Construct Validity and Measurement Approach for Assessing Lay Perceptions of LLMs in Household Energy Management

### Introduction

The proposed study investigates lay perceptions regarding the accuracy, utility, and trustworthiness of Large Language Models (LLMs) for household energy management tasks. This research is situated at the confluence of established findings on human cognitive limitations in energy decision-making, the burgeoning field of human-AI interaction focusing on trust and reliance, and the critical role of individual differences (specifically numeracy, energy literacy, and AI literacy) in shaping technology perception and use. This review outlines the theoretical and empirical basis for the constructs included in the proposed survey, justifying the selection and adaptation of measurement instruments to ensure construct validity and address the specific research questions and hypotheses.

### Misperceptions and Cognitive Heuristics in Energy Decision-Making

A substantial body of research highlights systematic inaccuracies in public understanding of household energy consumption. Attari et al. (2010) famously demonstrated that individuals concurrently underestimate the energy consumption of high-impact appliances and activities (e.g., heating, cooling, driving) while overestimating the savings from low-impact, often more visible, curtailment behaviors (e.g., turning off lights). This pattern suggests a reliance on cognitive heuristics—mental shortcuts such as availability (judging based on easily recalled examples), anchoring (relying heavily on initial information), or affect (judging based on feelings)—rather than deliberate calculation or accurate knowledge (Kantenbacher & Attari, 2021; Schley & DeKay, 2015). Novice users often employ flawed "rules of thumb," for instance, associating device size or runtime duration linearly with energy use, which may not hold true (van den Broek & Walker, 2019; Kantenbacher & Attari, 2021).

These findings align with theories of **bounded rationality** (Simon, 1957), suggesting that cognitive limitations and incomplete information lead individuals to "satisfice" rather than optimize their energy-related decisions (Blasch et al., 2019). The consequence is often suboptimal conservation choices and potentially misguided support for energy policies (Marghetis et al., 2019).

**Measurement Approach Justification:**

*   **Part 1 (Household Energy Perceptions - Attari et al., 2010 adaptation):** Including relative estimation tasks directly assesses participants' baseline adherence to the specific misperception patterns documented in the foundational literature. This allows for testing H1 (comparing perceived LLM accuracy patterns to known human biases) and provides a crucial baseline for interpreting other findings.
*   **Part 3A (Perceived LLM Accuracy):** Custom items directly probe beliefs about LLM accuracy specifically for tasks known to be difficult for humans (appliance estimation, savings calculation). Items 3A.3 vs. 3A.4 are designed to explicitly test whether users perceive LLMs as overcoming the human bias of overvaluing curtailment vs. efficiency actions (relevant to H1).

### The Role of Literacies in Information Processing and Evaluation

Individual differences in knowledge and cognitive skills are critical determinants of how people seek, interpret, evaluate, and utilize complex information, including that provided by AI systems.

**Energy Literacy:** Defined as a multidimensional construct encompassing cognitive knowledge, affective attitudes (values, concerns), and behavioral patterns related to energy (DeWaters & Powers, 2011; Cotton et al., 2021). Higher cognitive energy literacy provides the necessary domain expertise to potentially evaluate the plausibility and relevance of LLM-generated energy advice. Affective components relate to motivation and concern, while behavioral components reflect existing engagement (Mei-Shiu et al., 2018).

*   **Measurement Approach Justification (Part 6 - Energy Literacy):** The survey includes selected items representing all three dimensions, adapted from the widely used ELQ framework (DeWaters & Powers, 2011, 2013). Using objective multiple-choice items for cognitive knowledge (Part 6B) provides a robust measure of domain understanding needed to test H2b (critical evaluation). Self-report Likert scales for affective and behavioral dimensions (Part 6A) capture attitudes and current practices.

**AI Literacy:** This involves understanding AI principles, capabilities, limitations, ethical considerations, and the skills needed for critical interaction (Long & Magerko, 2020; Carolus et al., 2023). Understanding that LLMs can "hallucinate" or generate biased outputs based on training data is crucial for calibrating trust and reliance (Choe et al., 2024; Puppart & Aru, 2025). However, the relationship between AI literacy and reliance is complex; some studies find increased literacy can *reduce* over-reliance (Morrill & Noetel, 2023), while others suggest higher perceived competence might paradoxically *decrease* critical behaviors like fact-checking (Rheu & Cho, 2025).

*   **Measurement Approach Justification (Part 7 - AI Literacy):** The MAILS-Short scale (Koch et al., 2024) is employed as a validated, concise measure of self-perceived AI literacy across multiple relevant facets (e.g., Understand AI, Detect AI, AI Ethics, Self-Efficacy). This directly addresses the construct validity requirements for testing H2c, allowing examination of how different literacy facets relate to beliefs about LLM capabilities in the energy domain.

**Numeracy:** Defined as the objective ability to comprehend, use, and interpret fundamental numerical and statistical concepts (Cokely et al., 2012; Peters et al., 2006). It is distinct from mathematical self-efficacy or anxiety and is crucial for processing quantitative information prevalent in energy contexts (kWh, cost savings, percentages). Lower numeracy is linked to poorer comprehension and decision-making in various domains, including health and finance, and is expected to impact the evaluation of quantitative energy information, whether from humans or AI (Marghetis et al., 2019).

*   **Measurement Approach Justification (Part 8 - Objective Numeracy):** The survey includes objective items adapted from the Berlin Numeracy Test and Cognitive Reflection Test, commonly used in energy perception research (Marghetis et al., 2019; Attari et al., 2010). These items provide a validated measure of the fundamental quantitative reasoning skills needed to assess the plausibility of LLM energy estimates, directly addressing H2a.

### Human-AI Trust, Reliance, and LLM Perceptions

Trust is a core construct in human-AI interaction, defined as "the attitude that an agent will help achieve an individual’s goals in a situation characterized by uncertainty and vulnerability" (Lee & See, 2004). It is multidimensional, typically involving perceptions of the AI's **ability** (competence, reliability), **benevolence** (intentions towards the user), and **integrity** (adherence to principles like honesty, fairness) (Mayer et al., 1995; Glikson & Woolley, 2020). **Trust calibration** – aligning trust levels with the AI's actual capabilities for a specific task – is crucial for effective and safe interaction, preventing both misuse (over-reliance on faulty AI) and disuse (under-reliance on capable AI) (Buçinca et al., 2021; Lee & See, 2004).

Recent work distinguishes between general **propensity to trust** technology (Scholz et al., 2025) and **task-specific trust** developed through interaction or specific beliefs. Furthermore, **trust and distrust** may operate as distinct, co-existing constructs rather than bipolar opposites (Scharowski et al., 2025; Lai et al., 2024), necessitating separate measurement. For human-like LLMs, the distinction between **cognitive trust** (based on perceived capability and reliability) and **affective trust** (based on feelings of connection, empathy, or care) is also relevant (Shang & Hsieh, 2025; Duro et al., 2025).

LLMs present unique challenges due to their fluency, which can inflate perceptions of competence, and their susceptibility to generating confident but incorrect information ("hallucinations"), making trust calibration difficult (Kim et al., 2024). Users' mental models of *how* LLMs work (e.g., retrieving facts vs. generating probabilistic text) can significantly influence their expectations and trust (Chen et al., 2025).

**Measurement Approach Justification:**

*   **Part 3 (Beliefs about LLMs for Energy Tasks):** Directly measures perceived accuracy and utility (cognitive antecedents of trust) and task-specific trust for the core energy tasks, allowing assessment of beliefs related to RQ1, H1, H3.
*   **Part 4 (LLM Usage):** Captures behavioral intention and self-reported past behavior specifically for energy tasks, serving as key outcomes for RQ3, H3, H4.
*   **Part 5 (General Trust in AI):** Uses the TPA scale (Jian et al., 2000), a standard instrument measuring general trust/distrust facets (reliability, dependability, confidence vs. suspicion, wariness, deceptiveness). This captures the broader trust disposition relevant to RQ4 and H4's mediation/moderation hypotheses. While TPA-Revised factor structures are debated (Scharowski et al., 2025), using the full standard scale allows for flexible analysis based on emerging consensus.
*   *(Self-Correction Note: While the Shang et al. (2025) semantic differential scale offers explicit cognitive/affective measurement, the TPA was retained for better coverage of standard trust/distrust facets within runtime constraints. The TPA implicitly covers key cognitive aspects like reliability and competence).*

### Synthesis and Conclusion

The proposed survey instrument integrates measures grounded in established theories and empirical findings relevant to energy decision-making, cognitive literacies, and human-AI trust. By combining validated scales for individual differences (numeracy, energy literacy, AI literacy) and general AI trust (TPA) with custom-designed items targeting specific beliefs and behaviors related to LLMs in the household energy domain, the survey is well-equipped to address the study's research questions and hypotheses. The measurement approach allows for testing the relationships between individual characteristics, perceptions of LLM accuracy/utility/trustworthiness for energy tasks, general AI trust, and the willingness to rely on these novel tools for managing household energy consumption. This comprehensive assessment provides a strong foundation for understanding the complex interplay of factors shaping the adoption and potential impact of LLMs in this critical application area.