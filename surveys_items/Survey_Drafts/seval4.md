




# https://gemini.google.com/gem/0784d6af1d0f/ea338acd148f97d1

## Review of AI Literacy, Energy Literacy, and Related Constructs

This review examines the constructs of Artificial Intelligence (AI) literacy and energy literacy, exploring their documented relationships with user trust, reliance, confidence, and cognitive biases. Furthermore, it critically appraises the instruments used to measure these constructs, drawing upon the provided literature.

### AI Literacy and Its Correlates

AI literacy is broadly conceptualized as the competencies required to critically evaluate, effectively communicate and collaborate with, and utilize AI technologies across various contexts (Long & Magerko, 2020[cite: 118412, 119399, 121109]; Ng et al., 2021b [cite: 117786]). However, the operationalization of AI literacy varies considerably across studies, highlighting a lack of consensus that impedes robust measurement and theoretical integration (Koch et al., 2024[cite: 38777]; Laupichler et al., 2023a[cite: 119736]; Ng et al., 2021b [cite: 117786]). Common dimensions include understanding AI concepts, usage skills, evaluation capabilities, and ethical awareness (Ng et al., 2021b [cite: 117786]), although the necessity of including the ability to *create* AI within general AI literacy remains debated (Carolus et al., 2023 [cite: 119884, 121282]).

**Relationship with Trust, Reliance, and Overreliance:**

* **Trust**: The relationship between AI literacy and trust is complex and perhaps paradoxical. Some findings suggest AI literacy positively predicts trust (Gillespie et al., 2023 [cite: 118420]), potentially because understanding fosters acceptance. However, interventions designed to increase AI literacy, particularly those highlighting AI limitations, have sometimes resulted in *decreased* trust (Morrill & Noetel, 2023 [cite: 31598, 118716]), suggesting literacy fosters critical appraisal and potentially better trust calibration rather than blind faith. The directionality likely depends on *what* aspects of literacy are measured or manipulated.
* **Reliance/Overreliance**: A critical concern in human-AI interaction is overreliance – the uncritical acceptance of AI outputs, even when incorrect (Buçinca et al., 2021[cite: 70816]; Kim et al., 2025 [cite: 71222]). Poor AI literacy is implicated as a key driver of overreliance (Morrill & Noetel, 2023 [cite: 118298, 118411]), potentially because users lack accurate mental models of AI capabilities and limitations (Bansal et al., 2019[cite: 74978]; Steyvers & Kumar, 2024 [cite: 73880]). Studies show that users unfamiliar with AI/ML are more likely to over-rely (Jacobs et al., 2021 [cite: 118417]), and brief AI/ML tutorials can reduce this tendency (Chiang & Yin, 2022[cite: 118418]; Morrill & Noetel, 2023 [cite: 118301]). However, the mediating mechanisms remain unclear; improvements in literacy do not always directly mediate reductions in overreliance (Morrill & Noetel, 2023 [cite: 118303]). Conversely, underreliance (failing to use beneficial AI advice) is also a concern (Schemmer et al., 2023 [cite: 73125]), though less studied in relation to literacy.
* **Confidence**: AI literacy, particularly self-perceived competence or self-efficacy, is often linked to users' confidence in interacting with AI (Carolus et al., 2023[cite: 121192]; Chai et al., 2021[cite: 116747]; Sing et al., 2021 [cite: 101262]). Higher self-efficacy generally predicts greater behavioral intention to learn or use AI (Chai et al., 2020 [cite: 101265, 120761]).
* **Automation Bias**: This bias, the tendency to favor automated system outputs over contradictory human inputs or vigilance (Mosier et al., 1998[cite: 178333]; Parasuraman & Riley, 1997 [cite: 53737, 178333]), is closely related to overreliance. Low AI literacy might exacerbate automation bias by hindering critical evaluation of AI outputs. While AI literacy is not directly tested against automation bias in the provided texts, the documented link between low literacy and overreliance suggests such a connection is plausible (Cummings, 2015[cite: 75042]; Dzindolet et al., 2003 [cite: 74392, 74448]).
* **Attitude**: AI literacy often correlates positively with positive attitudes towards AI and negatively with AI anxiety or fear (Ayanwale et al., 2022[cite: 101265, 101312, 120761]; Carolus et al., 2023[cite: 117032, 121307]; Sindermann et al., 2021 [cite: 100600, 100600]).

### AI Literacy Measurement Instruments

The measurement of AI literacy faces challenges, including the proliferation of unvalidated scales and the frequent reliance on self-report measures, which are susceptible to biases like overestimation of one's knowledge (the Dunning-Kruger effect) (Gerlich, 2025[cite: 176960]; Grassini, 2024[cite: 117410]; Kohn et al., 2021[cite: 53661]; Laupichler et al., 2023a[cite: 119736]; Weber et al., 2023 [cite: 119693, 119785]).

**Subjective Scales (Self-Assessment):**

* **MAILS (Meta AI Literacy Scale)**: A comprehensive scale assessing self-perceived AI literacy (Use & Apply, Know & Understand, Detect AI, AI Ethics) and related psychological competencies (AI Self-Efficacy, AI Self-Competency). It aims for broad applicability but requires further validation (Carolus et al., 2023 [cite: 116927, 117028, 119884, 121066]). A short form (MAILS-S, 10 items) exists (Koch et al., 2024 [cite: 31166, 38785, 98855, 118093]).
* **AILQ (AI Literacy Questionnaire)**: Developed by Ng et al. (2024) based on their conceptual framework, covering cognitive, affective, behavioral, and ethical learning dimensions (Ng et al., 2024 [cite: 31188, 31682]).
* **SNAIL (Scale for the assessment of non-experts’ AI literacy)**: An exploratory factor analysis yielded three factors (Interaction Skills, Interpretation Skills, Ethical Awareness) from an initial item set targeting non-experts (Laupichler et al., 2023a [cite: 117721, 117752, 118142]).
* **PAILQ-6 (Perceived Artificial Intelligence Literacy Questionnaire)**: A concise 6-item scale focusing on self-perceived knowledge, demonstrating good psychometric properties in initial validation (Grassini, 2024 [cite: 31599, 117410, 117500]).
* **SAIL4ALL (Scale of Artificial Intelligence Literacy for all)**: Measures AI literacy across four themes: What is AI?, What can AI do?, How does AI Work?, How should AI be used? (Soto-Sanfiel et al., 2024 [cite: 31600, 31696]).
* **Ayanwale et al. (2022) Scales**: Adapted items from Chai et al. (2020, 2021) to measure teacher confidence, perceived usefulness, AI anxiety, attitude, relevance, and readiness regarding teaching AI[cite: 101110, 101296, 120606, 120792].
* **Other Self-Report Scales**: Various other scales exist, often developed for specific contexts (e.g., MAIRS-MS for medical students [cite: 116854, 121145]) or lacking rigorous validation (Koch et al., 2024 [cite: 38796, 118099]). Morrill & Noetel (2023) adapted Laupichler et al.'s items into a short-form subjective scale[cite: 31680, 118569].

**Objective Scales (Knowledge/Skill Tests):**

* **AICOS (AI Competency Objective Scale)**: Developed to address limitations of subjective scales, AICOS measures factual knowledge across facets like Apply AI, Create AI, Detect AI, Ethics AI, Generative AI, and Understanding AI (Markus et al., 2025 [cite: 31571, 31716, 117962, 118000]). A short version is also proposed[cite: 118006].
* **Weber et al. (2023) Objective Scale**: Assesses objective knowledge, distinguishing between socio-technical and user/creator dimensions[cite: 31701, 118577, 120171]. Morrill & Noetel (2023) adapted two items from this scale[cite: 118577].
* **GLAT (Generative AI Literacy Assessment Test)**: An initial 25-item test focusing specifically on generative AI literacy (Jin et al., 2024 [cite: 30407, 31726, 116735, 121064]).
* **Intervention-Specific Tests**: Many studies use custom knowledge tests tailored to their specific educational intervention content, limiting generalizability (e.g., Morrill & Noetel, 2023[cite: 118576]; Puppart & Aru, 2025 [cite: 31681]).

**Critique**: The field suffers from a lack of standardized, validated objective measures. Self-report scales dominate but are prone to bias[cite: 116994, 119694, 120064]. Instruments like AICOS [cite: 118003] and the work by Weber et al. [cite: 120191] represent important steps towards objective assessment. There's a need to test the predictive validity of existing scales against behavioral outcomes like reliance or task performance (Koch et al., 2024 [cite: 118725]).

### Trust in AI / Automation

Trust is a multifaceted psychological state involving confident expectations about an agent's (human or AI) actions in situations involving risk and vulnerability (Adams et al., 2003[cite: 57151]; Mayer et al., 1995[cite: 55042, 56187, 57418, 73460, 74325]; Lee & See, 2004 [cite: 53664, 53739, 71168, 72005, 73153, 74327]). It involves both cognitive assessments (e.g., perceived competence, reliability, predictability) and affective components (e.g., feelings of confidence, warmth, integrity, benevolence) (Glikson & Woolley, 2020[cite: 37831, 72059, 74390, 177990]; McAllister, 1995[cite: 53048]; Razin & Feigh, 2024 [cite: 40018, 40193]).

**Key Concepts**:

* **Trust vs. Reliance**: Trust is an attitude or psychological state, while reliance is the behavior of acting upon that trust (e.g., following AI advice) (Lee & See, 2004[cite: 71168, 72005, 73153, 74327]; Scharowski et al., 2025 [cite: 56041]). While correlated, the relationship isn't always direct; a user might trust an AI but still choose not to rely on it, perhaps due to high self-confidence (Lee & Moray, 1994 [cite: 57517, 71948, 72936]).
* **Trust Calibration**: This refers to the alignment between a user's trust level and the AI's actual capabilities or reliability for a specific task (Lee & See, 2004 [cite: 71168, 72005, 73153, 74327]). Miscalibration includes over-trust (leading to overreliance) and under-trust (leading to under-reliance)[cite: 30447, 30448]. Measuring calibration requires assessing both subjective trust and objective performance/accuracy[cite: 30449].
* **Trustworthiness Factors**: Dimensions often assessed include competence/ability, reliability/predictability, integrity, benevolence, and understandability/transparency (Hoff & Bashir, 2015[cite: 37762, 53664, 72141, 74326]; Mayer et al., 1995[cite: 53664, 55042, 56187, 57418, 73460, 74325]; Razin & Feigh, 2024 [cite: 40018, 40193]).
* **Dispositional vs. Task-Specific Trust**: Dispositional trust (or trust propensity) is a general tendency to trust technology or others (Mayer et al., 1995 [cite: 53664, 55042, 56187, 57418, 73460, 74325]), while task-specific trust relates to a particular AI in a specific context[cite: 30445]. Both influence behavior (Merritt & Ilgen, 2008 [cite: 54526, 54597, 72141]).
* **Trust vs. Distrust**: Some theories posit trust and distrust as independent constructs rather than bipolar opposites, potentially involving different underlying cognitive and affective processes (Chang & Fang, 2013[cite: 56239, 56295]; Lewicki et al., 1998[cite: 56126, 56152]; McKnight & Chervany, 2001 [cite: 56514]).

### Trust Measurement Instruments

Measuring trust in AI/automation is challenging due to its latent nature (Kohn et al., 2021[cite: 53738, 54118]; Razin & Feigh, 2024 [cite: 40018, 40193]). Reliance on a few popular, sometimes flawed or unvalidated, self-report scales is common (Kohn et al., 2021 [cite: 53661, 53888, 54118]).

**Self-Report Scales:**

* **Checklist for Trust Between People and Automation (TPA)**: (Jian, Bisantz, & Drury, 2000 [cite: 30395, 53900, 55183, 55994, 56208]). A widely used 12-item scale measuring trust/distrust. However, recent validation attempts in AI contexts suggest poor psychometric quality, questioning its suitability (Perrig et al., 2023 [cite: 31720, 55182]). Scharowski et al. (2025) found its data did not fit theoretical models well[cite: 55996].
* **Human-Computer Trust (HCT) Instrument**: (Madsen & Gregor, 2000 [cite: 31668, 52995, 53974, 57200, 100166]). A 25-item scale measuring five facets: perceived reliability, technical competence, understandability, faith, and personal attachment. Used less frequently but demonstrates factor structure[cite: 53975].
* **Merritt Scale (Trust in Automation)**: (Merritt, 2011 [cite: 53179, 54525, 117593]). Measures trust based on perceived competence, reliability, and helpfulness. Adapted in various studies[cite: 117593].
* **Trust in Automation (TiA) Questionnaire**: (Körber, 2018 [cite: 54842, 54960]). Measures facets like Reliability/Competence, Understanding/Predictability, Familiarity, Intention of Developers, Propensity to Trust, and overall Trust. Items listed in the document[cite: 31676]. Used in driving simulator studies[cite: 54876, 54919].
* **Trust Perception Scale – AI (TPA)**: (Lai et al., 2024, derived from Schaefer, 2016 [cite: 54744]). A revised scale distinguishing Trust and Distrust factors, showing better psychometric properties than the original TPA in AI contexts[cite: 30446, 55182]. Referenced as TPA-Revised[cite: 31169]. Items measure perceived competence, benevolence, and integrity.
* **Propensity to Trust in Automated Technology (PTT-A)**: (Schulz et al., 2025 [cite: 31685, 54179, 72111]). Measures dispositional trust specifically towards technology, distinguishing it from Propensity to Trust Humans (PTT-H). Validated scale with facets: Trusting Stance, Competence, Benevolence, Integrity[cite: 72114, 72235, 72377].
* **Trust of Automated Systems Test (TOAST)**: (Wojton et al., 2020 [cite: 54046]). Captures two facets, but details are limited in the provided text[cite: 54047].
* **Trust in AI Scale (TAIS)**: (Shang et al., 2024 [cite: 31687, 31732]). Developed through multiple studies, resulting in a final version with factors like Global Trust, Vigilance, Unbiasedness, Integrity, Transparency, Ability[cite: 31719, 56820].
* **General Attitudes towards Artificial Intelligence Scale (GAAIS)**: (Schepman & Rodway, 2020 [cite: 31721, 53280, 99102]). Measures positive and negative attitudes towards AI[cite: 121247].
* **Attitude Towards Artificial Intelligence (ATAI) Scale**: (Sindermann et al., 2021 [cite: 31690, 100600, 100800]). A short scale assessing positive (Acceptance) and negative (Fear) attitudes towards AI[cite: 100603, 100901]. AIAS-4 is a similar 4-item scale (Grassini, 2023 [cite: 31601, 39368, 98453]).
* **TILLMI (Trust-In-LLMs Index)**: (Duro et al., 2025 [cite: 31670]). Developed specifically for LLMs, a final 6-item version measures affective and cognitive trust[cite: 31671].
* **Other Scales**: Various other scales are mentioned, including the Complacency-Potential Rating Scale (Singh et al., 1993 [cite: 30395, 57201]), SHAPE Automation Trust Index (SATI) (Goillau et al., 2001 [cite: 31668, 57201]), scales measuring trust in specific technologies like chatbots or SDVs (Scharowski et al., 2025 [cite: 31683, 55992]), and adapted scales for specific contexts (e.g., TXAI for Explainable AI, Perrig et al., 2023 [cite: 31720]).

**Behavioral Measures:**

* **Reliance Metrics (JAS Paradigm)**: Measuring reliance using a Judge-Advisor System (JAS) framework, where a human makes an initial decision, sees AI advice, and makes a final decision (Bonaccio & Dalal, 2006[cite: 38453, 53521, 73120]; Sniezek & Van Swol, 2001 [cite: 73817]).
    * *Switch Fraction*: Proportion of trials where the user changes their initial decision to match the AI's advice[cite: 177157, 177490].
    * *Agreement Fraction*: Proportion of trials where the user's final decision matches the AI's advice[cite: 177157, 177490].
    * *Appropriateness of Reliance (AoR)*: Metrics like Relative AI Reliance (RAIR - switching to correct AI advice) and Relative Self-Reliance (RSR - sticking with correct self-decision against incorrect AI advice) assess calibration (Schemmer et al., 2023 [cite: 73126, 176684, 176705]).
    * *Technology Dominance*: Uses odds ratios to quantify detrimental algorithmic aversion (under-reliance) and automation bias (over-reliance) (Cabitza et al., 2023 [cite: 176687, 176704]).
* **Monitoring Behavior**: Frequency or duration of monitoring the automated system (Muir & Moray, 1996[cite: 53397, 57436, 57468]; Riley, 1994 [cite: 6632, 52957]). Can indicate low trust (defensive monitoring) or complacency (low monitoring despite risk) (Adams et al., 2003 [cite: 53012]).
* **Trust Games**: Measuring behavioral trust through economic games where participants invest resources based on trust in an automated agent (Schulz et al., 2025 [cite: 31685, 54179, 72111]).

**Critique**: The field relies heavily on self-report scales, particularly the TPA (Jian et al., 2000), despite its questionable validity in AI contexts[cite: 55184, 55994, 56209]. There is a need for greater use of validated, multi-dimensional scales (e.g., PTT-A, TAIS, TPA-Revised, HCT) and behavioral measures (e.g., AoR metrics) to capture the complexity of trust and reliance (Kohn et al., 2021[cite: 53661, 53888, 54118]; Razin & Feigh, 2024 [cite: 40018, 40193]). Measuring trust miscalibration requires comparing subjective trust/confidence with objective performance[cite: 30449, 30451], ideally using objective calibration tasks[cite: 30452].

### Energy Literacy and Appliance Knowledge

Energy literacy encompasses the knowledge, attitudes, and behaviors related to energy production, consumption, and its environmental/societal impacts (DeWaters & Powers, 2011[cite: 210, 241, 866, 918, 146642, 147297, 148999]; DeWaters et al., 2013 [cite: 31524, 31711]). A key component is *device energy literacy* – understanding the relative energy consumption of household appliances and activities (Attari et al., 2010[cite: 412, 75000, 118294, 146843, 148433, 148492, 148723]; Van Den Broek & Walker, 2019 [cite: 2516, 148947]).

**Key Findings & Concepts**:

* **Low Public Knowledge**: Numerous studies indicate that general energy literacy is low (Bittle et al., 2009[cite: 830, 916, 147261]; NEETF, 2002 [cite: 830, 916, 147261]), with people often overestimating their knowledge [cite: 831, 859, 147262] and being unable to identify major energy sources or impacts[cite: 834, 862, 147265].
* **Appliance Misperceptions (Range Compression)**: People systematically underestimate the energy use of high-consumption appliances (e.g., heating/cooling, water heaters) and overestimate low-consumption ones (e.g., lighting, phone chargers), a phenomenon termed range compression (Attari et al., 2010[cite: 412, 75000, 118294, 146843, 148433, 148492, 148723, 149005]; DeKay et al., 2014[cite: 2002, 2068, 2178, 2286, 148433, 148498, 148500]; Marghetis et al., 2019 [cite: 31714, 39616]). This leads to prioritizing ineffective conservation actions (e.g., turning off lights) over high-impact ones (e.g., efficiency upgrades) (Attari, 2021[cite: 75076, 75097]; Dietz et al., 2009[cite: 2000, 2128, 75083, 148431]; Lundberg et al., 2019 [cite: 75188]).
* **Cognitive Heuristics**: Energy judgments often rely on flawed heuristics (simple mental shortcuts) rather than accurate calculation (Van Den Broek & Walker, 2019 [cite: 2516, 148947]). Examples include judging energy use based on:
    * *Cognitive Accessibility*: How often one interacts with or thinks about a device (e.g., lights seem high-use because they are frequently used/visible) (DeKay et al., 2014 [cite: 1994, 2006, 2058, 148425, 148437, 148489]).
    * *Heat Production*: Devices that get hot are perceived as high-energy (Kantenbacher & Attari, 2021[cite: 37911, 38035, 70474]; Van Den Broek & Walker, 2019 [cite: 2706, 149137]).
    * *Size*: Larger appliances are judged as using more energy (Baird & Brier, 1981[cite: 2578, 2641, 149009, 149011, 149072]; Van Den Broek & Walker, 2019 [cite: 2706, 149137]).
    * *Usage Time/Frequency*: How long or often a device is used (DeKay et al., 2014[cite: 1994, 2299]; Van Den Broek & Walker, 2019 [cite: 2706, 149137]).
    * *Other Novice Heuristics*: E.g., devices with energy labels use more energy; charging devices use more energy (Attari, 2021 [cite: 75111]).
* **Expert Heuristics**: Energy experts utilize more accurate heuristics related to device function (e.g., heating/cooling), components (e.g., motors), and observable cues (e.g., requires cooling) (Kantenbacher & Attari, 2021 [cite: 37911, 37915, 38030, 38113, 70474, 70477]). Interventions teaching expert heuristics can improve novice estimates (Marghetis et al., 2019 [cite: 39616, 39693, 75109, 150293]).
* **Role of Numeracy and Literacy**: Higher numeracy (understanding numerical concepts) is consistently linked to more accurate energy perceptions (Attari et al., 2010[cite: 146433]; DeKay et al., 2014[cite: 2062, 2291]; Frederick et al., 2011 [cite: 2575]). Similarly, higher energy/investment literacy correlates with better appliance choices and decision strategies (Blasch et al., 2017[cite: 414, 146845, 146955]; Blasch et al., 2019[cite: 415, 529, 146602, 146846, 146958]; Blasch et al., 2022 [cite: 405, 146836]). Presenting energy costs in monetary units (vs. kWh) aids comprehension and efficient choice (Blasch et al., 2019 [cite: 171, 267, 295, 146602, 146698, 146726, 146790]).
* **Load Shape Awareness**: Understanding *when* energy is used (load shape) is becoming critical with time-of-use pricing, yet awareness is low (Canfield et al., 2021 [cite: 3119, 3156]). This concept extends energy literacy beyond just *how much* energy is used[cite: 3127, 3164].

### Energy Literacy Measurement Instruments

Measuring energy literacy faces similar challenges to AI literacy, relying heavily on surveys assessing knowledge, attitudes, and self-reported behaviors.

* **Energy Literacy Questionnaire (ELQ)**: Developed by DeWaters & Powers (2008/2011/2013) for secondary students. It assesses cognitive (knowledge across various energy topics), affective (attitudes, values, self-efficacy), and behavioral domains (DeWaters & Powers, 2011[cite: 211, 866, 918, 146642, 147297, 148999]; DeWaters et al., 2013 [cite: 31524, 31711, 147349, 147387]). Validated and shown to have acceptable internal consistency[cite: 919, 951, 147350, 147382].
* **Attari et al. (2010) Survey**: Assessed public perceptions of energy consumption and savings across various domains (household devices, transportation, recycling) using estimation tasks (e.g., energy used in one hour, % savings from actions), attitude scales (e.g., NEP, climate change beliefs), self-reported behaviors, and numeracy questions[cite: 408, 464, 118294, 146431, 148433]. Critiqued for potential anchoring effects due to reference points (Frederick et al., 2011 [cite: 2575, 149006]).
* **Blasch et al. (2017, 2019, 2022) Measures**: Utilized surveys and online experiments assessing energy and investment literacy (often through calculation tasks or knowledge questions), decision-making strategies (heuristic vs. optimization), and choice efficiency in appliance selection scenarios[cite: 414, 415, 406, 146603, 146837].
* **Energy/Appliance Knowledge Tests**: Objective tests assessing knowledge of specific energy facts (e.g., NEETF, 2002[cite: 831, 859, 147262]; Bittle et al., 2009 [cite: 830, 916, 147261]) or relative appliance consumption (e.g., ENLITEN energy game pairwise comparison task; Herrmann et al., 2018 [cite: 1384, 147815]).
* **Load Shape Awareness Measure**: Assessed by comparing self-reported typical daily electricity use patterns (via graphical selection or drawing) with actual hourly meter data (Canfield et al., 2021 [cite: 3119, 3156]).
* **Heuristic Elicitation/Assessment**: Qualitative methods like group ranking tasks with think-aloud protocols (Van Den Broek & Walker, 2019 [cite: 2640, 149071]) or expert interviews (Kantenbacher & Attari, 2021 [cite: 37911, 38113, 70474]) identify heuristics. Surveys can assess self-reported use of specific heuristics (Van Den Broek & Walker, 2019 [cite: 2818, 149249]).

**Critique**: Energy literacy measurement often relies on self-reported behaviors and attitudes, which may be subject to social desirability bias (Luyben, 1982[cite: 3326, 3553, 3875, 149757, 149985, 150306]; Abrahamse et al., 2005 [cite: 149984]). Knowledge assessment varies from broad surveys to specific appliance estimation tasks, with methodological concerns like anchoring effects needing consideration[cite: 2575, 149006]. Assessing underlying cognitive processes like heuristic use often requires qualitative or specialized experimental methods[cite: 2640, 37911, 149071].

### Summary and Integration

Both AI and energy literacy are crucial for navigating modern technological and environmental challenges. Research indicates that literacy in both domains influences user perceptions, trust, and behavior, though often mediated by cognitive biases and heuristics. Measurement is a significant challenge, with a frequent reliance on self-report scales of uncertain validity and a need for more objective, behaviorally grounded assessments. Trust in AI is particularly complex, involving multiple dimensions and calibration challenges, where AI literacy plays a potentially critical role in mitigating overreliance. Energy literacy suffers from widespread public misperceptions rooted in poor mental models and faulty heuristics, impacting conservation efforts. Future research should focus on developing validated, multi-method measures for both constructs and exploring interventions (like providing expert heuristics or calibrated AI feedback) that improve literacy and promote well-calibrated trust and effective decision-making. The interplay between these domains, especially as AI is increasingly used for energy management advice[cite: 30230], warrants further investigation, particularly concerning whether AI can help overcome human energy biases or if users project their own biases onto AI[cite: 30644, 31053].


-----

# https://gemini.google.com/gem/0784d6af1d0f/7e974406bba25479

## Critical Analysis of the Research Proposal and Survey Draft

This analysis evaluates the proposal's problem definition, conceptual framework, survey design, novelty, and justification, focusing on areas for improvement and potential challenges.

### 1. Analysis of the Problem: Nature, Scope, and Core Requirements

* **Nature:** The proposal correctly identifies a critical intersection: lay understanding of household energy use, known to be flawed by cognitive biases (Attari, DeKay, Davidson, & Bruine de Bruin, 2010; Kantenbacher & Attari, 2021), and the increasing use of LLMs as information sources, which themselves have variable accuracy and can induce miscalibrated trust[cite: 2, 378, 381, 382, 603, 750, 776, 777, 778, 779, 780]. The core issue is the potential for a "dual-bias environment" where human energy misperceptions interact with LLM unreliability, potentially leading to suboptimal energy decisions[cite: 780].
* **Scope:** The scope focuses specifically on *lay perceptions* of LLM accuracy/utility for *household energy management tasks* (estimation and planning)[cite: 1, 168, 378, 661]. This is appropriately narrowed. However, it implicitly assumes users *can* form stable perceptions of LLM accuracy in this domain, which might be challenging given the abstract nature of energy and the opacity of LLMs[cite: 780, 870]. The proposal primarily focuses on individual cognitive factors (literacies, numeracy) and trust, potentially underemphasizing socio-cultural or contextual influences on energy decisions and AI adoption (Wilson & Dowlatabadi, 2007)[cite: 10833, 10946].
* **Core Requirements:** The central requirement is understanding the relationship between individual differences, beliefs about LLMs, trust calibration, and reliance intentions[cite: 2, 3, 170, 386, 603]. A key challenge, acknowledged later in the critiques within `proposal_survey_concat.pdf`, is the operationalization and measurement of these constructs, particularly "mental models" of LLMs and "trust calibration"[cite: 664, 839]. The initial proposal lacked direct measures of calibration, relying instead on subjective beliefs[cite: 664, 665].

### 2. Conceptual Framework

* **Integration:** The proposal aims to integrate theories of energy decision-making (bounded rationality, heuristics)[cite: 2, 381, 750], literacies (energy, AI, numeracy)[cite: 2, 666], and human-AI trust/calibration[cite: 2, 170, 386, 575]. This integration is relevant and necessary.
* **Deficiencies:** The initial framework (as critiqued in `proposal_survey_concat.pdf`) lacked explicit connections *between* these domains[cite: 837]. How exactly does AI literacy influence the *calibration* of trust specifically for energy tasks? How do pre-existing energy *biases* (Attari et al., 2010) interact with beliefs about *LLM* capabilities? The rewritten framework attempts to address this by positing that literacy enables better evaluation and that energy biases might be projected onto LLMs (H1b)[cite: 575, 739, 740].
* **Mental Models:** Measuring "mental models" of LLMs is identified as a core issue[cite: 821, 839]. The initial proposal lacked a robust approach. The rewritten sections propose a multi-pronged strategy: assessing objective knowledge (how LLMs work, limitations like hallucination), subjective beliefs (functional understanding, attributions, human-likeness), and behavioral correlates (calibration task performance, reliance choices)[cite: 478, 489, 491, 598, 599, 600, 601, 1058, 1071, 1072, 1073]. This is a significant improvement, moving beyond simple self-reports towards a more nuanced, operational definition grounded in cognitive science (Norman, 1987; Rouse & Morris, 1986) [cite: 42070, 42103] and relevant to Human-AI SMMs (Andrews et al., 2023)[cite: 42049]. However, truly capturing the dynamic, often implicit nature of mental models via surveys remains challenging (Hoffman et al., 2019)[cite: 55085]. Qualitative methods or more interactive tasks might be needed for deeper insights.

### 3. Evaluation of the Current Survey Draft (`proposal_survey_concat.pdf`)

* **Alignment with RQs:** The survey draft includes sections designed to measure:
    * RQ1 (Beliefs/Trust): Custom items on perceived LLM accuracy/utility for estimation vs. planning[cite: 661, 663]. *Weakness:* Initial draft lacked calibration measures[cite: 664]. Rewritten proposal adds an Objective Calibration Task[cite: 627].
    * RQ2 (Individual Diffs): Incorporates established/adapted scales for Numeracy (Berlin Numeracy Test), Energy Literacy (DeWaters & Powers, 2011; Attari et al., 2010), and AI Literacy (MAILS Short Form)[cite: 666, 1772, 1779, 1793, 40033]. *Strength:* Uses validated instruments where possible[cite: 667].
    * RQ3 (Reliance): Assesses self-reported likelihood of using LLMs for energy tasks and behavioral intention in scenarios[cite: 668]. *Weakness:* Relies on self-report; the added Objective Calibration Task provides a behavioral proxy[cite: 668, 600, 1072].
    * RQ4 (Trust Calibration/Uncertainty): Initial draft weak[cite: 669]. Rewritten proposal adds the Objective Calibration Task and suggests scenario-based manipulation of LLM confidence expression[cite: 1823, 627].
* **Construction:** The survey uses a mix of Likert scales, multiple-choice (for knowledge), and open-ended estimations (Attari task)[cite: 1772, 1779, 40251, 40424]. *Strength:* Triangulates constructs using different methods[cite: 601, 961, 1073]. *Weakness:* Length and potential for respondent fatigue, although efforts were made to use short forms[cite: 963]. The inclusion of attention checks is good practice[cite: 1508].
* **Measurement Issues:**
    * **Mental Models:** As discussed, measuring mental models via surveys is difficult. The proposed combination of knowledge, belief, and calibration items is a pragmatic approach[cite: 601, 1073].
    * **Trust Calibration:** The addition of the Objective Calibration Task (comparing user confidence judgments against LLM accuracy) is a crucial improvement[cite: 627, 727]. This directly measures miscalibration (Hoff & Bashir, 2015; Lee & See, 2004)[cite: 54549, 56208]. Scenario-based methods offer a less direct alternative[cite: 370].
    * **Energy Literacy:** Using established scales (DeWaters & Powers, 2011) [cite: 1779] and the Attari et al. (2010) estimation task provides validated cognitive and behavioral measures[cite: 1772].
    * **AI Literacy:** MAILS (Carolus et al., 2023; Koch et al., 2024) is a recent, validated scale covering multiple dimensions[cite: 1736, 1743, 40033, 40095, 99208]. Objective knowledge items supplement this self-report measure[cite: 1088].

### 4. Evaluation of the Study Proposal (`pd.pdf` and revisions in `proposal_survey_concat.pdf`)

* **Novelty:** The core novelty lies in examining the *interaction* between well-documented energy biases and the evaluation of LLMs[cite: 604, 739, 750, 819, 965]. The hypothesis that individuals might project their own energy estimation biases onto LLMs (H1b) is particularly innovative[cite: 740]. Investigating how different *literacies* moderate trust calibration in this specific, complex domain is also timely[cite: 820, 893].
* **Logical Structure:** The initial proposal's structure was somewhat implicit[cite: 837]. The rewritten RQs and Hypotheses provide a clearer, more logically connected flow[cite: 576]. For example, H1 addresses basic beliefs, H2 links these to individual differences, H3 connects beliefs/calibration to reliance, and H4 examines the moderating role of uncertainty presentation[cite: 577, 578, 579, 580].
* **Justification:** The rationale strongly grounds the research in the practical importance of energy conservation, the documented failures of human intuition in this domain (Attari et al., 2010; Kantenbacher & Attari, 2021)[cite: 4, 1772, 382, 750], the rapid rise of LLMs as information sources, and the established risks of miscalibrated trust in automation (Lee & See, 2004; Parasuraman & Riley, 1997)[cite: 386, 57887, 57889, 739, 876]. The proposal effectively argues for the *need* for this specific investigation[cite: 4, 602, 753, 877, 878]. The theoretical framework (cognitive biases, literacies, trust calibration) provides justification for the chosen constructs and hypotheses[cite: 2, 575].
* **Deficiencies:**
    * *Generalizability:* The reliance on online survey methods and potentially non-representative samples (common in online research, e.g., via Prolific [cite: 8334]) may limit generalizability. The proposal acknowledges this implicitly by focusing on *lay perceptions* rather than specific demographic groups.
    * *Ecological Validity:* The use of hypothetical scenarios and pre-written LLM responses, while necessary for experimental control, differs from real-world, dynamic interactions with LLMs[cite: 1815, 1817]. Users might interact differently with an LLM they query themselves.
    * *Causality:* The cross-sectional survey design primarily allows for correlational findings. While hypotheses posit predictive relationships (e.g., literacy -> calibration -> reliance), establishing causality requires experimental or longitudinal designs[cite: 838, 8556]. The scenario manipulations (accuracy, confidence tone) provide some causal insight[cite: 1817, 1823, 1826].

### 5. Suggestions for Improving Beliefs about LLM Accuracy Questions and Measuring Trust Miscalibration

* **Improving Beliefs Questions (Sec 3.1):**
    * *Specificity:* Instead of general accuracy/utility ratings, use more concrete tasks. Ask participants to rate the likely accuracy of an LLM providing a *specific* energy estimate (e.g., "How accurate is an LLM likely to be if it estimates a typical refrigerator uses 5 kWh per day?") or the utility of a *specific* piece of advice (e.g., "How useful is an LLM's advice if it suggests replacing all incandescent bulbs first to save energy?").
    * *Contrast High/Low Impact:* Include items covering both low-energy (e.g., phone charger) and high-energy (e.g., heating system) appliances/actions, mirroring the Attari et al. (2010) design[cite: 1772, 517]. This allows testing H1b (projection of human bias).
    * *Probe Underlying Beliefs:* Add questions tapping into *why* participants rate accuracy/utility as they do (their mental model)[cite: 489]. Examples: "LLM estimates are accurate because they access vast real-time data" vs. "LLM energy estimates might be inaccurate because their training data is outdated"[cite: 489, 491]. Assess beliefs about LLM limitations, like hallucination potential, specifically for energy facts[cite: 491].
* **Scenario-Based LLM Beliefs Questions:** Present short vignettes describing a user asking an LLM an energy-related question and receiving a specific answer. Then ask targeted belief questions:
    * *Scenario 1 (Estimation - Low Impact):* "You ask an LLM how much energy a typical phone charger uses per year. It replies 'about 3 kWh'. How accurate do you think this estimate is?" (Scale: Very Inaccurate - Very Accurate). Follow-up: "Is the LLM more likely to overestimate or underestimate the energy use of small devices?" [cite: 509]
    * *Scenario 2 (Estimation - High Impact):* "You ask an LLM about the annual energy use of central air conditioning. It replies 'around 3500 kWh'. How plausible does this number seem for an average home?" (Scale: Very Implausible - Very Plausible)[cite: 510, 775].
    * *Scenario 3 (Savings Explanation):* "An LLM explains why Energy Star appliances save energy: 'They use advanced components and better insulation, often consuming at least 50% less energy than standard models.' How accurate do you think the 'at least 50% less' part of this explanation is?" (Scale: Very Inaccurate - Very Accurate)[cite: 626].
    * *Scenario 4 (Personalized Plan):* "An LLM generates a personalized energy-saving plan for your home based on its size and location. How much would you trust this plan's *specific* recommendations?" (Scale: Not at all - Very Much)[cite: 514, 515].
* **Measuring Trust Miscalibration:**
    * *Objective Calibration Task (Included in Revised Proposal):* This is the strongest approach[cite: 627, 727]. Participants see actual (or simulated but realistic) LLM answers to energy questions (varying in correctness/bias) and rate their confidence in the LLM's answer being correct[cite: 593, 727, 1830, 1831]. Miscalibration = discrepancy between confidence and actual correctness (e.g., high confidence in wrong answers = over-trust). Calculate Brier scores or over/underconfidence indices[cite: 367, 627]. Correlate calibration scores with reliance intentions/behavior[cite: 368, 961].
    * *Subjective vs. Objective Knowledge Comparison:* Correlate participants' *perceived* LLM accuracy ratings (from improved Section 3.1 items) with their *own* performance/bias on the Attari task[cite: 573]. A positive correlation (e.g., those who overestimate savings from small actions also think LLMs are highly accurate at estimating savings from small actions) supports H1b[cite: 574].
    * *Scenario-Based Reliance Decisions:* In the scenarios above, include a behavioral choice (e.g., "Based on this LLM advice, would you: (a) Follow the advice, (b) Seek more info first, (c) Ignore the advice?")[cite: 373, 1833, 1834, 1835]. Compare choices across scenarios where the LLM advice (in the vignette) is manipulated to be correct vs. incorrect. High rates of "Follow" for incorrect advice indicates miscalibrated over-reliance[cite: 375, 856].

### 6. Novel Survey Ideas (Combining Attari + LLM Accuracy)

Building on Attari et al. (2010)[cite: 1772], Kantenbacher & Attari (2021)[cite: 1780], and Marghetis et al. (2019)[cite: 1781]:

1.  **Paired Estimation Task:**
    * *Phase 1:* Participants complete a standard Attari-style estimation task (e.g., estimate kWh/year for various appliances)[cite: 1772]. Calculate their individual bias score (e.g., slope of log(estimated) vs log(actual)).
    * *Phase 2:* Present the same appliances. For each, show a *purported* LLM estimate (manipulated to be accurate, overestimate, or underestimate, perhaps mirroring common human biases). Ask participants: "How accurate do you think this LLM estimate is?" (e.g., 7-point Likert). Ask: "How confident are you in your accuracy rating?" (e.g., percentage scale).
    * *Analysis:* Correlate participants' own estimation bias (Phase 1) with their perceived LLM accuracy pattern (Phase 2) to test H1b. Calculate objective calibration based on confidence vs. actual LLM estimate accuracy (manipulated). Examine if energy literacy/numeracy predict better calibration.
2.  **Choice Task with LLM Advisor:**
    * *Task:* Present pairs of energy-saving actions (like Marghetis et al., 2019 [cite: 1781]) (e.g., "Replace 1 incandescent bulb with LED" vs. "Turn off TV when not watching for a week"). Ask participants to choose the action that saves *more* energy.
    * *Manipulation:* Before choosing, participants receive advice from either (a) no advisor, (b) a human expert (brief description), or (c) an LLM (brief description). The advice suggests which action saves more (manipulated to be correct or incorrect).
    * *Measures:* Record participant's choice. Ask about confidence in their choice. Ask about trust in the advisor (if present).
    * *Analysis:* Does LLM advice improve choice accuracy compared to no advisor or human expert? Is reliance on LLM advice (indicated by choosing the LLM-suggested option) appropriate (i.e., higher for correct LLM advice)? Is reliance moderated by energy/AI literacy or trust? Does the LLM advice interact with known choice biases (e.g., preference for curtailment)?
3.  **Heuristic Elicitation + LLM Evaluation:**
    * *Phase 1:* Elicit participants' own energy-saving heuristics using open-ended questions or scenarios (similar to Kantenbacher & Attari, 2021's expert interviews, but adapted for laypeople)[cite: 1780]. Code these for accuracy (vs. expert heuristics).
    * *Phase 2:* Present participants with energy-saving tips purportedly generated by an LLM. Some tips reflect accurate heuristics, others reflect common novice misconceptions[cite: 1794]. Ask participants to rate the *usefulness* and *trustworthiness* of each tip.
    * *Analysis:* Do participants rate LLM tips aligned with their own (potentially inaccurate) heuristics as more trustworthy/useful? Does energy/AI literacy predict better discrimination between good and bad LLM-generated heuristic advice?

### 7. Annotated Bibliography: Validated Surveys (Relevant to Proposal)

Based on the provided `proposal_survey_concat.pdf` and supplementary files, here are 15 relevant validated or widely used instruments (summaries based on content in the files):

1.  **Energy Literacy Questionnaire (ELQ)** (DeWaters & Powers, 2011; DeWaters et al., 2013)[cite: 1779, 1559, 1560]: Measures energy literacy across cognitive (knowledge of concepts, sources, use, impacts), affective (attitudes, values), and behavioral (self-reported conservation actions) dimensions. Validated for secondary students but components adaptable for adults[cite: 8413, 8414, 8445, 8446, 1340]. *Relevance:* Core construct for RQ2/H2.
2.  **Attari et al. (2010) Energy Survey Items**[cite: 1772, 1087]: Includes quantitative estimation tasks for appliance energy use (kWh/year), energy savings from actions, and energy use for transport/manufacturing. Also assesses energy attitudes and self-reported behaviors. Widely used benchmark for energy knowledge/misperceptions[cite: 7873]. *Relevance:* Operationalizes objective energy knowledge gaps and biases (H1b).
3.  **Berlin Numeracy Test (BNT)** (Cokely et al., 2012)[cite: 1782, 1792]: Measures statistical numeracy and risk literacy using short practical problems. Available in various formats (open-ended, multiple-choice, adaptive)[cite: 1782, 1792]. *Relevance:* Measures objective numeracy (RQ2/H2).
4.  **Meta AI Literacy Scale (MAILS / MAILS-Short)** (Carolus et al., 2023; Koch et al., 2024)[cite: 1736, 1743, 40033, 40095, 98015, 122004]: Self-report scale measuring AI Literacy (Use/Apply, Know/Understand, Detect, Ethics), Create AI ability, AI Self-Efficacy (Learning, Problem-solving), and AI Self-Competency (Persuasion literacy, Emotion regulation). Developed based on competency models; short form available[cite: 40034, 40099, 40100, 99208, 121802, 122011]. *Relevance:* Core measure for AI Literacy (RQ2/H2).
5.  **Trust in Automation / Automated Systems Scales (Jian, Bisantz, & Drury, 2000; Adams et al., 2004)**[cite: 1735, 54422, 57707, 56929, 57609]: Measures trust/distrust in automated systems. Jian et al.'s scale [cite: 1735, 54422, 56929] has been widely used but psychometric concerns exist (e.g., factor structure, anthropomorphism)[cite: 56930, 56931, 57609, 57610, 57611]. Adams et al. provide a review[cite: 57707]. *Relevance:* Foundational scales for trust in automation (RQ1/RQ3/RQ4).
6.  **Trust between People and Automation (TPA) Scale** (Madsen & Gregor, 2000)[cite: 1735, 54422, 55190]: Measures human-computer trust across dimensions like Perceived Reliability, Technical Competence, Understandability, Faith, and Personal Attachment[cite: 1735, 41102]. *Relevance:* Captures specific facets of trust applicable to AI (RQ1/RQ3/RQ4).
7.  **Scale of Trust in Automated Systems (TOAST)** (Jessup et al., 2005 - referenced in Küper et al., 2025 [cite: 1771, 56575] and Razin & Feigh, 2024 [cite: 40661]): Measures trust based on performance, process, and purpose dimensions. Used in HRI contexts. *Relevance:* Alternative validated trust measure (RQ1/RQ3/RQ4).
8.  **Trust in AI Scale (TAIS)** (Asan et al., 2020 - referenced in Küper et al., 2025 [cite: 1787, 56575]): Measures trust in AI, reduced to dimensions like Global Trust, Vigilance, Unbiasedness, Integrity, Transparency, Ability[cite: 1787]. *Relevance:* AI-specific trust measure (RQ1/RQ3/RQ4).
9.  **Trust Perception Scale - AI (TPA-AI)** (Scharowski et al., 2025)[cite: 1750, 56927]: Adaptation/validation of Jian et al. (2000) scale[cite: 1735], finding a two-factor structure (Trust, Distrust)[cite: 56932]. *Relevance:* Recent validation attempt for a core trust scale (RQ1/RQ3/RQ4).
10. **Attitude Towards Artificial Intelligence (ATAI) Scale** (Sindermann et al., 2021)[cite: 1757, 1794, 97756, 117143, 118854]: Measures general positive and negative attitudes/affect towards AI[cite: 1757, 102056]. Validated in German, Chinese, English[cite: 1757]. *Relevance:* Assesses general predisposition towards AI (RQ2/H2).
11. **AI Attitude Scale (AIAS-4)** (Grassini, 2023)[cite: 1741, 1795, 98876, 118854]: Brief 4-item scale measuring general attitude toward AI[cite: 1741, 98882]. *Relevance:* Concise measure of general AI attitude (RQ2/H2).
12. **Need for Cognition (NCS-6)** (Cacioppo & Petty, 1982 - used in Lee et al., 2024 [cite: 1745, 40251]): Short scale measuring tendency to engage in and enjoy effortful cognitive activities. *Relevance:* Potential individual difference moderator (RQ2).
13. **Cognitive Reflection Test (CRT-2)** (Frederick, 2005 - used in Lee et al., 2024 [cite: 1745, 40251]): Measures tendency to override an incorrect intuitive response and engage in further reflection. *Relevance:* Potential individual difference moderator (RQ2).
14. **New Ecological Paradigm (NEP) Scale (Revised)** (Dunlap et al., 2000 - used in Marghetis et al., 2019 [cite: 1782, 40423]): Measures pro-environmental orientation and worldview. *Relevance:* Potential individual difference related to energy attitudes/behavior (RQ2).
15. **Propensity to Trust in Automated Technology (PTT-A)** (Schulz et al., 2025)[cite: 1753, 56300]: Measures dispositional tendency to trust automation across dimensions like Trusting Stance, Competence, Benevolence, Integrity[cite: 1753]. *Relevance:* Captures individual differences in trust propensity (RQ2/H2).

### 8. Annotated Bibliography: Relevant LLM Research Papers (Selected from provided files)

Here are 15 relevant papers concerning LLMs, summarized with their relevance to the proposal:

1.  **Kim, S. S. Y., Liao, Q. V., Vorvoreanu, M., et al. (2024). “I’m Not Sure, But...”: Examining the Impact of Large Language Models’ Uncertainty Expression on User Reliance and Trust.** [cite: 1015, 1471, 39502, 55246]
    * *Summary:* Experimentally investigates how different natural language expressions of uncertainty by LLMs affect user reliance and trust calibration. Finds first-person uncertainty expressions ("I'm not sure") reduce over-reliance on incorrect answers more effectively than general expressions[cite: 1016, 1166, 39504, 39662, 39665, 55249].
    * *Relevance:* Directly informs RQ4/H4 regarding the impact of LLM uncertainty communication on user trust and reliance. Provides empirical methods for testing uncertainty expression.
2.  **Steyvers, M., Kumar, A. A., Benjamin, C., et al. (2024). Measuring and modeling the perception of explanations from large language models.** [cite: 41517]
    * *Summary:* Examines how explanations from LLMs affect user perception of LLM reliability. Finds tailored explanations reflecting model confidence improve user calibration compared to default explanations[cite: 41519, 41521].
    * *Relevance:* Relevant to RQ1/H1 (beliefs about LLM accuracy) and RQ4 (trust calibration). Suggests explanation tailoring as an intervention. Also provides methods for measuring calibration (confidence judgments vs. actual accuracy)[cite: 41563].
3.  **Bansal, G., Nushi, B., Kamar, E., et al. (2019). Beyond accuracy: The role of mental models in human-AI team performance.** [cite: 42452, 75060, 118689, 148512]
    * *Summary:* Argues that alignment between a user's mental model of an AI system and the system's actual capabilities is crucial for effective human-AI team performance, beyond just AI accuracy. Explores how explanations influence mental model formation[cite: 75061, 118690].
    * *Relevance:* Directly addresses the proposal's focus on mental models and their link to reliance/performance. Supports the importance of measuring mental models and trust calibration.
4.  **Almashor, M., & Miyashita, Y. (2024). Can Private Llm Agents Synthesize Household Energy Consumption Data?** [cite: 11978]
    * *Summary:* Explores using LLM-driven agents in simulations to generate synthetic household energy consumption data, leveraging emergent behaviors to mimic human actions and overcome privacy issues of real data[cite: 11981, 11982, 11995, 11996]. Early results show promise in replicating consumption patterns[cite: 11984, 12083, 12091].
    * *Relevance:* While focused on data synthesis, it touches upon LLMs simulating human *behavior* related to energy, suggesting potential for LLMs to model energy decision-making itself, relevant to the proposal's domain.
5.  **Chen, A., Kim, S. S. Y., Dharmasiri, A., et al. (2025). Portraying Large Language Models as Machines, Tools, or Companions Affects What Mental Capacities Humans Attribute to Them.** [cite: 1463, 98179, 122173]
    * *Summary:* Investigates how framing LLMs (mechanistic, functional, intentional) via short videos affects the mental capacities lay individuals attribute to them. Finds intentional framing increases attribution, particularly for mind-related capacities[cite: 122176, 122210, 122211].
    * *Relevance:* Directly relevant to understanding and measuring lay mental models of LLMs. Shows how external portrayals shape beliefs, impacting trust and interaction.
6.  **Kim, S. S. Y., Vaughan, J. W., Liao, Q. V., et al. (2025). Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies.** [cite: 71578]
    * *Summary:* Through think-aloud and controlled experiments, identifies that LLM explanations increase reliance (including over-reliance), while providing sources and highlighting inconsistencies reduces over-reliance on incorrect answers[cite: 71579, 71581, 71607, 71609, 71610, 71683, 71684, 71685].
    * *Relevance:* Directly addresses factors influencing appropriate reliance (RQ3/H3) and trust calibration (RQ4/H4). Offers practical insights for designing LLM responses (e.g., emphasize sources).
7.  **Luo, X., Rechardt, A., Sun, G., et al. (2024). Large language models surpass human experts in predicting neuroscience results.** [cite: 73178]
    * *Summary:* Shows LLMs can predict outcomes of neuroscience experiments better than human experts by synthesizing vast literature, suggesting potential as scientific assistants but also highlighting their complex capabilities[cite: 73179].
    * *Relevance:* Demonstrates high-level LLM capabilities relevant to tasks requiring information synthesis, potentially applicable to generating energy plans (part of the proposal's scope). Underscores the difficulty lay users might have in accurately assessing LLM limits.
8.  **Park, J. S., O’Brien, J., Cai, C. J., et al. (2023). Generative Agents: Interactive Simulacra of Human Behavior.** [cite: 12117]
    * *Summary:* Creates simulated agents powered by LLMs in a sandbox environment that exhibit believable emergent social behaviors (e.g., organizing a party)[cite: 11997, 12027].
    * *Relevance:* Shows LLMs' potential for human-like behavior simulation, relevant to understanding how users might perceive LLM capabilities and form mental models based on interaction.
9.  **Lai, V., Ngo, V., Wang, H., et al. (2024). MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning.**
    * *Summary:* Proposes a framework where multiple LLM agents collaborate, simulating expert consultation to improve medical reasoning accuracy. Highlights the potential of multi-agent LLM systems [from `llm_concat_cleaned.pdf`, general topic].
    * *Relevance:* While domain-specific, it showcases advanced LLM applications and the concept of AI collaboration, which might influence user mental models about AI capabilities and trustworthiness.
10. **Robinson, I., & Burden, J. (2025). Framing the Game: How Context Shapes LLM Decision-Making.** [cite: 9658]
    * *Summary:* Introduces a framework to evaluate LLM decision-making by systematically varying context framing in game scenarios (Prisoner's Dilemma). Finds LLM responses show significant, predictable sensitivity to framing effects[cite: 9661, 9663, 9664].
    * *Relevance:* Highlights that LLM outputs are not fixed but context-dependent, relevant to understanding LLM reliability and how framing might influence energy advice provided by LLMs.
11. **Lee, H. P. H., Sarkar, A., Tankelevitch, L., et al. (2025). The Impact of Generative AI on Critical Thinking: SelfReported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers.** [cite: 147956]
    * *Summary:* Surveys knowledge workers on GenAI use. Finds higher confidence in GenAI correlates with less critical thinking effort, while higher self-confidence correlates with more. GenAI shifts critical thinking towards verification and integration[cite: 147957, 147970, 147981].
    * *Relevance:* Provides empirical data on how GenAI use affects cognitive effort and critical thinking, relevant to understanding reliance patterns and the need for AI literacy (RQ2, RQ3).
12. **Passerini, A., Palminteri, S., & Gureckis, T. M. (2024). Human–large language model interaction for explainable conceptual structure discovery.**
    * *Summary:* Proposes using LLMs interactively with humans to discover conceptual structures, potentially enhancing mutual understanding and leveraging complementary strengths [from `llm_concat_cleaned.pdf`, general topic].
    * *Relevance:* Explores human-LLM collaboration for knowledge tasks, relevant to how users might interact with LLMs for energy planning and how interfaces might support this.
13. **Zamfirescu-Pereira, J. D., Wong, R. Y., Hartmann, B., et al. (2023). Why Johnny can't prompt: How non-AI experts try (and fail) to engineer chat prompts.**
    * *Summary:* Studies how non-experts struggle with prompt engineering for chatbots, revealing gaps in their mental models of how LLMs work [from `llm_concat_cleaned.pdf`, general topic].
    * *Relevance:* Directly addresses the formation and accuracy of lay mental models of LLMs, a core theme of the proposal. Highlights challenges users face in effectively using LLMs.
14. **Jha, A., Li, W., Jia, R., et al. (2023). SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models.** [cite: 148632, 148682]
    * *Summary:* Presents a large benchmark for measuring stereotypes in LLMs across 178 countries, finding significant biases[cite: 148632]. Uses generative probing.
    * *Relevance:* Demonstrates methods for probing LLM biases (relevant methodologically) and highlights the prevalence of biases that could affect energy advice fairness or accuracy.
15. **Santurkar, S., Durmus, E., Ladhak, F., et al. (2023). Whose Opinions Do Language Models Reflect?** [cite: 148666]
    * *Summary:* Investigates how well LLMs reflect opinions of different demographic groups (using US survey data). Finds models often align more with majority or high-socioeconomic status views [from `llm_concat_cleaned.pdf`, general topic].
    * *Relevance:* Highlights potential biases in LLM outputs that could skew energy advice or plans, relevant to trust and fairness considerations in the proposal's domain.



---------

# https://chatgpt.com/g/g-p-680c9643408c8191a128bbb19854ccee-ai-brainstorm/c/6816d209-4304-8006-9518-b13a8e2b8ee5


**Analyzing the research proposal**

To evaluate the user's research proposal, I need to start by identifying deficiencies, inaccuracies, or missed opportunities. I'll review how well the survey draft addresses the research questions, particularly focusing on critical areas like energy literacy, AI literacy, and human-AI reliance. I'll also consider how novel, logically structured, and justified the study is, especially when it comes to measuring mental models of LLMs. Finally, I'll suggest potential scenario-based questions to test LLM beliefs effectively.

**Creating a comprehensive research framework**

I need to help the user craft a rigorous research proposal by integrating past studies, specifically Attari and others, with a novel survey design. The proposal will look at perceptions of LLM accuracy for energy tasks and trust miscalibration. I'll suggest improvements to survey questions, provide an annotated bibliography of the 25 most relevant surveys and research papers on LLMs, and critique the current study proposal. I'll use the uploaded literature on energy heuristics and AI trust, making sure to reference everything properly in APA style.


Thought for 31 seconds


**1 Concise analysis of the problem, its scope, and core requirements**

Household-energy choices are numerically-complex, probabilistic, and highly contextual; lay users display systematic range-compression errors and anchoring on salient but low-impact actions (e.g., “turn off lights”) .
LLMs can supply rapid appliance-level estimates or tailored conservation plans, but they are vulnerable to hallucinations, training-data biases, and brittle numeracy. Recent evidence shows a **calibration gap**: users’ confidence in LLM answers substantially exceeds true model accuracy unless uncertainty is explicitly communicated (Steyvers et al., 2025) .
A valid study therefore must simultaneously (a) quantify baseline misconceptions, (b) capture mental-model beliefs about what LLMs “know,” (c) measure *trust mis-/over-calibration* at the answer level, and (d) test design levers (uncertainty phrasing, explanations, prior literacy) that may improve calibrated reliance under real-world constraints of limited attention, device screens, and time.

---

**2 Integrative conceptual framework**

```
Individual factors          AI factors                     Interaction factors
┌───────────────┐         ┌────────────────┐              ┌────────────────────────┐
│ • Numeracy    │         │ • Claimed acc. │──expressed→  │ Perceived AI reliability│
│ • Energy &    │         │ • Uncertainty  │              ├────────────────────────┤
│   AI literacy │  read   │ • Explanation  │              │ Trust calibration gap   │
│ • Cognitive   │────────▶│ • Prior examples│────────────▶│ (confidence – accuracy) │
│   style/bias  │         └────────────────┘              ├────────────────────────┤
│ • Aff. trust  │                                          │ Reliance / override     │
└───────────────┘                                          └────────────────────────┘
        │                                                     ▲
        └─► *Mental model of LLM competence & limits* ◄───────┘
```

*Transactive memory* logic (who/what “knows” what) controls **information allocation**; *prospect-theoretic* risk weighing governs **decision acceptance** under uncertainty; *dynamic influence systems* describe how discussion plus LLM input shape the group answer .

---

**3 Critique of the current survey draft & proposal**

| Area                              | Deficiencies / missed opportunities                                                                                                                                           | Recommendations                                                                                                                |
| --------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------ |
| **Alignment with RQs**            | Item blocks are not explicitly mapped to RQ1–RQ4; key construct “LLM mental-model accuracy” is absent.                                                                        | Add a construct-by-RQ matrix; insert an *AI Knowledge Boundary* battery (“What kinds of facts can the model update *today*?”). |
| **Novelty & literature fit**      | Proposal cites Attari (2010) but omits follow-ups on expert heuristics and magnitude scaling (Kantenbacher & Attari 2021) ; ignores recent AI-literacy scales (MAILS, GLAT) . | Situate hypotheses against those findings; include AI-literacy short forms for moderation tests.                               |
| **Trust calibration measurement** | Uses global Likert trust only; cannot compute (Confidence – Accuracy) gap.                                                                                                    | Embed an *objective calibration component* (see Section 4).                                                                    |
| **Mental-model diagnostics**      | No scenario-based probes; only generic attitude items.                                                                                                                        | Insert structured vignettes (Section 5) that force probability judgments about LLM performance.                                |
| **Statistical power**             | Power analysis assumes linear models; ordinal mixed models likely required for 0–100 error distributions.                                                                     | Re-estimate power via Monte-Carlo with cumulative-link or beta-regression.                                                     |
| **Logical flow**                  | “Utility” and “accuracy” conflated in hypotheses; risk of mediation ambiguity.                                                                                                | Separate constructs: perceived usefulness ► *intention*; perceived accuracy ► *trust*.                                         |
| **Real-world constraints**        | Device type, time-on-task, or cognitive load not captured; limits external validity.                                                                                          | Collect paradata (mobile vs desktop, completion time) and perceived mental workload (NASA-TLX short).                          |

---

**4 Improving ‘Beliefs about LLM Accuracy for Energy Tasks’ & measuring trust mis-calibration**

1. **Item-level probability judgments**
   *“If ChatGPT answered the previous appliance-energy question, how likely (0–100 %) do you think the answer is within **±20 %** of the true value?”* – permits Brier or calibration-slope computation.
2. **Confidence–error scatter**
   After revealing the correct value, ask: *“Given the true answer, rate how surprised you are (1–7).”* This produces subjective-error curves.
3. **Weight-of-Advice (WoA)**
   *“You guessed 1.2 kWh; the LLM says 3.6 kWh. Final answer?”* ➔ WoA = |final–initial| / |LLM–initial|.
4. **Two-factor trust (Trust vs Distrust)** using TILLMI-6  enables separation of calibrated vs defensive distrust.

---

**5 Scenario-based LLM-beliefs question examples**

| Scenario                                                                | Belief probe                                                                                        | Follow-up rationale                                  |
| ----------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- | ---------------------------------------------------- |
| **Real-time data**: “What was Indiana’s average kWh rate *last month*?” | *“Likelihood the LLM reports an up-to-date figure?” (1–7)*                                          | Tests understanding of training-cutoff vs live data. |
| **Tiered tariff calc.**                                                 | Confidence that LLM will compute exact annual cost with provided usage table.                       | Assesses perceived reasoning/math limits.            |
| **Contextual plan**                                                     | Probability the *single* tip recommended is truly highest-impact for a 2-bedroom Phoenix apartment. | Probes personalization depth.                        |
| **Vendor bias**                                                         | Chance response is unbiased when LLM is trained by a solar-panel firm.                              | Measures awareness of potential conflicts.           |

(Adapted & expanded from design notes in file 12) .

---

**6 Novel survey / task ideas integrating Attari-style estimation & LLM evaluation**

1. **Sequential estimation + LLM advice** (see file 35)
   Participants give an initial kWh estimate → view LLM answer (manipulated accuracy & confidence) → provide final estimate. Outcomes: WoA, reliance rate, calibration gap.
2. **Heuristic-teaching intervention**
   After block 1, half the sample receives LLM explanations embedding **expert heuristics** (“heat/cool rule”) from Kantenbacher & Attari; compare improvement in block 2 to generic tips.
3. **Confidence auction**
   Participants “bet” points on whose estimate (self vs LLM) is closer; points convert to raffle tickets—elicits economic trust-calibration.
4. **Directory accuracy task** (TMS measurement)
   Participants assign future questions either to themselves or to the LLM, revealing perceived knowledge boundaries. Track directory accuracy over trials.
5. **Interactive plan builder**
   LLM generates draft conservation plan with 8 actions; participants drag-and-drop to keep, modify, or discard. Coding of edits reveals mental models of LLM strengths.

---

**7 Annotated bibliography — 25 validated survey / scale instruments**

| #  | Instrument (short)                                               | Domain & structure                                                                     | Psychometrics / citation         |
| -- | ---------------------------------------------------------------- | -------------------------------------------------------------------------------------- | -------------------------------- |
| 1  | **MAILS-20 / MAILS-S-8**                                         | Meta-AI literacy; 4 competence facets (cognitive, technical, ethical, meta-reflective) | α ≥ .90; CFA invariance          |
| 2  | **GLAT**                                                         | Objective generative-AI literacy test; 18 MCQ + code-tracing                           | KR-20 = .82                      |
| 3  | **AILQ-36**                                                      | Affective-behaviour-cognitive-ethical AI literacy                                      | α=.89, test–retest =.83          |
| 4  | **PAILQ-6**                                                      | Perceived AI literacy (self-efficacy)                                                  | α=.83                            |
| 5  | **AI Anxiety Scale (AIX-10)**                                    | Discomfort toward AI                                                                   | α=.85; 2-factor                  |
| 6  | **AIAS-4**                                                       | General attitude toward AI                                                             | α=.73; stable across cultures    |
| 7  | **Sindermann Attitude-to-AI (AT-AI-10)**                         | Trust, fear, curiosity                                                                 | α=.88                            |
| 8  | **TILLMI-6**                                                     | Trust vs Distrust in LLMs                                                              | α=.87 (Trust), .81 (Distrust)    |
| 9  | **Trust Perception Scale v2**                                    | Benevolence, integrity, competence of AI                                               | α>.90                            |
| 10 | **Automation Trust Scale (Muir, 1994)**                          | Predictability, dependability, faith                                                   | α=.86                            |
| 11 | **Lee-Moray Compliance/ Reliance**                               | Behavioral trust via signal detection                                                  | d′ metrics                       |
| 12 | **Berlin Numeracy Test-4**                                       | Objective numeracy                                                                     | α=.78                            |
| 13 | **Energy Literacy Questionnaire-Short (ELQ-S-12)**               | Knowledge of energy units, behaviors                                                   | KR-20=.76                        |
| 14 | **Appliance Knowledge Quiz-10**                                  | kWh & power ranking task                                                               | Split-half =.72                  |
| 15 | **STEEM (Subjective Technology Expertise & Experience Measure)** | Prior tech exposure                                                                    | α=.80                            |
| 16 | **Need for Cognition-Short**                                     | Cognitive style moderator                                                              | α=.86                            |
| 17 | **Cognitive Reflection Test-7**                                  | Heuristic vs analytic thinking                                                         | KR-20=.68                        |
| 18 | **NASA-TLX-Short**                                               | Mental workload (6 items)                                                              | Valid across online tasks        |
| 19 | **Wake Forest Trust in General Technology Scale**                | Dispositional trust                                                                    | α=.91                            |
| 20 | **Privacy Concerns for IoT-8**                                   | Data-sharing attitudes                                                                 | α=.88                            |
| 21 | **Perceived Risk of AI Scale-6**                                 | Moral, functional, data-security risk                                                  | α=.79                            |
| 22 | **Technology Readiness Index-2 (TRI-2.0)**                       | Optimism, insecurity                                                                   | α=.86                            |
| 23 | **Self-Reported Expertise Directory (SRED)**                     | TMS measure; % chips allocation                                                        | Reliability >.80 in lab studies  |
| 24 | **Weight-of-Advice task**                                        | Behavioral trust; continuous outcome                                                   | WoA reliability = .78            |
| 25 | **Calibration Curve/Brier Score battery**                        | Objective trust-miscalibration                                                         | N/A (score not scale)            |

---

**8 Annotated bibliography — 25 key LLM research papers**

| #  | Citation & focus                                      | Key finding & relevance                                                                                 |
| -- | ----------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |
| 1  | Steyvers et al., 2025 (Nat. Mach. Intel.)             | Users over-estimate LLM accuracy; longer explanations inflate confidence. Baseline for calibration gap. |
| 2  | Adilazuarda et al., 2024 (Culture survey)             | Cultural bias probes; shows need for situated datasets—relevant for household-energy context.           |
| 3  | Askarisichani et al., 2022 (NYAS)                     | Predictive models for human–AI group decision; outlines TMS and influence constructs.                   |
| 4  | OpenAI, 2024 (GPT-4o Tech Report)                     | Benchmarks on numeric reasoning; residual large-unit errors → appliance tasks risk.                     |
| 5  | Kim et al., 2024 (LLM Uncertainty)                    | Calibrated language improves user trust alignment.                                                      |
| 6  | Lin et al., 2023 (Chain-of-Thought risks)             | COT explanations increase persuasion even when wrong—implications for miscalibration.                   |
| 7  | Holstein et al., 2023 (LLM + Human-in-the-loop)       | Shows confidence-weighted aggregation beats either alone.                                               |
| 8  | Chiang & Lee, 2023 (LLM math benchmarks)              | Quantifies systematic numeric hallucinations; appliance-energy values similar scale.                    |
| 9  | Zhang et al., 2024 (LLM factuality eval)              | Proposes automated factuality metric; can score energy answers for ground truth.                        |
| 10 | Buçinca et al., 2023 (Explanation + initiative)       | Rich explanations improve correction of AI errors only under high perceived stakes.                     |
| 11 | Lazaridou et al., 2022 (Emergent tool use)            | LLMs can learn to call calculators; suggests mitigation for numeric errors.                             |
| 12 | Rae et al., 2022 (Scaling effects on truthfulness)    | Larger models slightly more truthful but still 20 % error.                                              |
| 13 | Yang et al., 2022 (Synthesizing appliance Q\&A)       | Fine-tuning on domain corpus halves hallucination rate—design implication.                              |
| 14 | Maynez et al., 2020 (Faithfulness in summarization)   | Metric/faithfulness debate; parallels in energy plan summaries.                                         |
| 15 | Wu et al., 2023 (Value alignment)                     | Alignment reduces deceptive phrasing; may affect perceived benevolence.                                 |
| 16 | Bubeck et al., 2023 (Sparks of AGI)                   | Highlights reasoning leaps but also unit errors → trust caution.                                        |
| 17 | Liang et al., 2023 (Prompt engineering)               | Temperature & explicit “I might be wrong” reduce over-confidence outputs.                               |
| 18 | Thoppilan et al., 2022 (LaMDA safety)                 | Disallowed content filter sometimes censors energy advice—utility dimension.                            |
| 19 | Ganguli et al., 2022 (Red-teaming LLMs)               | Reveals hidden biases; survey can incorporate bias-awareness items.                                     |
| 20 | Min et al., 2022 (Retrieval-Augmented Generation)     | External docs improve factual correctness—study could test RAG vs vanilla.                              |
| 21 | Jiang et al., 2022 (Teach LLMs to say ‘I don’t know’) | Improves calibration; tie to uncertainty framing factor.                                                |
| 22 | Burns et al., 2022 (LLM in STEM tutoring)             | Learners over-credit AI hints; parallels appliance estimation.                                          |
| 23 | Lazar et al., 2023 (LLM social influence)             | AI suggestions sway moral judgments; underscores need for distrust items.                               |
| 24 | Yin & Zu 2023 (Numerical hallucination taxonomy)      | Classifies error types; informs coding of stimulus answers.                                             |
| 25 | Wei et al., 2022 (Chain-of-Thought vs confidence)     | COT raises subjective confidence more than correctness—direct calibration issue.                        |

---

### Closing remarks

The proposal is **promising** but demands stronger theoretical grounding, explicit calibration metrics, and richer scenario-based items. Implementing the recommended measurement upgrades and leveraging the validated instruments and literatures above will allow rigorous testing of how literacy, heuristics, and uncertainty framing shape *calibrated reliance* on LLMs for home-energy decisions.
