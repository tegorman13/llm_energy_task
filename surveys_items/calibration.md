

# https://chatgpt.com/c/68159039-15bc-8006-ac89-860afbcb0988

**Trust Calibration Task (Behavioral Scenarios):** The centerpiece of our methodology is a set of **energy-related decision scenarios** where participants receive advice from the LLM and must decide whether to trust it. This provides a **behavioral measure of trust and reliance**, directly addressing RQ1, RQ2, and RQ4. We will design **four scenarios** (short vignettes) that involve a common consumer energy decision. For example: (1) *Choosing an energy-efficient appliance* (e.g. *“Should I buy a more expensive high-efficiency refrigerator to save on electricity bills?”*), (2) *Reducing home energy usage* (e.g. *“What are the best ways to cut my heating costs in winter?”*), (3) *Evaluating a renewable energy option* (e.g. *“Does installing solar panels make financial sense for me?”*), and (4) *Understanding an energy bill or policy* (e.g. *“My electricity bill has a new rate structure—what does it mean and how can I adapt?”*). These are illustrative; final scenarios will be refined to be realistic and relatable. For each scenario, we will present the participant with a **question and an answer purportedly from ChatGPT**. The answer text will be a few paragraphs (approx. 150–200 words) giving advice or information. **Unbeknownst to participants, the answers are crafted to vary in accuracy and confidence**: in two of the four scenarios, the AI’s answer will be largely **correct and helpful**, aligning with expert knowledge (though not verbatim textbook, it will be a plausible good answer). In the other two scenarios, the AI’s answer will contain some **incorrect or misleading information** (while still sounding confident and plausible). For example, it might exaggerate savings or cite an incorrect fact about energy usage. Importantly, we ensure these “incorrect” answers are not obviously absurd; they should be the type of subtle error that a layperson with low knowledge might not catch, but someone with high energy literacy would recognize as wrong. This allows us to observe who is able to detect or doubt the bad advice. Additionally, to examine H4 (uncertainty framing), we manipulate the **tone** of the AI’s response: half of the answers (one correct and one incorrect, randomly assigned) will be written in a **cautiously worded style**, where the AI explicitly **verbalizes uncertainty** about some of its suggestions (e.g. “I’m not entirely sure, but you might save around \$X if this estimate is correct…” or “There’s some uncertainty here, but it’s possible that…”). The other half of the answers will be written in a **fully confident tone**, with no hedges (e.g. “This will save you \$X for sure.”). Apart from the tone and a sentence or two of uncertainty disclaimer, the content is similar, so that the key difference is whether the AI appears confident or acknowledges doubt. This yields a 2 (Actual Accuracy: correct vs incorrect info) × 2 (AI Confidence: high/confident vs low/uncertain) variation across the four scenario trials each participant sees. The pairing of scenario to condition will be counterbalanced across participants, so that each scenario is presented in all forms across the sample (to control for any scenario-specific difficulty). Participants are not told that some answers may be wrong; they are led to assume each is an answer from the AI to consider.

After reading each AI answer, the participant will answer a set of **follow-up questions** to indicate their trust and use of that answer:

* **Perceived Accuracy/Confidence Rating:** “How confident are you that the AI’s answer is correct and trustworthy?” (0% = not at all confident it’s correct, 100% = completely confident it’s correct). This analog scale captures their immediate trust in the specific answer.
* **Acceptance Decision:** We will ask whether they would **accept and follow** the AI’s advice or not. For instance: “If you had to make this decision now, based on this answer, what would you do?” with options like **(a)** *Accept the AI’s recommendation and proceed with it*, **(b)** *Seek additional information or second opinion before deciding* (which indicates partial trust or need to verify), **(c)** *Do not follow the AI’s advice* (reject it). We will treat choosing (a) as a sign of high trust/reliance on that answer, (b) as moderate trust (they found it somewhat useful but not sufficient to act on without confirmation), and (c) as low trust (they did not find it credible or acceptable). This categorical measure reflects **behavioral intention** based on trust.
* **Information Seeking (Optional):** In an actual use case, a user could try to verify the AI’s claims. We will simulate this by asking: “Would you like to see a verified explanation or source for the answer?” If they say yes, we will reveal a brief “expert explanation” that actually explains the correct facts (this serves as debriefing and also lets them not leave with misinformation). Their choice to view a second source can be taken as an indicator of lack of full trust. We will record whether they requested additional info for each scenario. (In analysis, this can be combined with the acceptance decision; e.g., option (b) above essentially means they would seek additional info.)
* **Perceived Uncertainty of AI:** We will also ask, “Did the AI’s answer seem uncertain or confident?” on a 5-point scale from “very confident” to “very uncertain,” as a **manipulation check** for our uncertainty framing. We expect participants to rate the explicitly hedged answers as more uncertain on average, confirming they noticed the difference.




---------

# https://gemini.google.com/app/5d2abc9eec2788f2

### **Human-AI Trust, Calibration, and Reliance Dynamics**

Trust in AI is a critical determinant of user acceptance and reliance.12 It is conceptualized as a multi-faceted psychological state reflecting beliefs about an AI's capabilities (competence, reliability) and intentions (integrity, benevolence).13 Recent work suggests differentiating between dispositional trust (a general tendency to trust AI) and task-specific trust (trust in an AI for a particular purpose) 13, as well as distinguishing between *trust* and *distrust* as potentially independent constructs.13 This study adopts the validated two-factor structure of the Trust Perception Scale (TPA), measuring Trust and Distrust separately.13*Calibrated trust* occurs when a user's trust level appropriately matches the AI's actual capabilities for the task at hand.11 Miscalibration, particularly over-trust leading to over-reliance on incorrect AI outputs, is a significant risk.8 Conversely, under-trust leading to under-reliance on correct AI outputs is also suboptimal.14 Measuring calibration requires assessing both user confidence/trust and objective performance/accuracy.10 Behavioral measures of calibration involve comparing user confidence judgments against their accuracy in evaluating AI outputs.10 Measuring reliance involves analyzing user choices to follow or disregard AI advice, particularly distinguishing between reliance on correct versus incorrect advice.8 Factors influencing calibration and reliance include the AI's expressed uncertainty 1 and the presence and nature of explanations.9





---------

# https://notebooklm.google.com/notebook/a04e8d62-e334-44e2-86ee-f1bf4b02a21f?pli=1

**Measuring Trust Miscalibration**

Measuring trust miscalibration requires comparing users' confidence or trust in LLM outputs against the *actual* correctness of those outputs. Purely self-report general or task-specific trust ratings are insufficient. Recommendations for measuring trust miscalibration include:

1.  **Objective Calibration Task:** This is the most rigorous approach and a key missing element.
    *   **Method:** Present participants with several pre-generated LLM responses to specific energy-related questions or tasks. Some responses should be objectively correct, and others objectively incorrect or misleading, covering a range of complexity (e.g., factual recall, quantitative estimation, advice).
    *   **Assessment:** For *each* LLM response shown, ask participants to:
        *   Judge the correctness (e.g., True/False/Unsure, or a multi-point scale like "Definitely Incorrect" to "Definitely Correct").
        *   Rate their confidence in *their own correctness judgment* (e.g., 0-100% slider).
        *   Rate their trust in *that specific LLM response* or their likelihood to *rely* on it (e.g., Likert scale).
    *   **Analysis:** Calculate objective calibration scores (e.g., Brier score, which measures the accuracy of probability forecasts). Compute over/underconfidence indices (mean confidence - mean accuracy). Correlate user judgment accuracy and calibration scores with their self-reported trust and reliance intentions. This directly tests if trust/reliance aligns with the LLM's actual performance as judged by the user.

2.  **Scenario-Based Calibration/Reliance:** While less direct than evaluating actual LLM outputs, this method is more behavioral than general self-report.
    *   **Method:** Present participants with concrete vignettes describing an interaction with an LLM providing energy advice, including specific details like numbers or recommendations. Critically, vary the *accuracy* of the LLM's advice in the vignette *between participants* (e.g., one group sees a scenario with correct advice, another with incorrect advice).
    *   **Assessment:** After reading the scenario, ask participants about their perceived accuracy of the LLM's advice in the vignette and their stated behavioral intention (e.g., Accept, Verify, Reject).
    *   **Analysis:** Compare perceived accuracy ratings and reliance intentions between groups exposed to correct vs. incorrect LLM advice. If participants rate incorrect advice as highly as correct advice, or are equally willing to rely on it, this indicates miscalibration (specifically, over-reliance).

Implementing the Objective Calibration Task within the survey would significantly strengthen the measurement of RQ4 and H4. If length is prohibitive, the Scenario-Based approach offers a compromise.
