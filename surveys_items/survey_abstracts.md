
# AI Survey


Ayanwale, M. A., Sanusi, I. T., Adelana, O. P., Aruleba, K. D., & Oyelere, S. S. (2022). Teachers’ readiness and intention to teach artificial intelligence in schools. Computers and Education: Artificial Intelligence, 3, 100099. https://doi.org/10.1016/j.caeai.2022.100099
The emergence of artificial intelligence (AI) as a subject to be incorporated into K-12 educational levels places new demand on relevant stakeholders, especially teachers that drive the teaching and learning process. It is therefore important to understand how ready teachers are to teach the emerging subject as the success of AI education would probably be closely dependent on the readiness of teachers. As a result, this study presents an insight into factors influencing the behavioural intention and readiness of Nigerian in-service teachers to teach artificial intelligence. A total of 368 teachers, from elementary to high school participated in the study. We utilised quantitative methodology using variance-based structural equation modelling to understand the relationship among the eight variables (AI anxiety, perceived usefulness, AI for social good, Attitude towards using AI, perceived confidence in teaching AI, relevance of AI, AI readiness, and behavioural intention) considered in the study. The result indicated that confidence in teaching AI predicts intention to teach AI while AI relevance strongly predicts readiness to teach AI. While other factors influence the teaching of AI, anxiety and social good could not predict teachers’ intention and readiness to implement AI in classrooms respectively. We discussed the implication of our findings in relation to AI implementation in schools and highlight future directions.

Carolus, A., Koch, M. J., Straka, S., Latoschik, M. E., & Wienrich, C. (2023). MAILS - Meta AI literacy scale: Development and testing of an AI literacy questionnaire based on well-founded competency models and psychological change- and meta-competencies. Computers in Human Behavior: Artificial Humans, 1(2), 100014. https://doi.org/10.1016/j.chbah.2023.100014
Valid measurement of AI literacy is important for the selection of personnel, identification of shortages in skill and knowledge, and evaluation of AI literacy interventions. A questionnaire is missing that is deeply grounded in the existing literature on AI literacy, is modularly applicable depending on the goals, and includes further psychological competencies in addition to the typical facets of AIL. This paper presents the development and validation of a questionnaire considering the desiderata described above. We derived items to represent different facets of AI literacy and psychological competencies, such as problem-solving, learning, and emotion regulation in regard to AI. We collected data from 300 German-speaking adults to confirm the factorial structure. The result is the Meta AI Literacy Scale (MAILS) for AI literacy with the facets Use & apply AI, Understand AI, Detect AI, and AI Ethics and the ability to Create AI as a separate construct, and AI Self-efficacy in learning and problem-solving and AI Self-management (i.e., AI persuasion literacy and emotion regulation). This study contributes to the research on AI literacy by providing a measurement instrument relying on profound competency models. Psychological competencies are included particularly important in the context of pervasive change through AI systems.

Chen, A., Kim, S. S. Y., Dharmasiri, A., Russakovsky, O., & Fan, J. E. (2025). Portraying Large Language Models as Machines, Tools, or Companions Affects What Mental Capacities Humans Attribute to Them. Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 1–14. https://doi.org/10.1145/3706599.3719710
As large language models (LLMs) become increasingly popular and prevalent in media and daily conversations, individuals encounter different portrayals of LLMs from various sources. It is important to understand how these portrayals can shape their beliefs about LLMs as this can have downstream impacts on adoption and usage behaviors. In this work, we investigate what mental capacities individuals attribute to LLMs after being exposed to short videos adopting one of three portrayals: mechanistic (LLMs as machines), functional (LLMs as tools), and intentional (LLMs as companions). We find that the intentional portrayal increases the attribution of mental capacities to LLMs, and that individuals tend to attribute mind-related capacities the most, followed by heart- then bodyrelated capacities. We discuss the implications of these findings, provide recommendations on how to portray LLMs, and outline directions for future research.

Duro, E. S. D., Veltri, G. A., Golino, H., & Stella, M. (2025). Measuring and identifying factors of individuals’ trust in Large Language Models (No. arXiv:2502.21028). arXiv. https://doi.org/10.48550/arXiv.2502.21028
Large Language Models (LLMs) can engage in human-looking conversational exchanges. Although conversations can elicit trust between users and LLMs, scarce empirical research has examined trust formation in human-LLM contexts, beyond LLMs’ trustworthiness or human trust in AI in general. Here, we introduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure individuals’ trust in LLMs, extending McAllister’s cognitive and affective trust dimensions to LLM-human interactions. We developed TILLMI as a psychometric scale, prototyped with a novel protocol we called LLM-simulated validity. The LLM-based scale was then validated in a sample of 1,000 US respondents. Exploratory Factor Analysis identified a two-factor structure. Two items were then removed due to redundancy, yielding a final 6-item scale with a 2-factor structure. Confirmatory Factor Analysis on a separate subsample showed strong model fit ($CFI = .995$, $TLI = .991$, $RMSEA = .046$, $p_{X^2} > .05$). Convergent validity analysis revealed that trust in LLMs correlated positively with openness to experience, extraversion, and cognitive flexibility, but negatively with neuroticism. Based on these findings, we interpreted TILLMI’s factors as “closeness with LLMs” (affective dimension) and “reliance on LLMs” (cognitive dimension). Younger males exhibited higher closeness with- and reliance on LLMs compared to older women. Individuals with no direct experience with LLMs exhibited lower levels of trust compared to LLMs’ users. These findings offer a novel empirical foundation for measuring trust in AI-driven verbal communication, informing responsible design, and fostering balanced human-AI collaboration.

Fang, C. M., Liu, A. R., Danry, V., Lee, E., Chan, S. W. T., Pataranutaporn, P., Maes, P., Phang, J., Lampe, M., Ahmad, L., & Agarwal, S. (2025). How AI and Human Behaviors Shape Psychosocial Effects of Chatbot Use: A Longitudinal Randomized Controlled Study (No. arXiv:2503.17473). arXiv. https://doi.org/10.48550/arXiv.2503.17473
AI chatbots, especially those with voice capabilities, have become increasingly human-like, with more users seeking emotional support and companionship from them. Concerns are rising about how such interactions might impact users’ loneliness and socialization with real people. We conducted a four-week randomized, controlled, IRB-approved experiment (n=981, >300K messages) to investigate how AI chatbot interaction modes (text, neutral voice, and engaging voice) and conversation types (open-ended, non-personal, and personal) influence psychosocial outcomes such as loneliness, social interaction with real people, emotional dependence on AI and problematic AI usage. Results showed that while voice-based chatbots initially appeared beneficial in mitigating loneliness and dependence compared with text-based chatbots, these advantages diminished at high usage levels, especially with a neutral-voice chatbot. Conversation type also shaped outcomes: personal topics slightly increased loneliness but tended to lower emotional dependence compared with open-ended conversations, whereas non-personal topics were associated with greater dependence among heavy users. Overall, higher daily usage - across all modalities and conversation types - correlated with higher loneliness, dependence, and problematic use, and lower socialization. Exploratory analyses revealed that those with stronger emotional attachment tendencies and higher trust in the AI chatbot tended to experience greater loneliness and emotional dependence, respectively. These findings underscore the complex interplay between chatbot design choices (e.g., voice expressiveness) and user behaviors (e.g., conversation content, usage frequency). We highlight the need for further research on whether chatbots’ ability to manage emotional content without fostering dependence or replacing human relationships benefits overall well-being.

Gerlich, M. (2025). AI Tools in Society: Impacts on Cognitive Offloading and the Future of Critical Thinking. Societies, 15(1), Article 1. https://doi.org/10.3390/soc15010006
The proliferation of artificial intelligence (AI) tools has transformed numerous aspects of daily life, yet its impact on critical thinking remains underexplored. This study investigates the relationship between AI tool usage and critical thinking skills, focusing on cognitive offloading as a mediating factor. Utilising a mixed-method approach, we conducted surveys and in-depth interviews with 666 participants across diverse age groups and educational backgrounds. Quantitative data were analysed using ANOVA and correlation analysis, while qualitative insights were obtained through thematic analysis of interview transcripts. The findings revealed a significant negative correlation between frequent AI tool usage and critical thinking abilities, mediated by increased cognitive offloading. Younger participants exhibited higher dependence on AI tools and lower critical thinking scores compared to older participants. Furthermore, higher educational attainment was associated with better critical thinking skills, regardless of AI usage. These results highlight the potential cognitive costs of AI tool reliance, emphasising the need for educational strategies that promote critical engagement with AI technologies. This study contributes to the growing discourse on AI’s cognitive implications, offering practical recommendations for mitigating its adverse effects on critical thinking. The findings underscore the importance of fostering critical thinking in an AI-driven world, making this research essential reading for educators, policymakers, and technologists.

Gnambs, T., Stein, J.-P., Zinn, S., Griese, F., & Appel, M. (2025). Attitudes, experiences, and usage intentions of artificial intelligence: A population study in Germany. Telematics and Informatics, 98, 102265. https://doi.org/10.1016/j.tele.2025.102265
Artificial intelligence (AI) increasingly affects individuals’ private and professional lives. Importantly, both the acceptance and adoption of new AI technologies in society is heavily impacted by the attitudes that people hold; yet, there is currently limited information on how people perceive and intend to use AI at the national and demographic levels. Therefore, this study examined a random sample of 1,098 German adults to assess their attitudes, experiences, and usage intentions regarding AI in work, healthcare, and education. The findings indicated that respondents generally held favorable attitudes towards AI, with AI applications in healthcare receiving more positive evaluations than AI in the context of work. Moreover, cognitive evaluations of AI were more positive than emotional or behavioral appraisals. Prior experiences with AI were, however, limited, particularly in healthcare and education. Demographic differences were generally small. Taken together, these findings demonstrate that, in Germany, AI is currently widely accepted in different domains, although most people have little first-hand experience with it. These insights can inform policymakers and stakeholders who care about the proliferation of AI in society.

Grassini, S. (2023). Development and validation of the AI attitude scale (AIAS-4): A brief measure of general attitude toward artificial intelligence. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1191628
The rapid advancement of artificial intelligence (AI) has generated an increasing demand for tools that can assess public attitudes toward AI. This study proposes the development and the validation of the AI Attitude Scale (AIAS), a concise self-report instrument designed to evaluate public perceptions of AI technology. The first version of the AIAS that the present manuscript proposes comprises five items, including one reverse-scored item, which aims to gauge individuals’ beliefs about AI’s influence on their lives, careers, and humanity overall. The scale is designed to capture attitudes toward AI, focusing on the perceived utility and potential impact of technology on society and humanity. The psychometric properties of the scale were investigated using diverse samples in two separate studies. An exploratory factor analysis was initially conducted on a preliminary 5-item version of the scale. Such exploratory validation study revealed the need to divide the scale into two factors. While the results demonstrated satisfactory internal consistency for the overall scale and its correlation with related psychometric measures, separate analyses for each factor showed robust internal consistency for Factor 1 but insufficient internal consistency for Factor 2. As a result, a second version of the scale is developed and validated, omitting the item that displayed weak correlation with the remaining items in the questionnaire. The refined final 1-factor, 4-item AIAS demonstrated superior overall internal consistency compared to the initial 5-item scale and the proposed factors. Further confirmatory factor analyses, performed on a different sample of participants, confirmed that the 1-factor model (4-items) of the AIAS exhibited an adequate fit to the data, providing additional evidence for the scale’s structural validity and generalizability across diverse populations. In conclusion, the analyses reported in this article suggest that the developed and validated 4-items AIAS can be a valuable instrument for researchers and professionals working on AI development who seek to understand and study users’ general attitudes toward AI.

Grassini, S. (2024). A Psychometric Validation of the PAILQ-6: Perceived Artificial Intelligence Literacy Questionnaire. Nordic Conference on Human-Computer Interaction, 1–10. https://doi.org/10.1145/3679318.3685359
The present article introduces and implements an initial validation for the Perceived Artificial Intelligence Literacy Questionnaire (PAILQ-6), a brief tool designed to assess individuals' self-perceived AI literacy. Amidst the growing integration of AI in various aspects of life and its ethical implications, understanding AI becomes crucial for effective interaction with AI technologies. The PAILQ-6 emerges in response to the need for an accessible instrument that evaluates general AI literacy without compromising on clarity or depth, suitable for both academic and practical applications. This paper presents the development process of the PAILQ-6, consisting of six items derived from established components of AI literacy, structured as a seven-point Likert scale for easy administration and digital compatibility. The validation study was conducted from data of a gender-balanced sample of 232 UK adults. The article demonstrates the PAILQ-6's reliability and validity through exploratory factor analysis, showing a two-factor structure. The findings reveal the scale's good internal consistency and convergent validity. The study highlights demographic predictors of AI literacy perceptions, indicating a possible gender disparity and the positive influence of higher education on perceived AI competency.


Hou, C., Zhu, G., Sudarshan, V., Lim, F. S., & Ong, Y. S. (2025). Measuring undergraduate students’ reliance on Generative AI during problem-solving: Scale development and validation. Computers & Education, 234, 105329. https://doi.org/10.1016/j.compedu.2025.105329
Reliance on AI describes the behavioral patterns of when and how individuals depend on AI suggestions, and appropriate reliance patterns are necessary to achieve effective human-AI collaboration. Traditional measures often link reliance to decision-making outcomes, which may not be suitable for complex problem-solving tasks where outcomes are not binary (i.e., correct or incorrect) or immediately clear. Therefore, this study aims to develop a scale to measure undergraduate students’ behaviors of using Generative AI during problem-solving tasks without directly linking them to specific outcomes. We conducted an exploratory factor analysis on 800 responses collected after students finished one problem-solving activity, which revealed four distinct factors: reflective use, cautious use, thoughtless use, and collaborative use. The overall scale has reached sufficient internal reliability (Cronbach’s alpha = .84). Two confirmatory factor analyses (CFAs) were conducted to validate the factors using the remaining 730 responses from this activity and 1173 responses from another problem-solving activity. CFA indices showed adequate model fit for data from both problem-solving tasks, suggesting that the scale can be applied to various human-AI problem-solving tasks. This study offers a validated scale to measure students’ reliance behaviors in different human-AI problem-solving activities and provides implications for educators to responsively integrate Generative AI in higher education.

Jin, Y., Martinez-Maldonado, R., Gašević, D., & Yan, L. (2024). GLAT: The Generative AI Literacy Assessment Test (No. arXiv:2411.00283). arXiv. https://doi.org/10.48550/arXiv.2411.00283
The rapid integration of generative artificial intelligence (GenAI) technology into education necessitates precise measurement of GenAI literacy to ensure that learners and educators possess the skills to engage with and critically evaluate this transformative technology effectively. Existing instruments often rely on self-reports, which may be biased. In this study, we present the GenAI Literacy Assessment Test (GLAT), a 20-item multiple-choice instrument developed following established procedures in psychological and educational measurement. Structural validity and reliability were confirmed with responses from 355 higher education students using classical test theory and item response theory, resulting in a reliable 2-parameter logistic (2PL) model (Cronbach’s alpha = 0.80; omega total = 0.81) with a robust factor structure (RMSEA = 0.03; CFI = 0.97). Critically, GLAT scores were found to be significant predictors of learners’ performance in GenAI-supported tasks, outperforming self-reported measures such as perceived ChatGPT proficiency and demonstrating external validity. These results suggest that GLAT offers a reliable and valid method for assessing GenAI literacy, with the potential to inform educational practices and policy decisions that aim to enhance learners’ and educators’ GenAI literacy, ultimately equipping them to navigate an AI-enhanced future.

Jiun-Yin Jian, Bisantz, A. M., & Drury, C. G. (2000). Foundations for an Empirically Determined Scale of Trust in Automated System. International Journal of Cognitive Ergonomics, 4(1), 53. https://doi.org/10.1207/S15327566IJCE0401_04
One component in the successful use of automated systems is the extent to which people trust the automation to perform effectively. In order to understand the relationship between trust in computerized systems and the use of those systems, we need to be able to effectively measure trust. Although questionnaires regarding trust have been used in prior studies, these questionnaires were theoretically rather than empirically generated and did not distinguish between 3 potentially different types of trust: human–human trust, human–machine trust, and trust in general. A 3-phased experiment, comprising a word elicitation study, a questionnaire study, and a paired comparison study, was performed to better understand similarities and differences in the concepts of trust and distrust, and among the different types of trust. Results indicated that trust and distrust can be considered opposites, rather than different concepts. Components of trust, in terms of words related to trust, were similar across the three types of trust. Results obtained from a cluster analysis were used to identify 12 potential factors of trust between people and automated systems. These 12 factors were then used to develop a proposed scale to measure trust in automation.




Kim, S. S. Y., Liao, Q. V., Vorvoreanu, M., Ballard, S., & Vaughan, J. W. (2024). “I’m Not Sure, But...”: Examining the Impact of Large Language Models’ Uncertainty Expression on User Reliance and Trust (No. arXiv:2405.00623). arXiv. https://doi.org/10.48550/arXiv.2405.00623
Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs’ expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants’ reliance, trust, and overall task performance. We find that first-person expressions (e.g., “I’m not sure, but...”) decrease participants’ confidence in the system and tendency to agree with the system’s answers, while increasing participants’ accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., “It’s not clear, but...”), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.


Koch, M. J., Carolus, A., Wienrich, C., & Latoschik, M. E. (2024). Meta AI literacy scale: Further validation and development of a short version. Heliyon, 10(21), e39686. https://doi.org/10.1016/j.heliyon.2024.e39686
The concept of AI literacy, its promotion, and measurement are important topics as they prepare society for the steadily advancing spread of AI technology. The first purpose of the current study is to advance the measurement of AI literacy by collecting evidence regarding the validity of the Meta AI Literacy Scale (MAILS) by Carolus and colleagues published in 2023: a self-assessment instrument for AI literacy and additional psychological competencies conducive for the use of AI. For this purpose, we first formulated the intended measurement purposes of the MAILS. In a second step, we derived empirically testable axioms and subaxioms from the purposes. We tested them in several already published and newly collected data sets. The results are presented in the form of three different empirical studies. We found overall evidence for the validity of the MAILS with some unexpected findings that require further research. We discuss the results for each study individually and also together. Also, avenues for future research are discussed. The study’s second purpose is to develop a short version (10 items) of the original instrument (34 items). It was possible to find a selection of ten items that represent the factors of the MAILS and show a good model fit when tested with confirmatory factor analysis. Further research will be needed to validate the short scale. This paper advances the knowledge about the validity and provides a short measure for AI literacy. However, more research will be necessary to further our understanding of the relationships between AI literacy and other constructs.

Laupichler, M. C., Aster, A., Haverkamp, N., & Raupach, T. (2023). Development of the “Scale for the assessment of non-experts’ AI literacy” – An exploratory factor analysis. Computers in Human Behavior Reports, 12, 100338. https://doi.org/10.1016/j.chbr.2023.100338
Artificial Intelligence competencies will become increasingly important in the near future. Therefore, it is essential that the AI literacy of individuals can be assessed in a valid and reliable way. This study presents the development of the “Scale for the assessment of non-experts’ AI literacy” (SNAIL). An existing AI literacy item set was distributed as an online questionnaire to a heterogeneous group of non-experts (i.e., individuals without a formal AI or computer science education). Based on the data collected, an exploratory factor analysis was conducted to investigate the underlying latent factor structure. The results indicated that a three-factor model had the best model fit. The individual factors reflected AI competencies in the areas of “Technical Understanding”, “Critical Appraisal”, and “Practical Application”. In addition, eight items from the original questionnaire were deleted based on high intercorrelations and low communalities to reduce the length of the questionnaire. The final SNAIL-questionnaire consists of 31 items that can be used to assess the AI literacy of individual non-experts or specific groups and is also designed to enable the evaluation of AI literacy courses’ teaching effectiveness.

Lee, E., Pataranutaporn, P., Amores, J., & Maes, P. (2024). Super-intelligence or Superstition? Exploring Psychological Factors Influencing Belief in AI Predictions about Personal Behavior (No. arXiv:2408.06602). arXiv. https://doi.org/10.48550/arXiv.2408.06602
Could belief in AI predictions be just another form of superstition? This study investigates psychological factors that influence belief in AI predictions about personal behavior, comparing it to belief in astrology- and personality-based predictions. Through an experiment with 238 participants, we examined how cognitive style, paranormal beliefs, AI attitudes, personality traits, and other factors affect perceived validity, reliability, usefulness, and personalization of predictions from different sources. Our findings reveal that belief in AI predictions is positively correlated with belief in predictions based on astrology and personality psychology. Notably, paranormal beliefs and positive attitudes about AI significantly increased perceived validity, reliability, usefulness, and personalization of AI predictions. Conscientiousness was negatively correlated with belief in predictions across all sources, and interest in the prediction topic increased believability across predictions. Surprisingly, we found no evidence that cognitive style has an impact on belief in fictitious AI-generated predictions. These results highlight the “rational superstition” phenomenon in AI, where belief is driven more by mental heuristics and intuition than critical evaluation. This research advances our understanding of the psychology of human-AI interaction, offering insights into designing and promoting AI systems that foster appropriate trust and skepticism, critical for responsible integration in an increasingly AI-driven world.

Li, J., King, R. B., Chai, C. S., Zhai, X., & Lee, V. W. Y. (2025). The AI Motivation Scale (AIMS): A self-determination theory perspective. Journal of Research on Technology in Education. https://www.tandfonline.com/doi/abs/10.1080/15391523.2025.2478424
Artificial Intelligence (AI) has a profound impact on university teaching and learning. However, there is a lack of instruments for measuring university students’ motivation to use AI in their learning. In this study, we developed and validated a questionnaire to measure students’ motivation to learn with AI. In Study 1, we developed the AI Motivation Scale (AIMS). Rooted in self-determination theory, the scale measures university students’ motivation to learn with AI across five dimensions: intrinsic motivation, identified regulation, introjected regulation, external regulation, and amotivation. Both within-network and between-network validation analyses indicated that the AIMS is psychometrically sound. In Study 2, we used the AIMS to explore whether students’ motivation to learn with AI is influenced by their university environment and promotes their engagement in learning with AI. The results showed that motivation to learn with AI mediated the positive relationship between supportive environments and engagement in learning with AI. The study shows that AIMS is a psychometrically sound instrument that can be used to assess university students’ motivation to learn with AI. It also sheds light on the pivotal role of motivation to learn with AI in the higher education context.



Long, D., & Magerko, B. (2020). What is AI Literacy? Competencies and Design Considerations. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, 1–16. https://doi.org/10.1145/3313831.3376727
Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learner-centered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper's contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.


Maertens, R., Götz, F. M., Golino, H. F., Roozenbeek, J., Schneider, C. R., Kyrychenko, Y., Kerr, J. R., Stieger, S., McClanahan, W. P., Drabot, K., He, J., & Van Der Linden, S. (2023). The Misinformation Susceptibility Test (MIST): A psychometrically validated measure of news veracity discernment. Behavior Research Methods, 56(3), 1863–1899. https://doi.org/10.3758/s13428-023-02124-2
Interest in the psychology of misinformation has exploded in recent years. Despite ample research, to date there is no validated framework to measure misinformation susceptibility. Therefore, we introduce Verification done, a nuanced interpretation schema and assessment tool that simultaneously considers Veracity discernment, and its distinct, measurable abilities (real/fake news detection), and biases (distrust/naïvité—negative/positive judgment bias). We then conduct three studies with seven independent samples (Ntotal = 8504) to show how to develop, validate, and apply the Misinformation Susceptibility Test (MIST). In Study 1 (N=409) we use a neural network language model to generate items, and use three psychometric methods—factor analysis, item response theory, and exploratory graph analysis—to create the MIST-20 (20 items; completion time < 2 minutes), the MIST-16 (16 items; < 2 minutes), and the MIST-8 (8 items; < 1 minute). In Study 2 (N = 7674) we confirm the internal and predictive validity of the MIST in five national quota samples (US, UK), across 2 years, from three different sampling platforms—Respondi, CloudResearch, and Prolific. We also explore the MIST’s nomological net and generate age-, region-, and country-specific norm tables. In Study 3 (N=421) we demonstrate how the MIST—in conjunction with Verification done—can provide novel insights on existing psychological interventions, thereby advancing theory development. Finally, we outline the versatile implementations of the MIST as a screening tool, covariate, and intervention evaluation framework. As all methods are transparently reported and detailed, this work will allow other researchers to create similar scales or adapt them for any population of interest.


Morales-García, W. C., Sairitupa-Sanchez, L. Z., Morales-García, S. B., & Morales-García, M. (2024). Adaptation and psychometric properties of a brief version of the general self-efficacy scale for use with artificial intelligence (GSE-6AI) among university students. Frontiers in Education, 9. https://doi.org/10.3389/feduc.2024.1293437
Background
Individual beliefs about one’s ability to carry out tasks and face challenges play a pivotal role in academic and professional formation. In the contemporary technological landscape, Artificial Intelligence (AI) is effecting profound changes across multiple sectors. Adaptation to this technology varies greatly among individuals. The integration of AI in the educational setting has necessitated a tool that measures self-efficacy concerning the adoption and use of this technology.
Objective
To adapt and validate a short version of the General Self-Efficacy Scale (GSE-6) for self-efficacy in the use of Artificial Intelligence (GSE-6AI) in a university student population.
Methods
An instrumental study was conducted with the participation of 469 medical students aged between 18 and 29 (M = 19.71; SD = 2.47). The GSE-6 was adapted to the AI context, following strict translation and cultural adaptation procedures. Its factorial structure was evaluated through confirmatory factorial analysis (CFA). Additionally, the factorial invariance of the scale based on gender was studied.
Results
The GSE-6AI exhibited a unidimensional structure with excellent fit indices. All item factorial loads surpassed the recommended threshold, and both Cronbach’s Alpha (α) and McDonald’s Omega (ω) achieved a value of 0.91. Regarding factorial invariance by gender, the scale proved to maintain its structure and meaning in both men and women.
Conclusion
The adapted GSE-6AI version is a valid and reliable tool for measuring self-efficacy in the use of Artificial Intelligence among university students. Its unidimensional structure and gender-related factorial invariance make it a robust and versatile tool for future research and practical applications in educational and technological contexts.


Morrill, J., & Noetel, M. (2023). A short-form AI literacy intervention can reduce over-reliance on AI. OSF. https://doi.org/10.31234/osf.io/hv9qc
Artificial intelligence (AI) is becoming very capable, and can match or even exceed human performance in various tasks. Human-AI teams are necessary to maintain oversight over AI, however, humans tend to over-rely on AI, especially when their trust is high. New research has emerged showing that interventions that improve AI literacy—knowledge and understanding of AI—appear to reduce over-reliance on AI, however it is unclear what is mediating this effect. Further, AI literacy in the general public is poor, and short-form interventions that aim to teach about important AI concepts are sparse. We aimed to test whether a short, 5-minute text-based AI literacy intervention would improve AI literacy and reduce over-reliance on AI. We recruited 153 undergraduate psychology students from the University of Queensland and randomly assigned them to receive either the AI literacy intervention or a similarly-lengthed control material unrelated to AI, and subsequently tested their reliance on AI in a human-AI team context. We found that, compared with the control condition, those who received the literacy intervention had significantly improved AI literacy and significantly reduced over-reliance, though there was no indirect effect of the intervention on over-reliance through AI literacy or trust. Our findings highlight the potential for short-form AI literacy interventions in not only improving AI literacy but reducing over-reliance on AI in a human-AI team context, however, more research is necessary to bring clarity to what may be mediating this effect.


Ng, D. T. K., Wu, W., Leung, J. K. L., Chiu, T. K. F., & Chu, S. K. W. (2024). Design and validation of the AI literacy questionnaire: The affective, behavioural, cognitive and ethical approach. British Journal of Educational Technology, 55(3), 1082–1104. https://doi.org/10.1111/bjet.13411
Artificial intelligence (AI) literacy is at the top of the agenda for education today in developing learners’ AI knowledge, skills, attitudes and values in the 21st century. However, there are few validated research instruments for educators to examine how secondary students develop and perceive their learning outcomes. After reviewing the literature on AI literacy questionnaires, we categorized the identified competencies in four dimensions: (1) affective learning (intrinsic motivation and self-efficacy/confidence), (2) behavioural learning (behavioural commitment and collaboration), (3) cognitive learning (know and understand; apply, evaluate and create) and (4) ethical learning. Then, a 32-item self-reported questionnaire on AI literacy (AILQ) was developed and validated to measure students’ literacy development in the four dimensions. The design and validation of AILQ were examined through theoretical review, expert judgement, interview, pilot study and first- and second-order confirmatory factor analysis. This article reports the findings of a pilot study using a preliminary version of the AILQ among 363 secondary school students in Hong Kong to analyse the psychometric properties of the instrument. Results indicated a four-factor structure of the AILQ and revealed good reliability and validity. The AILQ is recommended as a reliable measurement scale for assessing how secondary students foster their AI literacy and inform better instructional design based on the proposed affective, behavioural, cognitive and ethical (ABCE) learning framework. Practitioner notes What is already known about this topic AI literacy has drawn increasing attention in recent years and has been identified as an important digital literacy. Schools and universities around the world started to incorporate AI into their curriculum to foster young learners’ AI literacy. Some studies have worked to design suitable measurement tools, especially questionnaires, to examine students’ learning outcomes in AI learning programmes. What this paper adds Develops an AI literacy questionnaire (AILQ) to evaluate students’ literacy development in terms of affective, behavioural, cognitive and ethical (ABCE) dimensions. Proposes a parsimonious model based on the ABCE framework and addresses a skill set of AI literacy. Implications for practice and/or policy Researchers are able to use the AILQ as a guide to measure students’ AI literacy. Practitioners are able to use the AILQ to assess students’ AI literacy development.

Ovsyannikova, D., De Mello, V. O., & Inzlicht, M. (2025). Third-party evaluators perceive AI as more compassionate than expert humans. Communications Psychology, 3(1). https://doi.org/10.1038/s44271-024-00182-6
Empathy connects us but strains under demanding settings. This study explored how third parties evaluated AI-generated empathetic responses versus human responses in terms of compassion, responsiveness, and overall preference across four preregistered experiments. Participants (N = 556) read empathy prompts describing valenced personal experiences and compared the AI responses to select non-expert or expert humans. Results revealed that AI responses were preferred and rated as more compassionate compared to select human responders (Study 1). This pattern of results remained when author identity was made transparent (Study 2), when AI was compared to expert crisis responders (Study 3), and when author identity was disclosed to all participants (Study 4). Third parties perceived AI as being more responsive—conveying understanding, validation, and care—which partially explained AI’s higher compassion ratings in Study 4. These findings suggest that AI has robust utility in contexts requiring empathetic interaction, with the potential to address the increasing need for empathy in supportive communication contexts.

Pataranutaporn, P., Liu, R., Finn, E., & Maes, P. (2023). Influencing human–AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy and effectiveness. Nature Machine Intelligence, 5(10), 1076–1086. https://doi.org/10.1038/s42256-023-00720-7
As conversational agents powered by large language models become more human-like, users are starting to view them as companions rather than mere assistants. Our study explores how changes to a person’s mental model of an AI system affects their interaction with the system. Participants interacted with the same conversational AI, but were influenced by different priming statements regarding the AI’s inner motives: caring, manipulative or no motives. Here we show that those who perceived a caring motive for the AI also perceived it as more trustworthy, empathetic and better-performing, and that the effects of priming and initial mental models were stronger for a more sophisticated AI model. Our work also indicates a feedback loop in which the user and AI reinforce the user’s mental model over a short time; further work should investigate long-term effects. The research highlights the importance of how AI systems are introduced can notably affect the interaction and how the AI is experienced.

Puppart, B., & Aru, J. (2025). Short-term AI literacy intervention does not reduce over-reliance on incorrect ChatGPT recommendations (No. arXiv:2503.10556). arXiv. https://doi.org/10.48550/arXiv.2503.10556
In this study, we examined whether a short-form AI literacy intervention could reduce the adoption of incorrect recommendations from large language models. High school seniors were randomly assigned to either a control or an intervention group, which received an educational text explaining ChatGPT’s working mechanism, limitations, and proper use. Participants solved math puzzles with the help of ChatGPT’s recommendations, which were incorrect in half of the cases. Results showed that students adopted incorrect suggestions 52.1% of the time, indicating widespread over-reliance. The educational intervention did not significantly reduce over-reliance. Instead, it led to an increase in ignoring ChatGPT’s correct recommendations. We conclude that the usage of ChatGPT is associated with over-reliance and it is not trivial to increase AI literacy to counter over-reliance.


Rheu, M. (MJ), & Cho, J. (2025). The Trap of AI Literacy: The Paradoxical Relationships Between College Students’ Use of LLMs, AI Literacy, and Fact-checking Behavior. Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 1–7. https://doi.org/10.1145/3706599.3719681
This study examines factors influencing users’ critical engagement with large language models (LLMs), focusing on fact-checking behavior. While LLMs transform how individuals acquire knowledge, their rapid adoption raises concerns about uncritical acceptance due to limitations like hallucinations. A survey of college students and young professionals revealed nuanced effects of LLM literacy. Understanding LLM processes, such as input, processing, and output, encourages fact-checking. However, self-efficacy and knowledge of LLM features paradoxically reduce verification by fostering reliance on machine heuristics and elevating the perceived credibility of outputs. These findings highlight the complex role of AI literacy in promoting critical engagement and the importance of education to deepen users’ technological understanding.

Scantamburlo, T., Cortés, A., Foffano, F., Barrué, C., Distefano, V., Pham, L., & Fabris, A. (2025). Artificial Intelligence Across Europe: A Study on Awareness, Attitude and Trust. IEEE Transactions on Artificial Intelligence, 6(2), 477–490. https://doi.org/10.1109/TAI.2024.3461633
This article presents the results of an extensive study investigating the opinions on artificial intelligence (AI) of a sample of 4006 European citizens from eight distinct countries (France, Germany, Italy, Netherlands, Poland, Romania, Spain, and Sweden). The aim of the study is to gain a better understanding of people’s views and perceptions within the European context, which is already marked by important policy actions and regulatory processes. To survey the perceptions of the citizens of Europe, we design and validate a new questionnaire (PAICE) structured around three dimensions: people’s awareness, attitude, and trust. We observe that while awareness is characterized by a low level of self-assessed competency, the attitude toward AI is very positive for more than half of the population. Reflecting on the collected results, we highlight implicit contradictions and identify trends that may interfere with the creation of an ecosystem of trust and the development of inclusive AI policies. The introduction of rules that ensure legal and ethical standards, along with the activity of high-level educational entities, and the promotion of AI literacy are identified as key factors in supporting a trustworthy AI ecosystem. We make some recommendations for AI governance focused on the European context and conclude with suggestions for future work.

Scharowski, N., Perrig, S. A. C., Aeschbach, L. F., Felten, N. von, Opwis, K., Wintersberger, P., & Brühlmann, F. (2025). To Trust or Distrust Trust Measures: Validating Questionnaires for Trust in AI (No. arXiv:2403.00582). arXiv. https://doi.org/10.48550/arXiv.2403.00582
Despite the importance of trust in human-AI interactions, researchers must adopt questionnaires from other disciplines that lack validation in the AI context. Motivated by the need for reliable and valid measures, we investigated the psychometric quality of two trust questionnaires, the Trust between People and Automation scale (TPA) by Jian et al. (2000) and the Trust Scale for the AI Context (TAI) by Hoffman et al. (2023). In a pre-registered online experiment (N = 1485), participants observed interactions with trustworthy and untrustworthy AI (autonomous vehicle and chatbot). Results support the psychometric quality of the TAI while revealing opportunities to improve the TPA, which we outline in our recommendations for using the two questionnaires. Furthermore, our findings provide additional empirical evidence of trust and distrust as two distinct constructs that may coexist independently. Building on our findings, we highlight the opportunities and added value of measuring both trust and distrust in human-AI research and advocate for further work on both constructs.

Scholz, D. D., Kraus ,Johannes, & and Miller, L. (2025). Measuring the Propensity to Trust in Automated Technology: Examining Similarities to Dispositional Trust in Other Humans and Validation of the PTT-A Scale. International Journal of Human–Computer Interaction, 41(2), 970–993. https://doi.org/10.1080/10447318.2024.2307691
In this work, an integrative theoretical structure for the propensity to trust (PTT) is derived from literature. In an online study (N = 669), the validity of the structure was assessed and compared in two domains: propensity to trust in humans (PTT-H) and propensity to trust in automated technology (PTT-A). Based on this, an economic scale to measure PTT-A was derived and its psychometric quality was explored based on the first and an additional second study. The observed correlational pattern to basic personality traits supports the convergent validity of PTT-A. Moreover, discriminative predictive validity of PTT-A over PTT-H was supported by its higher relationships to technology-related outcomes. Additionally, incremental validity of PTT-A over basic personality traits was supported. Finally, the internal validity of the scale was replicated in an independent sample and re-test reliability was established. The findings support the added value of integrating PTT-A in research on the interaction with automated technology.

Shang, R., & Hsieh, G. (2025). Trusting Your AI Agent Emotionally and Cognitively: Development and Validation of a Semantic Differential Scale for AI Trust. Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society, 1343–1356.
Trust is not just a cognitive issue but also an emotional one, yet the research in human-AI interactions has primarily focused on the cognitive route of trust development. Recent work has highlighted the importance of studying affective trust towards AI, especially in the context of emerging human-like LLM-powered conversational agents. However, there is a lack of validated and generalizable measures for the two-dimensional construct of trust in AI agents. To address this gap, we developed and validated a set of 27-item semantic differential scales for affective and cognitive trust through a scenario-based survey study. We then further validated and applied the scale through an experiment study. Our empirical findings showed how the emotional and cognitive aspects of trust interact with each other and collectively shape a person’s overall trust in AI agents. Our study methodology and findings also provide insights into the capability of the state-of-art LLMs to foster trust through different routes.

Soto-Sanfiel, M. T., Angulo-Brunet, A., & Lutz, C. (2024). The Scale of Artificial Intelligence Literacy for all (SAIL4ALL): A Tool for Assessing Knowledge on Artificial Intelligence in All Adult Populations and Settings. OSF. https://doi.org/10.31235/osf.io/bvyku
This study provides evidence of the psychometric quality of a new artificial intelligence (AI) literacy scale for comprehensive assessment of the concept across adult populations, regardless of the setting in which it is applied: the SAIL4ALL. It contains 56 items distributed across four themes [(1) What is AI? (a: Recognizing AI, Understanding Intelligence and Interdisciplinarity; b: General vs. Narrow); (2) What can AI do?; (3) How does AI work?; and (4) How should AI be used?], which can be used in combination or independently. Moreover, the scale is validated in two different response formats (true/false and 5-point Likert scale), each of which is applied depending on the context. The version with a true/false format is ideal for when AI literacy levels need to be assessed quickly because of its simplicity and ease of interpretation. Conversely, the 5-point Likert scale yields more nuanced responses based on the degree of confidence, providing richer insights into the respondents’ perceptions of their AI literacy. SAIL4ALL has demonstrated positive evidence of psychometric quality, and serves as a valuable tool for determining both actual and perceived knowledge of AI, thus guiding educational, organizational, and institutional AI literacy initiatives.

Stein, J.-P., Messingschlager, T., Gnambs, T., Hutmacher, F., & Appel, M. (2024). Attitudes towards AI: Measurement and associations with personality. Scientific Reports, 14(1), 2909. https://doi.org/10.1038/s41598-024-53335-2
Artificial intelligence (AI) has become an integral part of many contemporary technologies, such as social media platforms, smart devices, and global logistics systems. At the same time, research on the public acceptance of AI shows that many people feel quite apprehensive about the potential of such technologies—an observation that has been connected to both demographic and sociocultural user variables (e.g., age, previous media exposure). Yet, due to divergent and often ad-hoc measurements of AI-related attitudes, the current body of evidence remains inconclusive. Likewise, it is still unclear if attitudes towards AI are also affected by users’ personality traits. In response to these research gaps, we offer a two-fold contribution. First, we present a novel, psychologically informed questionnaire (ATTARI-12) that captures attitudes towards AI as a single construct, independent of specific contexts or applications. Having observed good reliability and validity for our new measure across two studies (N1 = 490; N2 = 150), we examine several personality traits—the Big Five, the Dark Triad, and conspiracy mentality—as potential predictors of AI-related attitudes in a third study (N3 = 298). We find that agreeableness and younger age predict a more positive view towards artificially intelligent technology, whereas the susceptibility to conspiracy beliefs connects to a more negative attitude. Our findings are discussed considering potential limitations and future directions for research and practice.

Tully, S., Longoni, C., & Appel, G. (2025). Lower Artificial Intelligence Literacy Predicts Greater AI Receptivity. Journal of Marketing. https://doi.org/10.1177/00222429251314491
As artificial intelligence (AI) transforms society, understanding factors that influence AI receptivity is increasingly important. The current research investigates which types of consumers have greater AI receptivity. Contrary to expectations revealed in four surveys, cross country data and six additional studies find that people with lower AI literacy are typically more receptive to AI. This lower literacy-greater receptivity link is not explained by differences in perceptions of AI’s capability, ethicality, or feared impact on humanity. Instead, this link occurs because people with lower AI literacy are more likely to perceive AI as magical and experience feelings of awe in the face of AI’s execution of tasks that seem to require uniquely human attributes. In line with this theorizing, the lower literacy-higher receptivity link is mediated by perceptions of AI as magical and is moderated among tasks not assumed to require distinctly human attributes. These findings suggest that companies may benefit from shifting their marketing efforts and product development towards consumers with lower AI literacy. Additionally, efforts to demystify AI may inadvertently reduce its appeal, indicating that maintaining an aura of magic around AI could be beneficial for adoption.

Weber, P., Pinski, M., & Baum, L. (2023). Toward an Objective Measurement of AI Literacy. https://aisel.aisnet.org/pacis2023/60/
Humans multitudinously interact with Artificial Intelligence (AI) as it permeates every aspect of contemporary professional and private life. The socio-technical competencies of humans, i.e., their AI literacy, shape human-AI interactions. While academia does explore AI literacy measurement, current literature exclusively approaches the topic from a subjective perspective. This study draws on a well-established scale development procedure employing ten expert interviews, two card-sorting rounds, and a betweensubject comparison study with 88 participants in two groups to define, conceptualize, and empirically validate an objective measurement instrument for AI literacy. With 16 items, our developed instrument discriminates between an AI-literate test and a control group. Furthermore, the structure of our instrument allows us to distinctly assess AI literacy aspects. We contribute to IS education research by providing a new instrument and conceptualizing AI literacy, incorporating critical themes from the literature. Practitioners may employ our instrument to assess AI literacy in their organizations.

Wojton, H. M., Porter, D., T Lane, S., Bieber, C., & Madhavan, P. (2020). Initial validation of the trust of automated systems test (TOAST). The Journal of Social Psychology, 160(6), 735–750. https://doi.org/10.1080/00224545.2020.1749020
Trust is a key determinant of whether people rely on automated systems in the military and the public. However, there is currently no standard for measuring trust in automated systems. In the present studies, we propose a scale to measure trust in automated systems that is grounded in current research and theory on trust formation, which we refer to as the Trust in Automated Systems Test (TOAST). We evaluated both the reliability of the scale structure and criterion validity using independent, military-affiliated and civilian samples. In both studies we found that the TOAST exhibited a two-factor structure, measuring system understanding and performance (respectively), and that factor scores significantly predicted scores on theoretically related constructs demonstrating clear criterion validity. We discuss the implications of our findings for advancing the empirical literature and in improving interface design.
Zhang, B., & Dafoe, A. (2019). Artificial Intelligence: American Attitudes and Trends. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3312874

----------

# AI Perception

Ayanwale, M. A., Sanusi, I. T., Adelana, O. P., Aruleba, K. D., & Oyelere, S. S. (2022). Teachers’ readiness and intention to teach artificial intelligence in schools. Computers and Education: Artificial Intelligence, 3, 100099. https://doi.org/10.1016/j.caeai.2022.100099
The emergence of artificial intelligence (AI) as a subject to be incorporated into K-12 educational levels places new demand on relevant stakeholders, especially teachers that drive the teaching and learning process. It is therefore important to understand how ready teachers are to teach the emerging subject as the success of AI education would probably be closely dependent on the readiness of teachers. As a result, this study presents an insight into factors influencing the behavioural intention and readiness of Nigerian in-service teachers to teach artificial intelligence. A total of 368 teachers, from elementary to high school participated in the study. We utilised quantitative methodology using variance-based structural equation modelling to understand the relationship among the eight variables (AI anxiety, perceived usefulness, AI for social good, Attitude towards using AI, perceived confidence in teaching AI, relevance of AI, AI readiness, and behavioural intention) considered in the study. The result indicated that confidence in teaching AI predicts intention to teach AI while AI relevance strongly predicts readiness to teach AI. While other factors influence the teaching of AI, anxiety and social good could not predict teachers’ intention and readiness to implement AI in classrooms respectively. We discussed the implication of our findings in relation to AI implementation in schools and highlight future directions.


Babiker, A., Alshakhsi, S., Al-Thani, D., Montag, C., & Ali, R. (2024). Attitude Towards AI: Potential Influence of Conspiracy Belief, XAI Experience and Locus of Control. International Journal of Human–Computer Interaction, 1–13. https://doi.org/10.1080/10447318.2024.2401249
The proliferation of Artificial Intelligence (AI) technologies, exemplified by Large Language Models (LLM), has ushered in a transformative era across various fields. As the AI revolution will impact societies in complex and uncertain ways, it is likely that persons tending towards belief of conspiracy theories also tend to form more negative and less positive attitudes towards AI. Such persons might believe that some evil force will use AI to destroy human mankind. Drawing on the Interplay of Modality, Person, Area, Country/Culture, and Transparency categories (IMPACT) framework, this study aims to investigate the interplay of locus of control (LOC), belief in conspiracy theories, and the perception of the importance and availability of eXplainable AI (XAI) on attitudes towards AI (measured via AI acceptance and fear). The study used an online survey with 281 participants from the UK and 281 from the Arab world. Statistical analysis revealed that in the UK but not in the Arab sample, female participants reported higher fear of AI and lower acceptance of AI compared to males. The regression results consistently confirmed the role of internal LOC, perceived XAI importance, and perceived availability of XAI in fostering AI acceptance, as well as the role of belief in conspiracy theories, external LOC, and perceiving availability of XAI as being low in increasing fear of AI. The perceived availability of XAI emerges as a crucial influencing factor; addressing it appropriately could enhance societal awareness and acceptance of AI while reducing fear. Personal factors and XAI influence attitudes towards AI in both Arab and UK cultures, enhancing result robustness and revealing nuanced differences.

Bewersdorff, A., Hornberger, M., Nerdel, C., & Schiff, D. S. (2025). AI advocates and cautious critics: How AI attitudes, AI interest, use of AI, and AI literacy build university students’ AI self-efficacy. Computers and Education: Artificial Intelligence, 8, 100340. https://doi.org/10.1016/j.caeai.2024.100340
This study investigates how cognitive, affective, and behavioral variables related to artificial intelligence (AI) build AI self-efficacy among university students. Based on these variables, we identify three meaningful student groups, which can guide educational initiatives. We recruited 1465 undergraduate and graduate students from the United States, the United Kingdom, and Germany and measured their AI self-efficacy, AI literacy, interest in AI, attitudes towards AI, and AI use. Using a path model, we examine the correlations and paths among these variables. Results reveal that AI usage and positive AI attitudes significantly predict interest in AI, which in turn and together with AI literacy, enhance AI self-efficacy. Moreover, using Gaussian Mixture Models, we identify three groups of students: “AI Advocates,” “Cautious Critics,” and “Pragmatic Observers,” each exhibiting unique patterns of AI-related cognitive, affective, and behavioral traits. Our findings demonstrate the necessity of educational strategies that not only focus on AI literacy but also aim to foster students’ AI attitudes, usage, and interest to effectively promote AI self-efficacy. Furthermore, we argue that educators who aim to design inclusive AI educational programs should take into account the distinct needs of different student groups identified in this study.



Chen, A., Kim, S. S. Y., Dharmasiri, A., Russakovsky, O., & Fan, J. E. (2025). Portraying Large Language Models as Machines, Tools, or Companions Affects What Mental Capacities Humans Attribute to Them. Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 1–14. https://doi.org/10.1145/3706599.3719710
As large language models (LLMs) become increasingly popular and prevalent in media and daily conversations, individuals encounter different portrayals of LLMs from various sources. It is important to understand how these portrayals can shape their beliefs about LLMs as this can have downstream impacts on adoption and usage behaviors. In this work, we investigate what mental capacities individuals attribute to LLMs after being exposed to short videos adopting one of three portrayals: mechanistic (LLMs as machines), functional (LLMs as tools), and intentional (LLMs as companions). We find that the intentional portrayal increases the attribution of mental capacities to LLMs, and that individuals tend to attribute mind-related capacities the most, followed by heart- then bodyrelated capacities. We discuss the implications of these findings, provide recommendations on how to portray LLMs, and outline directions for future research.


Choudhury, A., & Shamszare, H. (2023). Investigating the Impact of User Trust on the Adoption and Use of ChatGPT: Survey Analysis. Journal of Medical Internet Research, 25, e47184. https://doi.org/10.2196/47184
Background
ChatGPT (Chat Generative Pre-trained Transformer) has gained popularity for its ability to generate human-like responses. It is essential to note that overreliance or blind trust in ChatGPT, especially in high-stakes decision-making contexts, can have severe consequences. Similarly, lacking trust in the technology can lead to underuse, resulting in missed opportunities.
Objective
This study investigated the impact of users’ trust in ChatGPT on their intent and actual use of the technology. Four hypotheses were tested: (1) users’ intent to use ChatGPT increases with their trust in the technology; (2) the actual use of ChatGPT increases with users’ intent to use the technology; (3) the actual use of ChatGPT increases with users’ trust in the technology; and (4) users’ intent to use ChatGPT can partially mediate the effect of trust in the technology on its actual use.
Methods
This study distributed a web-based survey to adults in the United States who actively use ChatGPT (version 3.5) at least once a month between February 2023 through March 2023. The survey responses were used to develop 2 latent constructs: Trust and Intent to Use, with Actual Use being the outcome variable. The study used partial least squares structural equation modeling to evaluate and test the structural model and hypotheses.
Results
In the study, 607 respondents completed the survey. The primary uses of ChatGPT were for information gathering (n=219, 36.1%), entertainment (n=203, 33.4%), and problem-solving (n=135, 22.2%), with a smaller number using it for health-related queries (n=44, 7.2%) and other activities (n=6, 1%). Our model explained 50.5% and 9.8% of the variance in Intent to Use and Actual Use, respectively, with path coefficients of 0.711 and 0.221 for Trust on Intent to Use and Actual Use, respectively. The bootstrapped results failed to reject all 4 null hypotheses, with Trust having a significant direct effect on both Intent to Use (β=0.711, 95% CI 0.656-0.764) and Actual Use (β=0.302, 95% CI 0.229-0.374). The indirect effect of Trust on Actual Use, partially mediated by Intent to Use, was also significant (β=0.113, 95% CI 0.001-0.227).
Conclusions
Our results suggest that trust is critical to users’ adoption of ChatGPT. It remains crucial to highlight that ChatGPT was not initially designed for health care applications. Therefore, an overreliance on it for health-related advice could potentially lead to misinformation and subsequent health risks. Efforts must be focused on improving the ChatGPT’s ability to distinguish between queries that it can safely handle and those that should be redirected to human experts (health care professionals). Although risks are associated with excessive trust in artificial intelligence–driven chatbots such as ChatGPT, the potential risks can be reduced by advocating for shared accountability and fostering collaboration between developers, subject matter experts, and human factors researchers.

Ding, Y., & Najaf, M. (2024). Interactivity, humanness, and trust: A psychological approach to AI chatbot adoption in e-commerce. BMC Psychology, 12(1), 595. https://doi.org/10.1186/s40359-024-02083-z
This study aims to investigate the impact of interactivity and perceived humanness on trust toward AI chatbots in the e-commerce setting. Moreover, this study also aims to examine the mediation effect of trust toward AI chatbots in the relationship between interactivity and intention to adopt AI chatbots for e-commerce as well as in the relationship between perceived humanness and intention to adopt chatbots for e-commerce. This study used a time lag approach to collect the data from 343 customers from the southern region of China. The data were collected online through a questionnaire designed in Chinese language using a survey firm. The findings of this study indicated that there is a significant impact of interactivity and humanness on the trust toward chatbots. Moreover, the findings of this study indicated that there is a significant mediating effect of trust toward chatbots in the relationships of interactivity and perceived humanness to adopt chatbots for e-commerce. In addition, this study found a significant moderating influence on the perceived enjoyment of using chatbots in e-commerce settings. This study provides a unique perspective of expectation-confirmation theory for adopting emerging technologies for online shopping and also provides insights for designers and business firms to develop businesses to facilitate the AI chatbot feature for e-commerce.

Dvorak, F., Stumpf, R., Fehrler, S., & Fischbacher, U. (2025). Adverse reactions to the use of large language models in social interactions. PNAS Nexus, 4(4), pgaf112. https://doi.org/10.1093/pnasnexus/pgaf112
Large language models (LLMs) are poised to reshape the way individuals communicate and interact. While this form of AI has the potential to efficiently make many human decisions, there is limited understanding of how individuals will respond to its use in social interactions. In particular, it remains unclear how individuals interact with LLMs when the interaction has consequences for other people. Here, we report the results of a large-scale, preregistered online experiment (n=3,552) showing that human players’ fairness, trust, trustworthiness, cooperation, and coordination in economic two-player games decrease when the decision of the interaction partner is taken over by ChatGPT. On the contrary, we observe no adverse reactions when individuals are uncertain whether they are interacting with a human or a LLM. At the same time, participants often delegate decisions to the LLM, especially when the model’s involvement is not disclosed, and individuals have difficulty distinguishing between decisions made by humans and those made by AI.


Fang, C. M., Liu, A. R., Danry, V., Lee, E., Chan, S. W. T., Pataranutaporn, P., Maes, P., Phang, J., Lampe, M., Ahmad, L., & Agarwal, S. (2025). How AI and Human Behaviors Shape Psychosocial Effects of Chatbot Use: A Longitudinal Randomized Controlled Study (No. arXiv:2503.17473). arXiv. https://doi.org/10.48550/arXiv.2503.17473
AI chatbots, especially those with voice capabilities, have become increasingly human-like, with more users seeking emotional support and companionship from them. Concerns are rising about how such interactions might impact users’ loneliness and socialization with real people. We conducted a four-week randomized, controlled, IRB-approved experiment (n=981, >300K messages) to investigate how AI chatbot interaction modes (text, neutral voice, and engaging voice) and conversation types (open-ended, non-personal, and personal) influence psychosocial outcomes such as loneliness, social interaction with real people, emotional dependence on AI and problematic AI usage. Results showed that while voice-based chatbots initially appeared beneficial in mitigating loneliness and dependence compared with text-based chatbots, these advantages diminished at high usage levels, especially with a neutral-voice chatbot. Conversation type also shaped outcomes: personal topics slightly increased loneliness but tended to lower emotional dependence compared with open-ended conversations, whereas non-personal topics were associated with greater dependence among heavy users. Overall, higher daily usage - across all modalities and conversation types - correlated with higher loneliness, dependence, and problematic use, and lower socialization. Exploratory analyses revealed that those with stronger emotional attachment tendencies and higher trust in the AI chatbot tended to experience greater loneliness and emotional dependence, respectively. These findings underscore the complex interplay between chatbot design choices (e.g., voice expressiveness) and user behaviors (e.g., conversation content, usage frequency). We highlight the need for further research on whether chatbots’ ability to manage emotional content without fostering dependence or replacing human relationships benefits overall well-being.

Glikson, E., & Woolley, A. W. (2020). Human Trust in Artificial Intelligence: Review of Empirical Research. Academy of Management Annals, 14(2), 627–660. https://doi.org/10.5465/annals.2018.0057
Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into organizations critically depends on workers’ trust in AI technology. This review explains how AI differs from other technologies and presents the existing empirical research on the determinants of human “trust” in AI, conducted in multiple disciplines over the last 20 years. Based on the reviewed literature, we identify the form of AI representation (robot, virtual, and embedded) and its level of machine intelligence (i.e., its capabilities) as important antecedents to the development of trust and propose a framework that addresses the elements that shape users’ cognitive and emotional trust. Our review reveals the important role of AI’s tangibility, transparency, reliability, and immediacy behaviors in developing cognitive trust, and the role of AI’s anthropomorphism specifically for emotional trust. We also note several limitations in the current evidence base, such as the diversity of trust measures and overreliance on short-term, small sample, and experimental studies, where the development of trust is likely to be different than in longer-term, higher stakes field environments. Based on our review, we suggest the most promising paths for future research.

Grassini, S. (2023). Development and validation of the AI attitude scale (AIAS-4): A brief measure of general attitude toward artificial intelligence. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1191628
The rapid advancement of artificial intelligence (AI) has generated an increasing demand for tools that can assess public attitudes toward AI. This study proposes the development and the validation of the AI Attitude Scale (AIAS), a concise self-report instrument designed to evaluate public perceptions of AI technology. The first version of the AIAS that the present manuscript proposes comprises five items, including one reverse-scored item, which aims to gauge individuals’ beliefs about AI’s influence on their lives, careers, and humanity overall. The scale is designed to capture attitudes toward AI, focusing on the perceived utility and potential impact of technology on society and humanity. The psychometric properties of the scale were investigated using diverse samples in two separate studies. An exploratory factor analysis was initially conducted on a preliminary 5-item version of the scale. Such exploratory validation study revealed the need to divide the scale into two factors. While the results demonstrated satisfactory internal consistency for the overall scale and its correlation with related psychometric measures, separate analyses for each factor showed robust internal consistency for Factor 1 but insufficient internal consistency for Factor 2. As a result, a second version of the scale is developed and validated, omitting the item that displayed weak correlation with the remaining items in the questionnaire. The refined final 1-factor, 4-item AIAS demonstrated superior overall internal consistency compared to the initial 5-item scale and the proposed factors. Further confirmatory factor analyses, performed on a different sample of participants, confirmed that the 1-factor model (4-items) of the AIAS exhibited an adequate fit to the data, providing additional evidence for the scale’s structural validity and generalizability across diverse populations. In conclusion, the analyses reported in this article suggest that the developed and validated 4-items AIAS can be a valuable instrument for researchers and professionals working on AI development who seek to understand and study users’ general attitudes toward AI.


Gröner, F., & Chiou, E. K. (2024). Investigating the Impact of User Interface Designs on Expectations About Large Language Models’ Capabilities. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 68(1), 155–161. https://doi.org/10.1177/10711813241260399
Large Language Models (LLMs) with their novel conversational interaction format could create incorrectly calibrated expectations about their capabilities. The present study investigates human expectations toward a generic LLM?s capabilities and limitations. Participants of an online study were shown a series of prompts that cover a wide range of tasks and asked to assess the likelihood of the LLM being able to help with those tasks. The result is a catalog of people?s general expectations of LLM capabilities across various task domains. Depending on the actual capabilities of a specific system, this could inform developers of potential over- or under-reliance on this technology due to these misconceptions. To explore a potential way of correcting misconceptions we also attempted to manipulate their expectations with three different interface designs. In most of the tested task domains, such as computation and text processing, however, these seem to be insufficient to overpower people?s initial expectations.

Gu, C., Zhang, Y., & Zeng, L. (2024). Exploring the mechanism of sustained consumer trust in AI chatbots after service failures: A perspective based on attribution and CASA theories. Humanities and Social Sciences Communications, 11(1), 1–12. https://doi.org/10.1057/s41599-024-03879-5
In recent years, artificial intelligence (AI) technology has been widely employed in brand customer service. However, the inherent limitations of computer-generated natural language content occasionally lead to failures in human-computer interactions, potentially damaging a company’s brand image. Therefore, it is crucial to explore how to maintain consumer trust after AI chatbots fail to provide successful service. This study constructs a model to examine the impact of social interaction cues and anthropomorphic factors on users’ sustained trust by integrating the Computers As Social Actors (CASA) theory with attribution theory. An empirical analysis of 462 survey responses reveals that CASA factors (perceived anthropomorphic characteristics, perceived empathic abilities, and perceived interaction quality) can effectively enhance user trust in AI customer service following interaction failures. This process of sustaining trust is mediated through different attributions of failure. Furthermore, AI anxiety, as a cognitive characteristic of users, not only negatively impacts sustained trust but also significantly moderates the effect of internal attributions on sustained trust. These findings expand the research domain of human-computer interaction and provide insights for the practical development of AI chatbots in communication and customer service fields.

Lee, E., Pataranutaporn, P., Amores, J., & Maes, P. (2024). Super-intelligence or Superstition? Exploring Psychological Factors Influencing Belief in AI Predictions about Personal Behavior (No. arXiv:2408.06602). arXiv. https://doi.org/10.48550/arXiv.2408.06602
Could belief in AI predictions be just another form of superstition? This study investigates psychological factors that influence belief in AI predictions about personal behavior, comparing it to belief in astrology- and personality-based predictions. Through an experiment with 238 participants, we examined how cognitive style, paranormal beliefs, AI attitudes, personality traits, and other factors affect perceived validity, reliability, usefulness, and personalization of predictions from different sources. Our findings reveal that belief in AI predictions is positively correlated with belief in predictions based on astrology and personality psychology. Notably, paranormal beliefs and positive attitudes about AI significantly increased perceived validity, reliability, usefulness, and personalization of AI predictions. Conscientiousness was negatively correlated with belief in predictions across all sources, and interest in the prediction topic increased believability across predictions. Surprisingly, we found no evidence that cognitive style has an impact on belief in fictitious AI-generated predictions. These results highlight the “rational superstition” phenomenon in AI, where belief is driven more by mental heuristics and intuition than critical evaluation. This research advances our understanding of the psychology of human-AI interaction, offering insights into designing and promoting AI systems that foster appropriate trust and skepticism, critical for responsible integration in an increasingly AI-driven world.

Menon, D., & Shilpa, K. (2023). “Chatting with ChatGPT”: Analyzing the factors influencing users’ intention to Use the Open AI’s ChatGPT using the UTAUT model. Heliyon, 9(11), e20962. https://doi.org/10.1016/j.heliyon.2023.e20962
Open AI’s ChatGPT has emerged as a popular AI language model that can engage in natural language conversations with users. Based on a qualitative research approach using semistructured interviews with 32 ChatGPT users from India, this study examined the factors influencing users’ acceptance and use of ChatGPT using the unified theory of acceptance and usage of technology (UTAUT) model. The study results demonstrated that the four factors of UTAUT, along with two extended constructs, i.e. perceived interactivity and privacy concerns, can explain users’ interaction and engagement with ChatGPT. The study also found that age and experience can moderate the impact of various factors on the use of ChatGPT. The theoretical and practical implications of the study were also discussed.

Montag, C., & Ali, R. (2025). Can We Assess Attitudes Toward AI with Single Items? Associations with Existing Attitudes Toward AI Measures and Trust in ChatGPT. Journal of Technology in Behavioral Science. https://doi.org/10.1007/s41347-025-00481-7
A growing number of researchers investigate individual differences in attitudes toward Artificial Intelligence (AI), which is not surprising given that the AI revolution is impacting societies around the globe. Different frameworks have been proposed to study both positive and negative attitudes toward AI. To our knowledge, the present work is the first to simultaneously investigate the ATAI (Attitudes Toward Artificial Intelligence Scale) and the GAAIS (General Attitudes Towards Artificial Intelligence Scale). Further, two single items assessing positive and negative attitudes toward AI were added to the study to see if they would grasp substantial parts of the variance of the already established ATAI and GAAIS inventories. Correlations were of moderate to large effect size when comparing associations between the single-item measures and both ATAI and GAAI scales (German speaking sample 1 = 151 participants; German speaking sample 2 = 386). Finally, also associations with trusting the generative AI ChatGPT were included as external validation measurement in both investigated samples. Results revealed that all attitudes toward AI measures were associated with trusting ChatGPT. Moreover, a stepwise regression model demonstrated that the acceptance scale of the ATAI was the best predictor for trust in ChatGPT in sample 1, with more predictors in sample 2. The present work shows substantial overlap between the available attitudes towards AI measures, and this could be replicated in two samples. These insights can help future researchers and AI designers to choose the appropriate survey tool when considering to assess attitudes toward AI.

Pataranutaporn, P., Liu, R., Finn, E., & Maes, P. (2023). Influencing human–AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy and effectiveness. Nature Machine Intelligence, 5(10), 1076–1086. https://doi.org/10.1038/s42256-023-00720-7
As conversational agents powered by large language models become more human-like, users are starting to view them as companions rather than mere assistants. Our study explores how changes to a person’s mental model of an AI system affects their interaction with the system. Participants interacted with the same conversational AI, but were influenced by different priming statements regarding the AI’s inner motives: caring, manipulative or no motives. Here we show that those who perceived a caring motive for the AI also perceived it as more trustworthy, empathetic and better-performing, and that the effects of priming and initial mental models were stronger for a more sophisticated AI model. Our work also indicates a feedback loop in which the user and AI reinforce the user’s mental model over a short time; further work should investigate long-term effects. The research highlights the importance of how AI systems are introduced can notably affect the interaction and how the AI is experienced.


Ravšelj, D., Keržič, D., Tomaževič, N., Umek, L., Brezovar, N., A. Iahad, N., Abdulla, A. A., Akopyan, A., Aldana Segura, M. W., AlHumaid, J., Allam, M. F., Alló, M., Andoh, R. P. K., Andronic, O., Arthur, Y. D., Aydın, F., Badran, A., Balbontín-Alvarado, R., Ben Saad, H., … Aristovnik, A. (2025). Higher education students’ perceptions of ChatGPT: A global study of early reactions. PLOS ONE, 20(2), e0315011. https://doi.org/10.1371/journal.pone.0315011
The paper presents the most comprehensive and large-scale global study to date on how higher education students perceived the use of ChatGPT in early 2024. With a sample of 23,218 students from 109 countries and territories, the study reveals that students primarily used ChatGPT for brainstorming, summarizing texts, and finding research articles, with a few using it for professional and creative writing. They found it useful for simplifying complex information and summarizing content, but less reliable for providing information and supporting classroom learning, though some considered its information clearer than that from peers and teachers. Moreover, students agreed on the need for AI regulations at all levels due to concerns about ChatGPT promoting cheating, plagiarism, and social isolation. However, they believed ChatGPT could potentially enhance their access to knowledge and improve their learning experience, study efficiency, and chances of achieving good grades. While ChatGPT was perceived as effective in potentially improving AI literacy, digital communication, and content creation skills, it was less useful for interpersonal communication, decision-making, numeracy, native language proficiency, and the development of critical thinking skills. Students also felt that ChatGPT would boost demand for AI-related skills and facilitate remote work without significantly impacting unemployment. Emotionally, students mostly felt positive using ChatGPT, with curiosity and calmness being the most common emotions. Further examinations reveal variations in students’ perceptions across different socio-demographic and geographic factors, with key factors influencing students’ use of ChatGPT also being identified. Higher education institutions’ managers and teachers may benefit from these findings while formulating the curricula and instructions/regulations for ChatGPT use, as well as when designing the teaching methods and assessment tools. Moreover, policymakers may also consider the findings when formulating strategies for secondary and higher education system development, especially in light of changing labor market needs and related digital skills development.


Razin, Y. S., & Feigh, K. M. (2024). Converging Measures and an Emergent Model: A Meta-Analysis of Human-Machine Trust Questionnaires. J. Hum.-Robot Interact., 13(4), 58:1-58:41. https://doi.org/10.1145/3677614
Trust is crucial for technological acceptance, continued usage, and teamwork. However, human-robot trust, and human-machine trust more generally, suffer from terminological disagreement and construct proliferation. By comparing, mapping, and analyzing well-constructed trust survey instruments, this work uncovers a consensus structure of trust in human–machine interaction. To do so, we identify the most frequently cited and best-validated human-machine and human-robot trust questionnaires as well as the best-established factors that form the dimensions and antecedents of such trust. To reduce both confusion and construct proliferation, we provide a detailed mapping of terminology between questionnaires. Furthermore, we perform a meta-analysis of the regression models which emerged from the experiments that employed multi-factorial survey instruments. Based on this meta-analysis, we provide the most complete, experimentally validated model of human-machine and human-robot trust to date. This convergent model establishes an integrated framework for future research. It determines the current boundaries of trust measurement and where further investigation and validation are necessary. We close by discussing how to choose an appropriate trust survey instrument and how to design for trust. By identifying the internal workings of trust, a more complete basis for measuring trust is developed that is widely applicable.


Sindermann, C., Sha, P., Zhou, M., Wernicke, J., Schmitt, H. S., Li, M., Sariyska, R., Stavrou, M., Becker, B., & Montag, C. (2021). Assessing the Attitude Towards Artificial Intelligence: Introduction of a Short Measure in German, Chinese, and English Language. KI - Künstliche Intelligenz, 35(1), 109–118. https://doi.org/10.1007/s13218-020-00689-0
In the context of (digital) human–machine interaction, people are increasingly dealing with artificial intelligence in everyday life. Through this, we observe humans who embrace technological advances with a positive attitude. Others, however, are particularly sceptical and claim to foresee substantial problems arising from such uses of technology. The aim of the present study was to introduce a short measure to assess the Attitude Towards Artificial Intelligence (ATAI scale) in the German, Chinese, and English languages. Participants from Germany (N = 461; 345 females), China (N = 413; 145 females), and the UK (N = 84; 65 females) completed the ATAI scale, for which the factorial structure was tested and compared between the samples. Participants from Germany and China were additionally asked about their willingness to interact with/use self-driving cars, Siri, Alexa, the social robot Pepper, and the humanoid robot Erica, which are representatives of popular artificial intelligence products. The results showed that the five-item ATAI scale comprises two negatively associated factors assessing (1) acceptance and (2) fear of artificial intelligence. The factor structure was found to be similar across the German, Chinese, and UK samples. Additionally, the ATAI scale was validated, as the items on the willingness to use specific artificial intelligence products were positively associated with the ATAI Acceptance scale and negatively with the ATAI Fear scale, in both the German and Chinese samples. In conclusion we introduce a short, reliable, and valid measure on the attitude towards artificial intelligence in German, Chinese, and English language.

Zhang, B., & Dafoe, A. (2019). Artificial Intelligence: American Attitudes and Trends. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.3312874
This report presents a broad look at the American public’s attitudes toward artificial intelligence (AI) and AI governance, based on findings from a nationally representative survey of 2,000 American adults. As the study of the public opinion toward AI is relatively new, we aimed for breadth over depth, with our questions touching on: workplace automation; attitudes regarding international cooperation; the public’s trust in various actors to develop and regulate AI; views about the importance and likely impact of different AI governance challenges; and historical and cross-national trends in public opinion regarding AI. Our results provide preliminary insights into the character of US public opinion regarding AI.

Markus, A., Carolus, A., & Wienrich, C. (2025). Objective Measurement of AI Literacy: Development and Validation of the AI Competency Objective Scale (AICOS) (No. arXiv:2503.12921). arXiv. https://doi.org/10.48550/arXiv.2503.12921
As Artificial Intelligence (AI) becomes more pervasive in various aspects of life, AI literacy is becoming a fundamental competency that enables individuals to move safely and competently in an AI-pervaded world. There is a growing need to measure this competency, e.g., to develop targeted educational interventions. Although several measurement tools already exist, many have limitations regarding subjective data collection methods, target group differentiation, validity, and integration of current developments such as Generative AI Literacy. This study develops and validates the AI Competency Objective Scale (AICOS) for measuring AI literacy objectively. The presented scale addresses weaknesses and offers a robust measurement approach that considers established competency and measurement models, captures central sub-competencies of AI literacy, and integrates the dimension of Generative AI Literacy. The AICOS provides a sound and comprehensive measure of AI literacy, and initial analyses show potential for a modular structure. Furthermore, a first edition of a short version of the AICOS is developed. Due to its methodological foundation, extensive validation, and integration of recent developments, the test represents a valuable resource for scientific research and practice in educational institutions and professional contexts. The AICOS significantly contributes to the development of standardized measurement instruments and enables the targeted assessment and development of AI skills in different target groups.


Salah, M., Alhalbusi, H., Ismail, M. M., & Abdelfattah, F. (2024). Chatting with ChatGPT: Decoding the mind of Chatbot users and unveiling the intricate connections between user perception, trust and stereotype perception on self-esteem and psychological well-being. Current Psychology, 43(9), 7843–7858. https://doi.org/10.1007/s12144-023-04989-0
Artificial Intelligence (AI) technology has revolutionized how we interact with information and entertainment, with ChatGPT, a language model developed by OpenAI, being among its prominent applications. However, knowledge regarding the psychological impact of interacting with ChatGPT is limited. This study investigated the relationships between trust in ChatGPT; ChatGPT’s user perceptions; perceived stereotyping by ChatGPT; and two psychological outcomes, namely, psychological well-being and self-esteem. This study hypothesized that the former three variables exhibit a positive direct relationship with self-esteem. Additionally, the study proposed that job anxiety moderates the associations among trust in ChatGPT, user perceptions of ChatGPT, and psychological well-being. Using a survey design, data were collected from 732 participants and analyzed using SEM and SmartPLS analysis. Notably, perceived stereotyping by ChatGPT significantly predicted self-esteem, while user perceptions of ChatGPT and trust in ChatGPT exhibited a positive direct relationship with self-esteem. Additionally, job anxiety moderated the relationship between ChatGPT’s user perceptions and psychological well-being. These results provide important insights into the psychological effects of interacting with AI technology and highlight job anxiety’s role in moderating these effects. This study’s findings have implications for developing and using AI technology in various fields, including mental health and human-robot interactions.



Küper, A., Lodde, G. C., Livingstone, E., Schadendorf, D., & Krämer, N. (2025). Psychological Factors Influencing Appropriate Reliance on AI-enabled Clinical Decision Support Systems: Experimental Web-Based Study Among Dermatologists. Journal of Medical Internet Research, 27(1), e58660. https://doi.org/10.2196/58660
Background: Artificial intelligence (AI)–enabled decision support systems are critical tools in medical practice; however, their reliability is not absolute, necessitating human oversight for final decision-making. Human reliance on such systems can vary, influenced by factors such as individual psychological factors and physician experience.
Objective: This study aimed to explore the psychological factors influencing subjective trust and reliance on medical AI’s advice, specifically examining relative AI reliance and relative self-reliance to assess the appropriateness of reliance.
Methods: A survey was conducted with 223 dermatologists, which included lesion image classification tasks and validated questionnaires assessing subjective trust, propensity to trust technology, affinity for technology interaction, control beliefs, need for cognition, as well as queries on medical experience and decision confidence.
Results: A 2-tailed t test revealed that participants’ accuracy improved significantly with AI support (t222=−3.3; P&lt;.001; Cohen d=4.5), but only by an average of 1% (1/100). Reliance on AI was stronger for correct advice than for incorrect advice (t222=4.2; P&lt;.001; Cohen d=0.1). Notably, participants demonstrated a mean relative AI reliance of 10.04% (139/1384) and a relative self-reliance of 85.6% (487/569), indicating a high level of self-reliance but a low level of AI reliance. Propensity to trust technology influenced AI reliance, mediated by trust (indirect effect=0.024, 95% CI 0.008-0.042; P&lt;.001), and medical experience negatively predicted AI reliance (indirect effect=–0.001, 95% CI –0.002 to −0.001; P&lt;.001).
Conclusions: The findings highlight the need to design AI support systems in a way that assists less experienced users with a high propensity to trust technology to identify potential AI errors, while encouraging experienced physicians to actively engage with system recommendations and potentially reassess initial decisions.



Wischnewski, M., Doebler, P., & Krämer, N. (2025). Development and validation of the Trust in AI Scale (TAIS). OSF. https://doi.org/10.31234/osf.io/eqa9y_v1
In everyday life, users increasingly interact and communicate with AI systems. Despite the importance of trust in AI as an influencing factor for this interaction, there is a shortage of validated scales to reliably measure users’ trust. In this paper, we present a theory-driven development and validation of the Trust in AI scale (TAIS) that consists of the subdimensions ability, integrity, transparency, unbiasedness, vigilance, and global trust. To validate the scale, we conducted two studies. In study 1 (N = 883 participants), we derived 57 items from theory and existing scales for which an exploratory factor analysis resulted in a 30 item scale. In study 2 (N = 1204 participants), we tested the psychometric quality of the scale through confirmatory factor analysis for ordinal data. Employing a bifactor model with global trust as the higher-order factor, our results confirm the six-factor structure. Correlational results of context variables and related scales support the convergent validity of the scale. Results show that existing scales rather correlate with the global trust factor but less with specific factors (especially vigilance) - indicating that the TAIS scale helps to uncover new facets of trust and thereby goes beyond what existing, less validated scales can provide.


---------------


# Trust

Adams, B., Bruyn, L. E., Houde, S., Angelopoulos, P., Iwasa-Madge, K., & McCann, C. (2003). Trust in automated systems. Ministry of National Defence, 3–7.
This report reviews research literature pertaining to trust in automated systems. Based on the review, we argue that trust in automation has many similarities with trust in the interpersonal domain, but also several unique dynamics and influences. Existing research has focused primarily on trust in automation that has an executive or control function, and to a lesser extent, has considered trust in automation that is designed to present information to operators (e.g. decision aids). We maintain that although there are many similarities between trust in automation and interpersonal trust, the dynamics of trust in automation also have some distinct qualities. Several models related to trust in automation have already been developed; in this report, a comprehensive -- although still preliminary -model of trust in military automation is proposed. Several sets of factors are likely to impact on the development of trust in automation, including properties of the automation, properties of the operator, and properties of the context in which interaction with automation occurs. The consequences of trust in automation have yet to be fully explored. Based on this review, measures and methods to study trust in automation are considered, and a program of research to study trust in automated systems is described.

Duro, E. S. D., Veltri, G. A., Golino, H., & Stella, M. (2025). Measuring and identifying factors of individuals’ trust in Large Language Models (No. arXiv:2502.21028). arXiv. https://doi.org/10.48550/arXiv.2502.21028
Large Language Models (LLMs) can engage in human-looking conversational exchanges. Although conversations can elicit trust between users and LLMs, scarce empirical research has examined trust formation in human-LLM contexts, beyond LLMs’ trustworthiness or human trust in AI in general. Here, we introduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure individuals’ trust in LLMs, extending McAllister’s cognitive and affective trust dimensions to LLM-human interactions. We developed TILLMI as a psychometric scale, prototyped with a novel protocol we called LLM-simulated validity. The LLM-based scale was then validated in a sample of 1,000 US respondents. Exploratory Factor Analysis identified a two-factor structure. Two items were then removed due to redundancy, yielding a final 6-item scale with a 2-factor structure. Confirmatory Factor Analysis on a separate subsample showed strong model fit ($CFI = .995$, $TLI = .991$, $RMSEA = .046$, $p_{X^2} > .05$). Convergent validity analysis revealed that trust in LLMs correlated positively with openness to experience, extraversion, and cognitive flexibility, but negatively with neuroticism. Based on these findings, we interpreted TILLMI’s factors as “closeness with LLMs” (affective dimension) and “reliance on LLMs” (cognitive dimension). Younger males exhibited higher closeness with- and reliance on LLMs compared to older women. Individuals with no direct experience with LLMs exhibited lower levels of trust compared to LLMs’ users. These findings offer a novel empirical foundation for measuring trust in AI-driven verbal communication, informing responsible design, and fostering balanced human-AI collaboration.

Jiun-Yin Jian, Bisantz, A. M., & Drury, C. G. (2000). Foundations for an Empirically Determined Scale of Trust in Automated System. International Journal of Cognitive Ergonomics, 4(1), 53. https://doi.org/10.1207/S15327566IJCE0401_04
Explores the underlying factors comprising the concepts of trust between people and man-machine systems.  Distinction between types of trust; Similarities and differences in the concepts of trust and distrust; Use of the factors in developing a scale to measure trust in automation.

Kim, S. S. Y., Liao, Q. V., Vorvoreanu, M., Ballard, S., & Vaughan, J. W. (2024). “I’m Not Sure, But...”: Examining the Impact of Large Language Models’ Uncertainty Expression on User Reliance and Trust (No. arXiv:2405.00623). arXiv. https://doi.org/10.48550/arXiv.2405.00623
Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs’ expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants’ reliance, trust, and overall task performance. We find that first-person expressions (e.g., “I’m not sure, but...”) decrease participants’ confidence in the system and tendency to agree with the system’s answers, while increasing participants’ accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g., “It’s not clear, but...”), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.

Kohn, S. C., De Visser, E. J., Wiese, E., Lee, Y.-C., & Shaw, T. H. (2021). Measurement of Trust in Automation: A Narrative Review and Reference Guide. Frontiers in Psychology, 12, 604977. https://doi.org/10.3389/fpsyg.2021.604977
With the rise of automated and autonomous agents, research examining Trust in Automation (TiA) has attracted considerable attention over the last few decades. Trust is a rich and complex construct which has sparked a multitude of measures and approaches to study and understand it. This comprehensive narrative review addresses known methods that have been used to capture TiA. We examined measurements deployed in existing empirical works, categorized those measures into self-report, behavioral, and physiological indices, and examined them within the context of an existing model of trust. The resulting work provides a reference guide for researchers, providing a list of available TiA measurement methods along with the model-derived constructs that they capture including judgments of trustworthiness, trust attitudes, and trusting behaviors. The article concludes with recommendations on how to improve the current state of TiA measurement.

Körber, M. (2019). Theoretical Considerations and Development of a Questionnaire to Measure Trust in Automation. In S. Bagnara, R. Tartaglia, S. Albolino, T. Alexander, & Y. Fujita (Eds.), Proceedings of the 20th Congress of the International Ergonomics Association (IEA 2018) (Vol. 823, pp. 13–30). Springer International Publishing. http://link.springer.com/10.1007/978-3-319-96074-6_2
The increasing number of interactions with automated systems has sparked the interest of researchers in trust in automation because it predicts not only whether but also how an operator interacts with an automation. In this work, a theoretical model of trust in automation is established and the development and evaluation of a corresponding questionnaire (Trust in Automation, TiA) are described.

Körber, M., Baseler, E., & Bengler, K. (2018). Introduction matters: Manipulating trust in automation and reliance in automated driving. Applied Ergonomics, 66, 18–31. https://doi.org/10.1016/j.apergo.2017.07.006
Trust in automation is a key determinant for the adoption of automated systems and their appropriate use. Therefore, it constitutes an essential research area for the introduction of automated vehicles to road traffic. In this study, we investigated the influence of trust promoting (Trust promoted group) and trust lowering (Trust lowered group) introductory information on reported trust, reliance behavior and take-over performance. Forty participants encountered three situations in a 17-min highway drive in a conditionally automated vehicle (SAE Level 3). Situation 1 and Situation 3 were non-critical situations where a take-over was optional. Situation 2 represented a critical situation where a take-over was necessary to avoid a collision. A non-driving-related task (NDRT) was presented between the situations to record the allocation of visual attention. Participants reporting a higher trust level spent less time looking at the road or instrument cluster and more time looking at the NDRT. The manipulation of introductory information resulted in medium differences in reported trust and influenced participants’ reliance behavior. Participants of the Trust promoted group looked less at the road or instrument cluster and more at the NDRT. The odds of participants of the Trust promoted group to overrule the automated driving system in the non-critical situations were 3.65 times (Situation 1) to 5 times (Situation 3) higher. In Situation 2, the Trust promoted group’s mean take-over time was extended by 1154 ms and the mean minimum time-to-collision was 933 ms shorter. Six participants from the Trust promoted group compared to no participant of the Trust lowered group collided with the obstacle. The results demonstrate that the individual trust level influences how much drivers monitor the environment while performing an NDRT. Introductory information influences this trust level, reliance on an automated driving system, and if a critical take-over situation can be successfully solved.

Pataranutaporn, P., Liu, R., Finn, E., & Maes, P. (2023). Influencing human–AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy and effectiveness. Nature Machine Intelligence, 5(10), 1076–1086. https://doi.org/10.1038/s42256-023-00720-7
As conversational agents powered by large language models become more human-like, users are starting to view them as companions rather than mere assistants. Our study explores how changes to a person’s mental model of an AI system affects their interaction with the system. Participants interacted with the same conversational AI, but were influenced by different priming statements regarding the AI’s inner motives: caring, manipulative or no motives. Here we show that those who perceived a caring motive for the AI also perceived it as more trustworthy, empathetic and better-performing, and that the effects of priming and initial mental models were stronger for a more sophisticated AI model. Our work also indicates a feedback loop in which the user and AI reinforce the user’s mental model over a short time; further work should investigate long-term effects. The research highlights the importance of how AI systems are introduced can notably affect the interaction and how the AI is experienced.

Scantamburlo, T., Cortés, A., Foffano, F., Barrué, C., Distefano, V., Pham, L., & Fabris, A. (2025). Artificial Intelligence Across Europe: A Study on Awareness, Attitude and Trust. IEEE Transactions on Artificial Intelligence, 6(2), 477–490. https://doi.org/10.1109/TAI.2024.3461633
This article presents the results of an extensive study investigating the opinions on artificial intelligence (AI) of a sample of 4006 European citizens from eight distinct countries (France, Germany, Italy, Netherlands, Poland, Romania, Spain, and Sweden). The aim of the study is to gain a better understanding of people’s views and perceptions within the European context, which is already marked by important policy actions and regulatory processes. To survey the perceptions of the citizens of Europe, we design and validate a new questionnaire (PAICE) structured around three dimensions: people’s awareness, attitude, and trust. We observe that while awareness is characterized by a low level of self-assessed competency, the attitude toward AI is very positive for more than half of the population. Reflecting on the collected results, we highlight implicit contradictions and identify trends that may interfere with the creation of an ecosystem of trust and the development of inclusive AI policies. The introduction of rules that ensure legal and ethical standards, along with the activity of high-level educational entities, and the promotion of AI literacy are identified as key factors in supporting a trustworthy AI ecosystem. We make some recommendations for AI governance focused on the European context and conclude with suggestions for future work.

Scharowski, N., Perrig, S. A. C., Aeschbach, L. F., Felten, N. von, Opwis, K., Wintersberger, P., & Brühlmann, F. (2025). To Trust or Distrust Trust Measures: Validating Questionnaires for Trust in AI (No. arXiv:2403.00582). arXiv. https://doi.org/10.48550/arXiv.2403.00582
Despite the importance of trust in human-AI interactions, researchers must adopt questionnaires from other disciplines that lack validation in the AI context. Motivated by the need for reliable and valid measures, we investigated the psychometric quality of two trust questionnaires, the Trust between People and Automation scale (TPA) by Jian et al. (2000) and the Trust Scale for the AI Context (TAI) by Hoffman et al. (2023). In a pre-registered online experiment (N = 1485), participants observed interactions with trustworthy and untrustworthy AI (autonomous vehicle and chatbot). Results support the psychometric quality of the TAI while revealing opportunities to improve the TPA, which we outline in our recommendations for using the two questionnaires. Furthermore, our findings provide additional empirical evidence of trust and distrust as two distinct constructs that may coexist independently. Building on our findings, we highlight the opportunities and added value of measuring both trust and distrust in human-AI research and advocate for further work on both constructs.

Scholz, D. D., Kraus ,Johannes, & and Miller, L. (2025). Measuring the Propensity to Trust in Automated Technology: Examining Similarities to Dispositional Trust in Other Humans and Validation of the PTT-A Scale. International Journal of Human–Computer Interaction, 41(2), 970–993. https://doi.org/10.1080/10447318.2024.2307691
In this work, an integrative theoretical structure for the propensity to trust (PTT) is derived from literature. In an online study (N = 669), the validity of the structure was assessed and compared in two domains: propensity to trust in humans (PTT-H) and propensity to trust in automated technology (PTT-A). Based on this, an economic scale to measure PTT-A was derived and its psychometric quality was explored based on the first and an additional second study. The observed correlational pattern to basic personality traits supports the convergent validity of PTT-A. Moreover, discriminative predictive validity of PTT-A over PTT-H was supported by its higher relationships to technology-related outcomes. Additionally, incremental validity of PTT-A over basic personality traits was supported. Finally, the internal validity of the scale was replicated in an independent sample and re-test reliability was established. The findings support the added value of integrating PTT-A in research on the interaction with automated technology.

Wojton, H. M., Porter, D., T Lane, S., Bieber, C., & Madhavan, P. (2020). Initial validation of the trust of automated systems test (TOAST). The Journal of Social Psychology, 160(6), 735–750. https://doi.org/10.1080/00224545.2020.1749020
Trust is a key determinant of whether people rely on automated systems in the military and the public. However, there is currently no standard for measuring trust in automated systems. In the present studies, we propose a scale to measure trust in automated systems that is grounded in current research and theory on trust formation, which we refer to as the Trust in Automated Systems Test (TOAST). We evaluated both the reliability of the scale structure and criterion validity using independent, military-affiliated and civilian samples. In both studies we found that the TOAST exhibited a two-factor structure, measuring system understanding and performance (respectively), and that factor scores significantly predicted scores on theoretically related constructs demonstrating clear criterion validity. We discuss the implications of our findings for advancing the empirical literature and in improving interface design.



Bach, T. A., Khan, A., Hallock, H., Beltrão, G., & Sousa, S. (2024). A Systematic Literature Review of User Trust in AI-Enabled Systems: An HCI Perspective. International Journal of Human–Computer Interaction, 40(5), 1251–1266. https://doi.org/10.1080/10447318.2022.2138826
User trust in Artificial Intelligence (AI) enabled systems has been increasingly recognized and proven as a key element to fostering adoption. It has been suggested that AI-enabled systems must go beyond technical-centric approaches and towards embracing a more human-centric approach, a core principle of the human-computer interaction (HCI) field. This review aims to provide an overview of the user trust definitions, influencing factors, and measurement methods from 23 empirical studies to gather insight for future technical and design strategies, research, and initiatives to calibrate the user-AI relationship. The findings confirm that there is more than one way to define trust. Selecting the most appropriate trust definition to depict user trust in a specific context should be the focus instead of comparing definitions. User trust in AI-enabled systems is found to be influenced by three main themes, namely socio-ethical considerations, technical and design features, and user characteristics. User characteristics dominate the findings, reinforcing the importance of user involvement from development through to monitoring of AI-enabled systems. Different contexts and various characteristics of both the users and the systems are also found to influence user trust, highlighting the importance of selecting and tailoring features of the system according to the targeted user group’s characteristics. Importantly, socio-ethical considerations can pave the way in making sure that the environment where user-AI interactions happen is sufficiently conducive to establish and maintain a trusted relationship. In measuring user trust, surveys are found to be the most common method followed by interviews and focus groups. In conclusion, user trust needs to be addressed directly in every context where AI-enabled systems are being used or discussed. In addition, calibrating the user-AI relationship requires finding the optimal balance that works for not only the user but also the system.

Hancock, P. A., Kessler, T. T., Kaplan, A. D., Stowers, K., Brill, J. C., Billings, D. R., Schaefer, K. E., & Szalma, J. L. (2023). How and why humans trust: A meta-analysis and elaborated model. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1081086
Trust exerts an impact on essentially all forms of social relationships. It affects individuals in deciding whether and how they will or will not interact with other people. Equally, trust also influences the stance of entire nations in their mutual dealings. In consequence, understanding the factors that influence the decision to trust, or not to trust, is crucial to the full spectrum of social dealings. Here, we report the most comprehensive extant meta-analysis of experimental findings relaying to such human-to-human trust. Our analysis provides a quantitative evaluation of the factors that influence interpersonal trust, the initial propensity to trust, as well as an assessment of the general trusting of others. Over two thousand relevant studies were initially identified for potential inclusion in the meta-analysis. Of these, (n = 338) passed all screening criteria and provided therefrom a total of (n = 2,185) effect sizes for analysis. The identified dependent variables were trustworthiness, propensity to trust, general trust, and the trust that supervisors and subordinates express in each other. Correlational results demonstrated that a large range of trustor, trustee, and shared, contextual factors impact each of trustworthiness, the propensity to trust, and trust within working relationships. The emphasis in the present work on contextual factors being one of several trust dimensions herein originated. Experimental results established that the reputation of the trustee and the shared closeness of trustor and trustee were the most predictive factors of trustworthiness outcome. From these collective findings, we propose an elaborated, overarching descriptive theory of trust in which especial note is taken of the theory’s application to the growing human need to trust in non-human entities. The latter include diverse forms of automation, robots, artificially intelligent entities, as well as specific implementations such as driverless vehicles to name but a few. Future directions as to the momentary dynamics of trust development, its sustenance and its dissipation are also evaluated.

Marikyan, & Papagiannidis. (2023). Technology acceptance model. In TheoryHub Book. https://open.ncl.ac.uk/theories/1/technology-acceptance-model/
TheoryHub reviews a wide range of theories, acting as a starting point for theory exploration in different research and teaching and learning contexts.


Hoffman, R. R., Mueller, S. T., Klein, G., & Litman, J. (2019). Metrics for Explainable AI: Challenges and Prospects (No. arXiv:1812.04608). arXiv. https://doi.org/10.48550/arXiv.1812.04608
The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user’s trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.



Schepman, A., & and Rodway, P. (2023). The General Attitudes towards Artificial Intelligence Scale (GAAIS): Confirmatory Validation and Associations with Personality, Corporate Distrust, and General Trust. International Journal of Human–Computer Interaction, 39(13), 2724–2741. https://doi.org/10.1080/10447318.2022.2085400
Acceptance of Artificial Intelligence (AI) may be predicted by individual psychological correlates, examined here. Study 1 reports confirmatory validation of the General Attitudes towards Artificial Intelligence Scale (GAAIS) following initial validation elsewhere. Confirmatory Factor Analysis confirmed the two-factor structure (Positive, Negative) and showed good convergent and divergent validity with a related scale. Study 2 tested whether psychological factors (Big Five personality traits, corporate distrust, and general trust) predicted attitudes towards AI. Introverts had more positive attitudes towards AI overall, likely because of algorithm appreciation. Conscientiousness and agreeableness were associated with forgiving attitudes towards negative aspects of AI. Higher corporate distrust led to negative attitudes towards AI overall, while higher general trust led to positive views of the benefits of AI. The dissociation between general trust and corporate distrust may reflect the public’s attributions of the benefits and drawbacks of AI. Results are discussed in relation to theory and prior findings.



Smith, A., van Wagoner, H. P., Keplinger, K., & Celebi, C. (2025). Navigating AI Convergence in Human–Artificial Intelligence Teams: A Signaling Theory Approach. Journal of Organizational Behavior. https://doi.org/10.1002/job.2856
Teams that combine human intelligence with artificial intelligence (AI) have become indispensable for solving complex tasks in various decision-making contexts in modern organizations. However, the factors that contribute to AI convergence, where human team members align their decisions with those of their AI counterparts, still remain unclear. This study integrates signaling theory with self-determination theory to investigate how specific signals—such as signal fit, optional AI advice, and signal set congruence—affect employees’ AI convergence in human–AI teams. Based on four experimental studies conducted in facial recognition and hiring contexts with approximately 1100 participants, the findings highlight the significant positive impact of congruent signals from both human and AI team members on AI convergence. Moreover, providing an option for employees to solicit AI advice also enhances AI convergence; when AI signals are chosen by employees rather than forced upon them, participants are more likely to accept AI advice. This research advances knowledge on human–AI teaming by (1) expanding signaling theory into the human–AI team context; (2) developing a deeper understanding of AI convergence and its drivers in human–AI teams; (3) providing actionable insights for designing teams and tasks to optimize decision-making in high-stakes, uncertain environments; and (4) introducing facial recognition as an innovative context for human–AI teaming.

Sun, N. (2023). Delegation to Virtual Agents in Critical Scenarios: Influencing Factors and Immersive Settings [University of Luxembourg]. https://orbilu.uni.lu/handle/10993/58847
Favored by the rapid advance of technologies such as artificial intelligence and computer graphics, virtual agents have been increasingly accessible, capable, and autonomous over the past decades. As a result of their growing technological prowess, interaction with virtual agents has been gradually evolving from a traditional user-tool relationship to one resembling interpersonal delegation, where users empower virtual agents to autonomously carry out specific tasks on their behalf. Forming a delegatory relationship with virtual agents can facilitate the user-agent interaction in numerous aspects, particularly regarding convenience and efficiency. Yet, it also comes with problems and challenges that may harm users drastically in critical scenarios and thus deserves extensive research. This thesis presents a thorough discussion of delegation to virtual agents based on a series of studies my colleagues and I conducted over the past four years. Several factors --including agent representation, theory of mind, rapport, and technological immersion-- are examined individually via empirical approaches to reveal their impacts on delegation to virtual agents. A conceptual model featuring three interrelated dimensions is proposed, constituting a theoretical framework to integrate the empirical findings. An overall evaluation of these works indicates that users’ decisions on delegating critical tasks to virtual agents are mainly based on rational thinking. Performance-related factors have a significant impact on delegation, whereas affective cues --such as rapport, agent representation, and theory of mind-- are influential only to a limited extent. Furthermore, the usage of immersive media devices (e.g., head-mounted displays) has a marginal effect on users’ delegatory decisions. Thus, it is advisable for developers to focus on performance-related aspects when designing virtual agents for critical tasks.

Sun, N., & Botev, J. (2021). Why Do We Delegate to Intelligent Virtual Agents? Influencing Factors on Delegation Decisions. Proceedings of the 9th International Conference on Human-Agent Interaction, 386–390. https://doi.org/10.1145/3472307.3484680
Recent intelligent virtual agents (IVAs) are increasingly tasked with critical activities such as financial investment and vehicle control. However, the mechanics behind delegatory behaviors toward IVAs are not fully explored, especially when multiple factors come into play. This paper aims to investigate how different agent-related factors, such as capability and trustworthiness, influence users’ decisions on critical-transaction delegation to IVAs. We conducted an experiment constituting a variation of the investment game where participants interact with a robot-like IVA in a virtual environment. We found that, early during the interaction, the informativeness of the agent is more important than other acknowledged factors, such as agent capability. Also, most of these factors have a stable impact on delegation decisions early during the interaction, whereas only a few factors, such as agent usability, have a dynamic impact. Our findings provide guidelines for the design of trustworthy IVA for virtual environments and various levels of immersion.



Sun, N., Botev, J., Khaluf, Y., & Simoens, P. (2022). Theory of Mind and Delegation to Robotic Virtual Agents. 2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN), 454–460. https://doi.org/10.1109/RO-MAN53752.2022.9900789
Despite already being commonplace, delegation to robotic virtual agents (VAs) is often considered challenging and error-prone in critical situations by the general public. Theory of mind, the human capacity to take another person’s perspective, is deemed an important enabler for human-human cooperation. This study explores the effect of a robotic VA’s ability to use theory of mind on users’ delegation behavior. To this end, we conducted a between-subjects experiment with participants playing the Colored Trails game with robotic VAs of varying levels of theory of mind. The results invalidate our hypothesis that the ToM level is a reliable indicator of delegation choices. Instead, we found that the participants’ performance strongly correlates with their delegatory intentions. Therefore, to facilitate delegation, designers of robots and robotic agents may consider refraining from using ToM-resemblance features and focusing on balancing user performance perception instead to induce the desired delegation behaviors.

Sun, N., Botev, J., & Simoens, P. (2023). The Effect of Rapport on Delegation to Virtual Agents. Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents, 1–3. https://doi.org/10.1145/3570945.3607321
This paper presents the initial results of a study exploring whether the perceived rapport with a virtual agent can influence users' decisions on delegating critical tasks to the agent. We hypothesize that users are more likely to delegate to virtual agents that attempt to build rapport with users than to agents that avoid building rapport. The samples we collected so far still need to validate the hypothesis fully. Nevertheless, we found that the perceived rapport with a virtual agent is highly relevant to trust in the agent.


# Energy

Attari, S. Z., DeKay, M. L., Davidson, C. I., & Bruine De Bruin, W. (2010). Public perceptions of energy consumption and savings. Proceedings of the National Academy of Sciences, 107(37), 16054–16059. https://doi.org/10.1073/pnas.1001509107
In a national online survey, 505 participants reported their perceptions of energy consumption and savings for a variety of household, transportation, and recycling activities. When asked for the most effective strategy they could implement to conserve energy, most participants mentioned curtailment (e.g., turning off lights, driving less) rather than efficiency improvements (e.g., installing more efficient light bulbs and appliances), in contrast to experts’ recommendations. For a sample of 15 activities, participants underestimated energy use and savings by a factor of 2.8 on average, with small overestimates for low-energy activities and large underestimates for high-energy activities. Additional estimation and ranking tasks also yielded relatively flat functions for perceived energy use and savings. Across several tasks, participants with higher numeracy scores and stronger proenvironmental attitudes had more accurate perceptions. The serious deficiencies highlighted by these results suggest that well-designed efforts to improve the public’s understanding of energy use and savings could pay large dividends.

Canfield, C., Bruine De Bruin, W., & Wong-Parodi, G. (2017). Perceptions of electricity-use communications: Effects of information, format, and individual differences. Journal of Risk Research, 20(9), 1132–1153. https://doi.org/10.1080/13669877.2015.1121909
Electricity bills could be an effective strategy for improving communications about consumers’ electricity use and promoting electricity savings. However, quantitative communications about electricity use may be difficult to understand, especially for consumers with low energy literacy. Here, we build on the health communication and graph comprehension literature to inform electricity bill design, with the goal of improving understanding, preferences for the presented communication, and intentions to save electricity. In a survey-based experiment, each participant saw a hypothetical electricity bill for a family with relatively high electricity use, covering information about (a) historical use, (b) comparisons to neighbors, and (c) historical use with appliance breakdown. Participants saw all information types in one of three formats including (a) tables, (b) bar graphs, and (c) icon graphs. We report on three main findings. First, consumers understood each type of electricity-use information the most when it was presented in a table, perhaps because tables facilitate simple point reading. Second, preferences and intentions to save electricity were the strongest for the historical use information, independent of format. Third, individuals with lower energy literacy understood all information less. We discuss implications for designing utility bills that are understandable, perceived as useful, and motivate consumers to save energy.

Cotton, D. R. E., Zhai, J., Miller, W., Dalla Valle, L., & Winter, J. (2021). Reducing energy demand in China and the United Kingdom: The importance of energy literacy. Journal of Cleaner Production, 278, 123876. https://doi.org/10.1016/j.jclepro.2020.123876
As the impacts of climate change become increasingly visible across the globe, awareness of the need for cleaner energy and demand reduction is growing. Energy literacy offers a strong potential for explaining and predicting energy-related behaviours, yet research and policies focused on this topic remain limited. In this study, energy literacy was measured in a sample of 2806 university students in the United Kingdom and China, in addition to their wider environmental attitudes using the New Ecological Paradigm scale. Findings indicate that energy literacy was relatively high overall, but there were significant differences between the knowledge, attitudes and behavioural intentions of participants in the two countries. Whilst the UK respondents rated themselves significantly more highly on perceived knowledge of energy issues, Chinese respondents provided significantly more correct answers in a knowledge test. UK respondents demonstrated more positive attitudes towards energy conservation than those from China, and were more likely to report energy-saving behaviours. However, Chinese respondents exhibited higher levels of trust in government and businesses to take action on energy issues. This paper provides a novel insight into cultural differences which may be crucial to policy and practice, and evidences the potential benefits of utilising a combination of educational and structural change to support transition to a cleaner, low-energy society.

DeWaters, J. E., & Powers, S. E. (2011). Energy literacy of secondary students in New York State (USA): A measure of knowledge, affect, and behavior. Energy Policy, 39(3), 1699–1710. https://doi.org/10.1016/j.enpol.2010.12.049
Energy literacy, which encompasses broad content knowledge as well as affective and behavioral characteristics, will empower people to make appropriate energy-related choices and embrace changes in the way we harness and consume energy. Energy literacy was measured with a written questionnaire completed by 3708 secondary students in New York State, USA. Results indicate that students are concerned about energy problems (affective subscale mean 73% of the maximum attainable score), yet relatively low cognitive (42% correct) and behavioral (65% of the maximum) scores suggest that students may lack the knowledge and skills they need to effectively contribute toward solutions. High school (HS) students scored significantly better than middle school (MS) students on the cognitive subscale; gains were greatest on topics included in NY State educational standards, and less on topics related to “practical” energy knowledge such as ways to save energy. Despite knowledge gains, there was a significant drop in energy conservation behavior between the MS and HS students. Intercorrelations between groups of questions indicate energy-related behaviors are more strongly related to affect than to knowledge. These findings underscore the need for education that improves energy literacy by impacting student attitudes, values and behaviors, as well as broad content knowledge.

DeWaters, J., & Powers, S. (2013). Establishing Measurement Criteria for an Energy Literacy Questionnaire. The Journal of Environmental Education, 44(1), 38–55. https://doi.org/10.1080/00958964.2012.711378
Energy literacy is a broad term encompassing content knowledge as well as a citizenship understanding of energy that includes affective and behavioral aspects. This article presents explicit criteria that will serve as a foundation for developing measurable objectives for energy literacy in three dimensions: cognitive (knowledge, cognitive skills), affective (attitude, values, personal responsibility); and behavioral. The outcome of this research is a framework from which a quantitative survey of energy literacy for secondary students in New York State, United States, can be created. Efforts supported by this research may help assess the broader impacts of educational programs in terms of their effectiveness for improving students’ energy literacy.

DeWaters, J., Qaqish, B., Graham, M., & Powers, S. (2013). Designing an Energy Literacy Questionnaire for Middle and High School Youth. The Journal of Environmental Education, 44(1), 56–78. https://doi.org/10.1080/00958964.2012.682615
A measurement scale has been developed to assess secondary students’ energy literacy—a citizenship understanding of energy that includes cognitive as well as affective and behavioral items. Instrument development procedures followed psychometric principles from educational and social psychology research. Initial exploration of the measure yielded promising results: internal consistencies for the cognitive, affective, and behavioral subscales, measured by Cronbach's α, ranged from 0.75 to 0.83; average discrimination indices ranged from 0.27 to 0.46. The instrument's validity was supported with contrasted-groups and developmental-age progression comparisons, as well as factor analyses. The energy literacy questionnaire provides an opportunity to measure baseline levels of energy literacy and to assess broader impacts of educational interventions.

Kantenbacher, J., & Attari, S. Z. (2021). Better rules for judging joules: Exploring how experts make decisions about household energy use. Energy Research & Social Science, 73, 101911. https://doi.org/10.1016/j.erss.2021.101911
Public understanding of home energy use is rife with biases and misunderstandings that can stymie the adoption of efficient technologies and conservation practices. Studying how energy experts make energy-related judgments can help design decision support tools to correct misperceptions held by novices. Here we conduct interviews with electrical engineers (n = 10), physicists (n = 10), and energy analysts (n = 10) to document expert judgments about energy use and to identify their cognitive shortcuts (heuristics) for household energy decision making. Performance on an energy estimation task confirmed that energy experts have more accurate estimates of home energy use than novices. We document 24 unique expert heuristics related to device functions, components, and observable cues used by experts while making energy-use judgments. A follow-up survey with the experts indicated that these expert heuristics are generally more accurate than novice heuristics. The library of heuristics created in this study can be useful additions to education programs designed to improve public energy literacy and decision making.

Marghetis, T., Attari, S. Z., & Landy, D. (2019). Simple interventions can correct misperceptions of home energy use. Nature Energy, 4(10), 874–881. https://doi.org/10.1038/s41560-019-0467-2
Public estimates of energy use suffer from severe biases. Failure to correct these may hinder efforts to conserve energy and undermine support for evidence-based policies. Here we present a randomized online experiment that showed that home energy perceptions can be improved. We tested two simple, potentially scalable interventions: providing numerical information (in watt-hours) about extremes of energy use and providing an explicit heuristic that addressed a common misperception. Both succeeded in improving numerical estimates of energy use, but in different ways. Numerical information about extremes primarily improved the use of the watt-hours response scale, while the heuristic improved underlying understanding of relative energy use. As a result, only the heuristic significantly benefitted judgements about energy-conserving behaviours. Because understanding of energy use also predicted self-reported energy-conservation behaviour, belief in climate change, and support for climate policies, targeting energy misperceptions may have the potential to shape individual behaviour and national policy support.


Mei-Shiu, C., Jan, D., & Clarkson University, Potsdam, NY, USA. (2018). Development and Validation of the Energy-Issue Attitude Questionnaire: Relations with Energy Knowledge, Affect, and Behavior. Journal of Advances in Education Research, 3(1). https://doi.org/10.22606/jaer.2018.31003
This study aims to develop the Energy-Issue Attitude Questionnaire (EIAQ). The EIAQ focuses on student responses to energy issues in society and includes ten constructs, organized pairwise with tension: energy-saving vs. carbon-reducing knowledge, having vs. being lifestyles, questioning vs. conforming to authorities, technology vs. nature approaches, and future vs. present goals. The EIAQ was validated with a criterion questionnaire on energy literacy, including knowledge, affect and behavior. Research participants were 4,689 Taiwanese secondary students. The results show that the EIAQ has desirable construct validity and reliability. Significant differences occur between the two attitudes in each pair. Energy attitudes have medium correlations with energy affect and behavior but low correlations with energy knowledge. The results of structural equation modeling show that energy behavior is directly predicted by “being” lifestyles and conformity to authorities, and indirectly predicted by energy-saving knowledge, mediated by energy affect.


Varghese, A. F., & Chandrasenan, D. (2023). Energy Literacy Scale (ELS): Validated Survey Instrument to Measure Energy Knowledge, Attitude, and Behaviour. In The 9th International Conference on Energy and Environment Research (pp. 793–800). Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-43559-1_75
The Energy Literacy Scale (ELS) was created to assess students’ energy-related knowledge and awareness of the implications of energy production and consumption, everyday energy use, and the adoption of energy-saving behaviors. The energy literacy scale was drafted and pilot tested among elementary school students across Kerala, India. Initial exploration of the measure yielded promising results: Cronbach’s reliability coefficients for cognitive, emotional, and behavioral subscales varied from 0.68 to 0.78, while average discrimination indices ranged from 0.28 to 0.43. Factor Analysis was used to select appropriate questions for the Energy Literacy Scale with due importance being given to each of the Knowledge, Attitudinal and Behavioral domains. The field-tested ELS includes three knowledge factors namely (1) Energy sources, Efficiency and Conservation, (2) Energy Use and Implications and (3) Basic Energy Concepts. Three behaviour and two attitude dimension sub-scales are included in the accepted instrument. The ELS is particularly useful for determining the baseline energy literacy skills of potential responders and evaluating the broader effects of educational initiatives.
