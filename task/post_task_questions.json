{
    "q39_tpa_dependable": { "id": "q39_tpa_dependable", "section": "post-task", "type": "likert", "points": 7, "text": "The AI system was dependable during the task.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q40_tpa_reliable": { "id": "q40_tpa_reliable", "section": "post-task", "type": "likert", "points": 7, "text": "The AI system's responses were reliable during the task.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q41_tpa_trust_info": { "id": "q41_tpa_trust_info", "section": "post-task", "type": "likert", "points": 7, "text": "I could trust the information provided by the AI system during the task.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q42_tpa_confident_abilities": { "id": "q42_tpa_confident_abilities", "section": "post-task", "type": "likert", "points": 7, "text": "I was confident in the AI system's abilities for the task.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q43_tpa_consistent": { "id": "q43_tpa_consistent", "section": "post-task", "type": "likert", "points": 7, "text": "The AI system behaved consistently during the task.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q44_tpa_suspicious": { "id": "q44_tpa_suspicious", "section": "post-task", "type": "likert", "points": 7, "text": "I was suspicious of the AI system's outputs during the task.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q45_tpa_wary": { "id": "q45_tpa_wary", "section": "post-task", "type": "likert", "points": 7, "text": "I was wary of the AI system during the task.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q46_tpa_questionable": { "id": "q46_tpa_questionable", "section": "post-task", "type": "likert", "points": 7, "text": "The AI system's actions or responses felt questionable during the task.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q47_tpa_uncertain_caps": { "id": "q47_tpa_uncertain_caps", "section": "post-task", "type": "likert", "points": 7, "text": "I was uncertain about the AI system's capabilities during the task.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q48_tpa_deceptive": { "id": "q48_tpa_deceptive", "section": "post-task", "type": "likert", "points": 7, "text": "The AI system seemed potentially deceptive or misleading during the task.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q49_llm_realtime_db": { "id": "q49_llm_realtime_db", "section": "post-task", "type": "likert", "points": 7, "text": "LLMs access and retrieve information from a constantly updated, real-time database like a search engine.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q50_llm_patterns": { "id": "q50_llm_patterns", "section": "post-task", "type": "likert", "points": 7, "text": "LLMs primarily generate responses by identifying patterns in the massive amounts of text data they were trained on.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q51_llm_intentions": { "id": "q51_llm_intentions", "section": "post-task", "type": "likert", "points": 7, "text": "LLMs have internal goals or intentions similar to humans.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q52_llm_reasoning_accuracy": { "id": "q52_llm_reasoning_accuracy", "section": "post-task", "type": "likert", "points": 7, "text": "When an LLM provides an explanation, it accurately reflects the internal \"reasoning\" process it used to arrive at the answer.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q53_llm_avoids_errors": { "id": "q53_llm_avoids_errors", "section": "post-task", "type": "likert", "points": 7, "text": "LLMs are designed to detect and avoid providing factually incorrect information (hallucinations).", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q54_llm_understands_meaning": { "id": "q54_llm_understands_meaning", "section": "post-task", "type": "likert", "points": 7, "text": "LLMs understand the meaning of the text they generate in the same way a human does.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q55_llm_confidence_accuracy": { "id": "q55_llm_confidence_accuracy", "section": "post-task", "type": "likert", "points": 7, "text": "An LLM's confidence in its answer (if expressed) generally reflects its actual likelihood of being correct.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q56_llm_math_reliable": { "id": "q56_llm_math_reliable", "section": "post-task", "type": "likert", "points": 7, "text": "LLMs can perform complex mathematical calculations reliably without errors.", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q57_over_trust_concern": { "id": "q57_over_trust_concern", "section": "post-task", "type": "likert", "points": 5, "text": "“I might be trusting this AI more than I should for important decisions.”", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q58_under_trust_concern": { "id": "q58_under_trust_concern", "section": "post-task", "type": "likert", "points": 5, "text": "“I was too skeptical of the AI, even when its answers were likely correct.”", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q59_perceived_accuracy": { "id": "q59_perceived_accuracy", "section": "post-task", "type": "slider", "text": "Approximately what <strong>percentage of the AI’s answers</strong> do you believe were <strong>correct</strong>?", "min": 0, "max": 100, "step": 5, "unit": "%" },
    "q60_future_use": { "id": "q60_future_use", "section": "post-task", "type": "likert", "points": 5, "text": "“If this AI advisor were available to me, I would use it regularly for managing my home’s energy use.”", "scale": ["Very Unlikely", "Very Likely"] },
    "q61_reliance_intention": { "id": "q61_reliance_intention", "section": "post-task", "type": "likert", "points": 5, "text": "“For decisions like buying a new appliance or making home improvements, I would base my decision primarily on the AI’s advice.”", "scale": ["Strongly Disagree", "Strongly Agree"] },
    "q62_mental_model_source": { "id": "q62_mental_model_source", "section": "post-task", "type": "multiple-choice", "text": "Where do you think this AI’s answers are coming from (what is the AI relying on)?", "options": [ { "value": "db", "text": "A database of factual energy information (like an encyclopedia)." }, { "value": "llm", "text": "General knowledge learned from lots of text it was trained on (it “strings together” information it learned)." }, { "value": "realtime", "text": "Real-time data from the internet and up-to-date sources." }, { "value": "human", "text": "Input from human energy experts behind the scenes." }, { "value": "unsure", "text": "Not sure." } ] },
    "q63_mental_model_limitations": { "id": "q63_mental_model_limitations", "section": "post-task", "type": "open-ended", "text": "In your own words, <strong>what do you think are the limitations of this AI advisor?</strong> (For example, are there things it likely doesn’t know or situations it might not handle well?)" },
    "q64_additional_comments": { "id": "q64_additional_comments", "section": "post-task", "type": "open-ended", "text": "(Optional) Do you have any <strong>other thoughts or feedback</strong> about using this AI energy advisor?" }
}
