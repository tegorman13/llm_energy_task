<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Security-Policy" content="default-src 'self' 'unsafe-inline' 'unsafe-eval' data: blob: https://cdnjs.cloudflare.com https://cdn.jsdelivr.net https://code.jquery.com https://unpkg.com https://d3js.org https://threejs.org https://cdn.plot.ly https://stackpath.bootstrapcdn.com https://maps.googleapis.com https://cdn.tailwindcss.com https://ajax.googleapis.com https://kit.fontawesome.com https://cdn.datatables.net https://maxcdn.bootstrapcdn.com https://code.highcharts.com https://tako-static-assets-production.s3.amazonaws.com https://www.youtube.com https://fonts.googleapis.com https://fonts.gstatic.com https://pfst.cf2.poecdn.net https://puc.poecdn.net https://i.imgur.com https://wikimedia.org https://*.icons8.com https://*.giphy.com https://picsum.photos https://images.unsplash.com; frame-src 'self' https://www.youtube.com https://trytako.com; child-src 'self'; manifest-src 'self'; worker-src 'self'; upgrade-insecure-requests; block-all-mixed-content;">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cognitive Science Simulation: Calibrating Trust in an LLM Energy Advisor</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <style>
        :root {
            --primary-color: #2c3e50; /* Dark Blue-Gray */
            --secondary-color: #3498db; /* Bright Blue */
            --accent-color: #e74c3c; /* Red */
            --light-gray: #f8f9fa;
            --medium-gray: #dfe6e9;
            --dark-gray: #7f8c8d;
            --text-color: #333;
            --card-bg: white;
            --font-family: 'Inter', 'Segoe UI', sans-serif;
        }
        body {
            font-family: var(--font-family);
            margin: 0;
            padding: 0;
            background-color: var(--light-gray);
            color: var(--text-color);
            line-height: 1.6;
        }
        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
        }
        .header {
            background: linear-gradient(135deg, var(--primary-color) 0%, #34495e 100%);
            color: white;
            padding: 30px 20px;
            text-align: center;
            border-radius: 12px 12px 0 0;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }
        .header h1 {
            margin-top: 0;
            font-size: 2.2rem;
            font-weight: 700;
        }
        .header p {
            margin-bottom: 0;
            opacity: 0.9;
            font-size: 1.15rem;
        }
        .tabs {
            display: flex;
            flex-wrap: wrap; /* Allow tabs to wrap */
            background-color: #34495e;
            border-radius: 0 0 12px 12px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
            position: sticky;
            top: 0;
            z-index: 1000;
        }
        .tab {
            background-color: transparent;
            color: white;
            border: none;
            outline: none;
            cursor: pointer;
            padding: 15px 20px; /* Adjusted padding for wrapping */
            flex-grow: 1; /* Distribute space evenly */
            min-width: 150px; /* Minimum width for tabs */
            text-align: center;
            transition: background-color 0.3s, color 0.3s;
            font-size: 0.95rem; /* Slightly adjusted font size */
            font-weight: 600;
            border-bottom: 4px solid transparent;
            white-space: nowrap;
        }
        .tab:hover {
            background-color: rgba(255,255,255,0.1);
        }
        .tab.active {
            background-color: var(--primary-color);
            color: white;
            border-bottom: 4px solid var(--accent-color);
        }
        .tab-content {
            display: none;
            padding: 30px;
            background-color: var(--card-bg);
            border-radius: 12px;
            box-shadow: 0 5px 20px rgba(0,0,0,0.07);
            margin: 25px 0;
            animation: fadeIn 0.5s ease-out;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(15px); }
            to { opacity: 1; transform: translateY(0); }
        }
        .tab-content.active {
            display: block;
        }
        .tab-content h2 {
            color: var(--primary-color);
            border-bottom: 2px solid var(--secondary-color);
            padding-bottom: 10px;
            margin-top: 0;
            margin-bottom: 25px;
            font-size: 1.8rem;
        }
        .tab-content h3 {
            color: var(--primary-color);
            margin-top: 25px;
            margin-bottom: 15px;
            font-size: 1.5rem;
        }
        .tab-content h4 {
            color: var(--secondary-color);
            margin-top: 20px;
            margin-bottom: 10px;
            font-size: 1.25rem;
        }
        .slider-container {
            margin: 20px 0;
            padding: 20px;
            background-color: var(--light-gray);
            border-radius: 8px;
            border-left: 5px solid var(--secondary-color);
        }
        .slider-label {
            display: flex;
            justify-content: space-between;
            margin-bottom: 10px;
            font-weight: 600;
            color: var(--primary-color);
        }
        .slider-value {
            padding: 4px 10px;
            background-color: var(--secondary-color);
            color: white;
            border-radius: 6px;
            font-weight: 600;
        }
        .slider {
            width: 100%;
            margin: 10px 0;
            height: 10px;
            -webkit-appearance: none;
            appearance: none;
            background: var(--medium-gray);
            border-radius: 5px;
            outline: none;
            transition: background-color 0.2s;
        }
        .slider:hover {
            background: #bdc3c7;
        }
        .slider::-webkit-slider-thumb {
            -webkit-appearance: none;
            appearance: none;
            width: 22px;
            height: 22px;
            border-radius: 50%;
            background: var(--secondary-color);
            cursor: pointer;
            border: 3px solid white;
            box-shadow: 0 2px 6px rgba(0,0,0,0.2);
            transition: background-color 0.2s, transform 0.2s;
        }
        .slider::-webkit-slider-thumb:hover {
            background: #2980b9;
            transform: scale(1.1);
        }
        .slider::-moz-range-thumb {
            width: 22px;
            height: 22px;
            border-radius: 50%;
            background: var(--secondary-color);
            cursor: pointer;
            border: 3px solid white;
            box-shadow: 0 2px 6px rgba(0,0,0,0.2);
            transition: background-color 0.2s, transform 0.2s;
        }
        .slider::-moz-range-thumb:hover {
            background: #2980b9;
            transform: scale(1.1);
        }
        .slider-description {
            font-size: 0.9rem;
            color: var(--dark-gray);
            margin-top: 8px;
            font-style: italic;
        }
        .visualization {
            width: 100%;
            min-height: 400px;
            background-color: #fdfdfd;
            border: 1px solid var(--medium-gray);
            border-radius: 8px;
            margin: 25px 0;
            position: relative;
            overflow: hidden;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
            transition: box-shadow 0.3s;
        }
        .visualization:hover {
            box-shadow: 0 6px 20px rgba(0,0,0,0.1);
        }
        .explanation-box {
            background-color: #eaf5ff;
            border-left: 5px solid var(--secondary-color);
            padding: 25px;
            margin: 25px 0;
            border-radius: 0 8px 8px 0;
            position: relative;
        }
        .explanation-box h3, .explanation-box h4 {
            color: var(--primary-color);
        }
        .card {
            background-color: var(--card-bg);
            border-radius: 12px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.07);
            overflow: hidden;
            transition: all 0.3s;
            height: 100%;
        }
        .card:hover {
            box-shadow: 0 7px 25px rgba(0,0,0,0.1);
            transform: translateY(-4px);
        }
        .card-header {
            padding: 18px 25px;
            background-color: var(--secondary-color);
            color: white;
            font-weight: 600;
            font-size: 1.2rem;
            border-bottom: 1px solid rgba(0,0,0,0.05);
        }
        .card-content {
            padding: 25px;
        }
        .card-content p, .card-content li {
            font-size: 1rem;
        }
        .card-content ul {
            padding-left: 20px;
            list-style: disc;
        }
        .card-content li {
            margin-bottom: 8px;
        }
        .flex-container {
            display: flex;
            flex-wrap: wrap;
            gap: 25px;
            margin-bottom: 30px;
        }
        .flex-item {
            flex: 1;
            min-width: 320px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
            border-radius: 8px;
            overflow: hidden;
        }
        thead {
            background-color: var(--primary-color);
            color: white;
        }
        th, td {
            text-align: left;
            padding: 14px 18px;
            border-bottom: 1px solid var(--medium-gray);
        }
        tbody tr:nth-child(even) {
            background-color: var(--light-gray);
        }
        tbody tr:hover {
            background-color: #e9ecef;
        }
        .tooltip {
            position: absolute;
            background-color: rgba(44, 62, 80, 0.95);
            color: white;
            padding: 12px 18px;
            border-radius: 8px;
            font-size: 0.9rem;
            pointer-events: none;
            z-index: 1001;
            display: none;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
            max-width: 320px;
            backdrop-filter: blur(3px);
        }
        .chart-legend {
            display: flex;
            justify-content: center;
            flex-wrap: wrap;
            gap: 20px;
            margin: 20px 0 10px;
            padding: 12px;
            background-color: var(--light-gray);
            border-radius: 6px;
        }
        .legend-item {
            display: flex;
            align-items: center;
            gap: 10px;
            font-size: 0.9rem;
        }
        .legend-color {
            width: 16px;
            height: 16px;
            border-radius: 4px;
            border: 1px solid rgba(0,0,0,0.1);
        }
        .paper-summary {
            background-color: #f0f9ff;
            padding: 25px;
            border-radius: 8px;
            margin-bottom: 30px;
            border-left: 5px solid var(--secondary-color);
            position: relative;
            transition: box-shadow 0.3s;
        }
        .paper-summary:hover {
            box-shadow: 0 6px 20px rgba(0,0,0,0.07);
        }
        .paper-summary h3 {
            margin-top: 0;
            color: var(--primary-color);
            border-bottom: 1px solid #bde0fe;
            padding-bottom: 10px;
        }
        .paper-summary ul {
            padding-left: 20px;
            list-style-type: disc;
        }
        .paper-summary li {
            margin-bottom: 10px;
        }
        .paper-badge {
            position: absolute;
            top: 15px;
            right: 15px;
            background-color: var(--secondary-color);
            color: white;
            border-radius: 15px;
            padding: 5px 12px;
            font-size: 0.8rem;
            font-weight: bold;
        }
        .highlight-box {
            background-color: #fff3cd;
            border: 1px solid #ffeeba;
            border-left: 5px solid #ffc107;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            color: #856404;
        }
        .highlight-box h4 {
            margin-top: 0;
            color: #856404;
        }
        .key-concept {
            background-color: #e2f0ff;
            border-left: 4px solid var(--secondary-color);
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 6px;
        }
        .key-concept h4 {
            margin-top: 0;
            color: var(--secondary-color);
            font-size: 1.15rem;
        }
        .equation {
            background-color: var(--light-gray);
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            font-family: 'Georgia', serif;
            margin: 25px 0;
            overflow-x: auto;
            border: 1px solid var(--medium-gray);
        }
        /* Enhanced Diagram Styles */
        .styled-diagram-container {
            background-color: #f9fafb; /* Lighter than card-bg for contrast */
            border: 1px solid var(--medium-gray);
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            box-shadow: inset 0 2px 4px rgba(0,0,0,0.03);
        }
        .diagram-title {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--primary-color);
            margin-bottom: 15px;
            text-align: center;
        }
        .flowchart-node {
            background-color: var(--secondary-color);
            color: white;
            padding: 10px 15px;
            border-radius: 6px;
            text-align: center;
            margin: 10px auto; /* Centering nodes */
            font-size: 0.9rem;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            max-width: 90%;
        }
        .flowchart-node.sub-node {
            background-color: #cce7ff; /* Lighter blue for sub-points */
            color: var(--primary-color);
            font-size: 0.85rem;
            padding: 8px 12px;
            margin-left: 20px; /* Indent sub-nodes */
            text-align: left;
        }
        .flowchart-arrow {
            text-align: center;
            color: var(--dark-gray);
            font-size: 1.5rem; /* Larger arrow */
            margin: 5px 0;
            line-height: 1;
        }
         .methods-diagram-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 10px;
            padding: 20px;
            background-color: #f9fafb;
            border-radius: 8px;
            border: 1px solid var(--medium-gray);
        }
        .methods-step {
            background-color: white;
            border: 1px solid var(--medium-gray);
            border-left: 5px solid var(--secondary-color);
            padding: 15px;
            border-radius: 6px;
            width: 80%;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            text-align: left;
        }
        .methods-step strong {
            color: var(--primary-color);
            display: block;
            margin-bottom: 5px;
        }
        .methods-arrow {
            font-size: 1.5rem;
            color: var(--dark-gray);
            transform: rotate(90deg); /* Pointing down */
        }


        .button-primary {
            background-color: var(--secondary-color);
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            font-weight: 600;
            transition: background-color 0.3s;
            font-size: 1rem;
        }
        .button-primary:hover {
            background-color: #2980b9;
        }
        .button-secondary {
            background-color: var(--accent-color);
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 6px;
            cursor: pointer;
            font-weight: 600;
            transition: background-color 0.3s;
            font-size: 1rem;
        }
        .button-secondary:hover {
            background-color: #c0392b;
        }
        .simulation-controls label {
            display: block;
            margin-bottom: 8px;
            font-weight: 600;
            color: var(--primary-color);
        }
        .simulation-controls select, .simulation-controls input[type="number"] {
            width: 100%;
            padding: 10px;
            margin-bottom: 15px;
            border-radius: 6px;
            border: 1px solid var(--medium-gray);
            background-color: white;
            font-size: 1rem;
        }
        .simulation-output p {
            margin-bottom: 10px;
        }
        .simulation-output strong {
            color: var(--primary-color);
        }
        .ai-explanation {
            background-color: #f0f4f8;
            padding: 15px;
            border-radius: 6px;
            margin-top: 10px;
            border: 1px solid #d3e0ea;
        }
        .ai-explanation.confident { border-left: 4px solid #27ae60; }
        .ai-explanation.hedged-medium { border-left: 4px solid #f39c12; }
        .ai-explanation.hedged-low { border-left: 4px solid #e74c3c; }

        .annotations-toggle {
            position: absolute;
            top: 10px;
            right: 10px;
            background-color: rgba(52, 152, 219, 0.8);
            color: white;
            border: none;
            border-radius: 4px;
            padding: 5px 10px;
            font-size: 12px;
            cursor: pointer;
            z-index: 10;
        }
        .annotation {
            position: absolute;
            background-color: rgba(255, 255, 255, 0.9);
            border: 1px solid var(--secondary-color);
            border-radius: 4px;
            padding: 8px;
            font-size: 12px;
            pointer-events: none;
            z-index: 5;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            max-width: 200px;
            display: none;
        }
         .energy-savings-chart { height: 500px; }
        .bar-label { font-size: 11px; font-weight: 500; fill: #333; }
        .action-label { font-size: 12px; text-anchor: end; dominant-baseline: middle; }


        @media screen and (max-width: 768px) {
            .flex-container {
                flex-direction: column;
            }
            .flex-item {
                width: 100%;
            }
            .tab {
                flex-basis: 50%; /* Two tabs per row on smaller screens */
                font-size: 0.9rem;
                padding: 15px 10px;
            }
            .header h1 {
                font-size: 1.8rem;
            }
            .header p {
                font-size: 1rem;
            }
            .tab-content h2 { font-size: 1.6rem; }
            .tab-content h3 { font-size: 1.3rem; }
            .tab-content h4 { font-size: 1.1rem; }
            .methods-step { width: 95%; }
        }
         @media screen and (max-width: 480px) {
            .tab {
                flex-basis: 100%; /* One tab per row on very small screens */
            }
        }

    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Calibrating Trust in an LLM Energy Advisor: The Impact of Linguistic Uncertainty</h1>
            <p>A Cognitive Science Simulation Dashboard</p>
        </div>

        <div class="tabs">
            <button class="tab active" onclick="openTab(event, 'overview')">Overview & New Proposal</button>
            <button class="tab" onclick="openTab(event, 'attari')">Attari et al. (Energy Perception)</button>
            <button class="tab" onclick="openTab(event, 'steyvers')">Steyvers et al. (AI Confidence)</button>
            <button class="tab" onclick="openTab(event, 'simulation')">Proposed Study Simulation</button>
            <button class="tab" onclick="openTab(event, 'methods')">Proposed Methods</button>
            <button class="tab" onclick="openTab(event, 'implications')">RQs, Hypotheses & Implications</button>
        </div>

        <div id="overview" class="tab-content active">
            <h2>New Proposal: Calibrating Trust in an LLM Energy Advisor</h2>
            <p>This dashboard explores systematic biases in human judgment, drawing from seminal work by Attari et al. (2010) on energy perception and Steyvers et al. (2025) on AI confidence calibration. The primary focus is the new research proposal: <strong>"Calibrating Trust in an LLM Energy Advisor: The Impact of Linguistic Uncertainty."</strong> This study investigates how Large Language Models (LLMs) expressing uncertainty might help users better calibrate their trust in AI-generated energy-saving advice, especially considering pre-existing human misperceptions about energy.</p>

            <div class="paper-summary">
                <h3>Core Ideas of the New Proposal</h3>
                <p>The research aims to understand if an LLM Energy Advisor's linguistic expression of uncertainty, informed by its internal confidence, can improve user trust calibration and lead to more appropriate reliance on AI advice in the energy domain.</p>
                <ul>
                    <li><strong>Overall Research Aim:</strong> To investigate whether an LLM Energy Advisor's linguistic expression of uncertainty, informed by its internal confidence, can improve users' ability to accurately assess the AI's advice, leading to better-calibrated trust and more appropriate reliance decisions.</li>
                    <li><strong>Core Manipulation:</strong> Varying the "linguistic expression of uncertainty" in the LLM's output (e.g., confident, hedged-medium, hedged-low), derived from the LLM's internal confidence metrics (adapting Steyvers et al., 2025).</li>
                    <li><strong>Context:</strong> Addressing energy misperceptions (Attari et al., 2010) through the lens of AI trust and uncertainty communication.</li>
                    <li><strong>Objective:</strong> Determine if transparently communicating an LLM's confidence leads to more appropriate reliance and better calibrated trust, considering users' (flawed) energy knowledge.</li>
                    <li><strong>Unique Aspect:</strong> Direct comparison by participants between their own energy beliefs and the AI's advice.</li>
                </ul>
            </div>

            <h3>Conceptual Frameworks</h3>
            <div class="flex-container">
                <div class="flex-item card">
                    <div class="card-header">Simplified Reasoning Path for User-AI Interaction</div>
                    <div class="card-content styled-diagram-container">
                        <div class="flowchart-node">User forms Initial Belief about AI Capabilities (Mental Model)</div>
                        <div class="flowchart-arrow">↓</div>
                        <div class="flowchart-node">Receives LLM Output & Advice (with varying uncertainty cues)</div>
                        <div class="flowchart-arrow">↓</div>
                        <div class="flowchart-node">Metacognitive Monitoring & Evaluation (influenced by user's knowledge)</div>
                        <div class="flowchart-arrow">↓</div>
                        <div class="flowchart-node">Perceived Accuracy of LLM</div>
                        <div class="flowchart-arrow">↓</div>
                        <div class="flowchart-node">Comparison to Actual Accuracy (and User's Own Estimate)</div>
                        <div class="flowchart-arrow">↓</div>
                        <div class="flowchart-node">Calibration Gap (Potential Over/Underestimation of LLM)</div>
                        <div class="flowchart-arrow">↓</div>
                        <div class="flowchart-node">Reliance Decision (e.g., Choose own estimate vs. AI's estimate)</div>
                        <div class="flowchart-arrow">↓</div>
                        <div class="flowchart-node">Trust Update & Potential for Over/Under-reliance</div>
                    </div>
                </div>
                <div class="flex-item card">
                    <div class="card-header">Measuring and Communicating LLM Confidence</div>
                    <div class="card-content styled-diagram-container">
                        <div class="flowchart-node">M5: Measuring & Communicating LLM Confidence/Certainty</div>
                        <div class="flowchart-arrow">↓</div>
                        <div class="flowchart-node sub-node">AI's Internal "Confidence"
                            <div class="flowchart-node sub-node" style="margin-left: 10px; margin-top: 5px; background-color: #a6cfff;">LLMs Calculate Confidence Measures</div>
                            <div class="flowchart-node sub-node" style="margin-left: 10px; background-color: #a6cfff;">Research on LLM Internal Calibration</div>
                            <div class="flowchart-node sub-node" style="margin-left: 10px; background-color: #a6cfff;">LLM Reluctance to Express Uncertainty</div>
                        </div>
                        <div class="flowchart-arrow">↓</div>
                        <div class="flowchart-node sub-node">Communicating Uncertainty to Humans (Core Manipulation)
                            <div class="flowchart-node sub-node" style="margin-left: 10px; margin-top: 5px; background-color: #a6cfff;">Linguistic Expression (Hedging)</div>
                            <div class="flowchart-node sub-node" style="margin-left: 10px; background-color: #a6cfff;">Display Methods (Implicit via text)</div>
                            <div class="flowchart-node sub-node" style="margin-left: 10px; background-color: #a6cfff;">Goal: Accurate Communication for Calibration</div>
                        </div>
                        <div class="flowchart-arrow">↓</div>
                        <div class="flowchart-node sub-node">Related to Explainable AI (XAI)
                             <div class="flowchart-node sub-node" style="margin-left: 10px; margin-top: 5px; background-color: #a6cfff;">Confidence Levels as XAI Mechanism</div>
                             <div class="flowchart-node sub-node" style="margin-left: 10px; background-color: #a6cfff;">Informative vs. Persuasive Explanations</div>
                        </div>
                    </div>
                </div>
            </div>
            <p>The following tabs delve into the foundational papers and provide an interactive simulation based on the new research proposal.</p>
        </div>

        <div id="attari" class="tab-content">
            <h2>Attari et al. (2010): Public Perceptions of Energy Consumption and Savings</h2>
            <div class="paper-summary">
                <div class="paper-badge">Attari et al. 2010</div>
                <h3>Key Findings: The "Compression Pattern"</h3>
                <p>Attari, DeKay, Davidson, & Bruine De Bruin (2010) found that individuals systematically misperceive energy consumption and the effectiveness of energy-saving actions. Their study, involving estimations relative to a 100W light bulb, revealed a "compression" pattern:</p>
                <ul>
                    <li><strong>Underestimation of High-Consumption Activities:</strong> People significantly underestimate the energy used by high-consumption devices (e.g., central AC, clothes dryers) by roughly a factor of 2.8.</li>
                    <li><strong>Overestimation of Low-Consumption Activities:</strong> Conversely, they tend to overestimate the energy used by low-consumption items (e.g., small electronics, turning off lights).</li>
                    <li><strong>Focus on Curtailment:</strong> Participants often emphasized less effective "curtailment" actions (e.g., turning off lights) over more impactful "efficiency" improvements (e.g., installing energy-efficient appliances or insulation).</li>
                    <li><strong>Implication:</strong> Baseline knowledge about energy is often noisy and too "flat," hindering the prioritization of effective energy-saving behaviors.</li>
                </ul>
                <h4>Relevance to New Proposal:</h4>
                <p>These findings are crucial for the new proposal ("Calibrating Trust in an LLM Energy Advisor") because they establish that users approaching an "LLM Energy Advisor" will likely bring these pre-existing, often inaccurate, mental models about energy. The new study measures participants' baseline energy knowledge (using an Attari-like estimation task in Phase 1) before they interact with the AI, allowing for an examination of how this prior knowledge influences their interaction with and reliance on AI advice.</p>
            </div>

            <div class="flex-container">
                <div class="flex-item card">
                    <div class="card-header">Interactive Energy Perception Model</div>
                    <div class="card-content">
                        <p>Adjust the sliders to see how the compression factor, perception slope, and numeracy might influence energy estimations. This simulates the core findings of Attari et al.</p>
                        <div class="slider-container">
                            <div class="slider-label"><span>Compression Factor:</span> <span id="attari-compression-value" class="slider-value">2.8</span></div>
                            <input type="range" min="1" max="5" step="0.1" value="2.8" class="slider" id="attari-compression-slider">
                            <p class="slider-description">Controls underestimation for high-energy activities.</p>
                        </div>
                        <div class="slider-container">
                            <div class="slider-label"><span>Perception Slope:</span> <span id="attari-slope-value" class="slider-value">0.28</span></div>
                            <input type="range" min="0.1" max="1" step="0.01" value="0.28" class="slider" id="attari-slope-slider">
                            <p class="slider-description">Relationship between perceived and actual energy (1.0 = perfect).</p>
                        </div>
                        <div class="slider-container">
                            <div class="slider-label"><span>Numeracy Level:</span> <span id="attari-numeracy-value" class="slider-value">Average</span></div>
                            <input type="range" min="1" max="3" step="1" value="2" class="slider" id="attari-numeracy-slider">
                            <p class="slider-description">Affects accuracy (Higher = more accurate).</p>
                        </div>
                    </div>
                </div>
                <div class="flex-item card">
                    <div class="card-header">Energy Perception Visualization</div>
                    <div class="card-content">
                        <div class="visualization" id="energy-perception-chart">
                             <button class="annotations-toggle" onclick="toggleAnnotations('energy-chart')">Annotations</button>
                             <div class="annotation" id="attari-annotation-1" style="top: 50px; left: 50px;">Perfect accuracy line (slope = 1)</div>
                             <div class="annotation" id="attari-annotation-2" style="top: 150px; left: 150px;">Perceived line is much flatter</div>
                        </div>
                        <div class="chart-legend">
                            <div class="legend-item"><div class="legend-color" style="background-color: #3498db;"></div><span>Perfect Accuracy</span></div>
                            <div class="legend-item"><div class="legend-color" style="background-color: #e74c3c;"></div><span>Perceived Energy</span></div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="card">
                <div class="card-header">Curtailment vs. Efficiency Actions</div>
                <div class="card-content">
                    <p>Attari et al. highlighted a preference for discussing curtailment actions over more impactful efficiency actions. The chart below visualizes this discrepancy for selected actions.</p>
                    <div class="visualization energy-savings-chart" id="curtailment-efficiency-chart">
                        <button class="annotations-toggle" onclick="toggleAnnotations('curtailment-chart')">Annotations</button>
                        <div class="annotation" id="attari-annotation-3" style="top: 50px; left: 50px;">Efficiency actions often save more.</div>
                        <div class="annotation" id="attari-annotation-4" style="top: 150px; left: 150px;">People underestimate high-impact actions.</div>
                    </div>
                     <div class="chart-legend">
                        <div class="legend-item"><div class="legend-color" style="background-color: #3498db;"></div><span>Actual Savings</span></div>
                        <div class="legend-item"><div class="legend-color" style="background-color: #e74c3c;"></div><span>Perceived Savings</span></div>
                        <div class="legend-item"><div class="legend-color" style="background-color: #2ecc71;"></div><span>(Label) Efficiency Action</span></div>
                        <div class="legend-item"><div class="legend-color" style="background-color: #f39c12;"></div><span>(Label) Curtailment Action</span></div>
                    </div>
                </div>
            </div>
        </div>

        <div id="steyvers" class="tab-content">
            <h2>Steyvers et al. (2025): What LLMs Know and What People Think They Know</h2>
            <div class="paper-summary">
                <div class="paper-badge">Steyvers et al. 2025</div>
                <h3>Key Findings: Calibration and Discrimination Gaps</h3>
                <p>Steyvers, Tejeda, Kumar, et al. (2025) investigated how LLMs communicate their internal confidence and how accurately humans perceive this, revealing critical gaps:</p>
                <ul>
                    <li><strong>Calibration Gap:</strong> A significant discrepancy exists between an LLM's internal confidence (e.g., token-level probability) and human-judged probability that the LLM is correct. Users often overestimate LLM accuracy, especially with default or longer explanations. (Measured by ECE - Expected Calibration Error).</li>
                    <li><strong>Discrimination Gap:</strong> Humans are often not very good at discriminating between correct and incorrect LLM answers based on default explanations, relative to the LLM's own internal confidence. (Measured by AUC - Area Under Curve).</li>
                    <li><strong>Effect of Explanation Style & Length:</strong>
                        <ul>
                            <li>Linguistic uncertainty cues (e.g., hedging language like "I am not sure") strongly influence human confidence; low-confidence expressions lead to lower human confidence.</li>
                            <li>Longer explanations tend to increase human confidence, often without a corresponding improvement in their ability to discriminate correct from incorrect answers.</li>
                        </ul>
                    </li>
                    <li><strong>Narrowing the Gap:</strong> Tailoring the LLM's explanation tone to reflect its internal confidence can narrow both the calibration and discrimination gaps, helping users weight AI advice more appropriately.</li>
                </ul>
                <h4>Relevance to New Proposal:</h4>
                <p>The new proposal ("Calibrating Trust in an LLM Energy Advisor") directly builds on Steyvers et al.'s work by:
                1. Adopting a similar methodology to first assess an LLM's internal confidence regarding energy estimates.
                2. Experimentally manipulating the linguistic expression of uncertainty in the LLM's explanations (confident, hedged-medium, hedged-low) based on this internal confidence.
                3. Aiming to see if these tailored uncertainty expressions can help users better calibrate their trust and improve reliance decisions in the energy domain, effectively trying to narrow the "calibration gap" for energy advice.
                </p>
            </div>

            <div class="flex-container">
                <div class="flex-item card">
                    <div class="card-header">Interactive AI Confidence Model</div>
                    <div class="card-content">
                        <p>Adjust sliders to see how LLM explanation style, length, and actual accuracy might affect human perception of AI confidence, illustrating concepts from Steyvers et al.</p>
                        <div class="slider-container">
                            <div class="slider-label"><span>Explanation Style:</span> <span id="steyvers-style-value" class="slider-value">Medium</span></div>
                            <input type="range" min="1" max="3" step="1" value="2" class="slider" id="steyvers-style-slider">
                            <p class="slider-description">1: Uncertain, 2: Medium, 3: Confident.</p>
                        </div>
                        <div class="slider-container">
                            <div class="slider-label"><span>Explanation Length:</span> <span id="steyvers-length-value" class="slider-value">Medium</span></div>
                            <input type="range" min="1" max="3" step="1" value="2" class="slider" id="steyvers-length-slider">
                            <p class="slider-description">1: Short, 2: Medium, 3: Long.</p>
                        </div>
                         <div class="slider-container">
                            <div class="slider-label"><span>Model's Actual Accuracy:</span> <span id="steyvers-accuracy-value" class="slider-value">70%</span></div>
                            <input type="range" min="50" max="90" step="5" value="70" class="slider" id="steyvers-accuracy-slider">
                            <p class="slider-description">Actual correctness rate of the LLM.</p>
                        </div>
                    </div>
                </div>
                 <div class="flex-item card">
                    <div class="card-header">AI Confidence Calibration Visualization</div>
                    <div class="card-content">
                        <div class="visualization" id="ai-calibration-chart">
                            <button class="annotations-toggle" onclick="toggleAnnotations('calibration-chart')">Annotations</button>
                            <div class="annotation" id="steyvers-annotation-1" style="top: 50px; left: 50px;">Perfect calibration (diagonal).</div>
                            <div class="annotation" id="steyvers-annotation-2" style="top: 150px; left: 150px;">Gap shows miscalibration.</div>
                        </div>
                        <div class="chart-legend">
                            <div class="legend-item"><div class="legend-color" style="background-color: #3498db;"></div><span>Perfect Calibration</span></div>
                            <div class="legend-item"><div class="legend-color" style="background-color: #e74c3c;"></div><span>Model Confidence</span></div>
                            <div class="legend-item"><div class="legend-color" style="background-color: #2ecc71;"></div><span>Human Perception</span></div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="flex-container">
                <div class="flex-item card">
                    <div class="card-header">Example AI Explanations & Confidence</div>
                    <div class="card-content">
                        <div id="ai-explanation-example-steyvers">
                            <h4 id="steyvers-question-text" class="font-semibold mb-2">Question: What is the capital of Australia?</h4>
                            <div id="steyvers-llm-answer" class="ai-explanation hedged-medium">
                                <strong>Answer:</strong> The capital of Australia is Canberra.
                                <br><strong>Explanation:</strong> It's a planned city built to be the capital.
                            </div>
                            <div class="mt-4">
                                <p>Model's Internal Confidence: <strong id="steyvers-model-confidence-value">70%</strong></p>
                                <p>Human-Perceived Confidence: <strong id="steyvers-human-confidence-value">85%</strong></p>
                                <p>Calibration Gap: <strong id="steyvers-calibration-gap-value">15%</strong></p>
                                <p>Correct Answer? <strong id="steyvers-is-correct" class="text-green-600">Yes</strong></p>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="flex-item card">
                    <div class="card-header">Discrimination Ability (AUC)</div>
                    <div class="card-content">
                         <p>This chart shows how well humans can discriminate between correct and incorrect AI answers compared to the model's own ability, based on simulated AUC scores.</p>
                        <div class="visualization" id="discrimination-chart">
                            <button class="annotations-toggle" onclick="toggleAnnotations('discrimination-chart')">Annotations</button>
                            <div class="annotation" id="steyvers-annotation-3" style="top: 50px; left: 50px;">Model is better at discriminating.</div>
                            <div class="annotation" id="steyvers-annotation-4" style="top: 150px; left: 150px;">Humans often struggle more.</div>
                        </div>
                        <div class="chart-legend">
                            <div class="legend-item"><div class="legend-color" style="background-color: #3498db;"></div><span>Model AUC</span></div>
                            <div class="legend-item"><div class="legend-color" style="background-color: #e74c3c;"></div><span>Human AUC</span></div>
                            <div class="legend-item"><div class="legend-color" style="background-color: #9b59b6;"></div><span>Discrimination Gap</span></div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <div id="simulation" class="tab-content">
            <h2>Proposed Study Simulation: Energy Advisor Interaction</h2>
            <p>This section provides a simplified, interactive simulation of the proposed experimental design: "Calibrating Trust in an LLM Energy Advisor: The Impact of Linguistic Uncertainty." The goal is to explore how an AI's expressed uncertainty influences your confidence and reliance on its energy-saving advice, considering your own initial estimates.</p>

            <div class="flex-container">
                <div class="flex-item card">
                    <div class="card-header">Step 1: Your Initial Energy Estimates (Phase 1)</div>
                    <div class="card-content simulation-controls">
                        <p>First, please provide your best estimate for the annual energy consumption (in kWh) for the following items. For reference, a 100W light bulb used for 3 hours a day for a year consumes about 110 kWh.</p>
                        <div id="user-energy-estimates-input">
                            </div>
                        <button id="submit-user-estimates" class="button-primary mt-4">Submit My Estimates & Proceed to AI Interaction</button>
                    </div>
                </div>

                <div class="flex-item card">
                    <div class="card-header">Step 2: AI Energy Advisor Interaction (Phase 2)</div>
                    <div class="card-content simulation-controls">
                        <p>Now, you will interact with an AI Energy Advisor for each item. The AI's internal confidence has been used to determine how it expresses uncertainty in its explanation.</p>
                        <div id="ai-interaction-area" class="hidden">
                            <h4 id="current-item-text" class="font-semibold text-lg mb-2">Current Item: </h4>
                            
                            <div>
                                <label for="ai-linguistic-tone">AI's Linguistic Tone (Experimental Condition):</label>
                                <select id="ai-linguistic-tone" class="mb-3">
                                    <option value="confident">Confident</option>
                                    <option value="hedged-medium" selected>Hedged - Medium Certainty</option>
                                    <option value="hedged-low">Hedged - Low Certainty (Uncertain)</option>
                                </select>
                            </div>

                            <p><strong>Your Previous Estimate:</strong> <span id="user-prior-estimate" class="font-bold">---</span> kWh</p>
                            <p><strong>AI Advisor's Estimate:</strong> <span id="ai-estimate-text" class="font-bold">---</span> kWh</p>
                            <div id="ai-explanation-text" class="ai-explanation hedged-medium my-3">AI explanation will appear here.</div>

                            <div class="slider-container mt-4">
                                <div class="slider-label"><span>Your Confidence in AI's Estimate:</span> <span id="user-confidence-ai-value" class="slider-value">50</span>%</div>
                                <input type="range" min="0" max="100" step="1" value="50" class="slider" id="user-confidence-ai-slider">
                            </div>

                            <p class="mt-4 font-semibold">Whose estimate do you think is more accurate?</p>
                            <div class="flex gap-4 mt-2">
                                <button id="choose-user-estimate" class="button-secondary">My Estimate</button>
                                <button id="choose-ai-estimate" class="button-primary">AI's Estimate</button>
                            </div>
                            <button id="next-item-simulation" class="button-primary mt-4 hidden">Next Item</button>
                        </div>
                         <div id="simulation-intro-text">
                            <p>Please complete Step 1 first.</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="card mt-6">
                <div class="card-header">Simulation Results & Individual Differences</div>
                <div class="card-content simulation-output">
                    <div id="simulation-results-summary">
                        <p>Complete the interaction to see a summary of your choices and confidence ratings.</p>
                    </div>
                    <div class="mt-4 simulation-controls">
                        <h4>Conceptual Impact of Individual Differences:</h4>
                        <label for="sim-energy-knowledge">Assumed Energy Knowledge Level:</label>
                        <select id="sim-energy-knowledge" class="mb-3">
                            <option value="low">Low</option>
                            <option value="medium" selected>Medium</option>
                            <option value="high">High</option>
                        </select>
                        <label for="sim-ai-literacy">Assumed AI Literacy Level:</label>
                        <select id="sim-ai-literacy" class="mb-3">
                            <option value="low">Low</option>
                            <option value="medium" selected>Medium</option>
                            <option value="high">High</option>
                        </select>
                        <div id="individual-differences-feedback" class="text-sm p-3 bg-gray-100 rounded-md border border-gray-300">
                            Select levels to see how these factors might conceptually influence interpretation of AI advice and reliance decisions.
                        </div>
                    </div>
                </div>
            </div>
        </div>


        <div id="methods" class="tab-content">
            <h2>Proposed Study: Methodology for "Calibrating Trust in an LLM Energy Advisor"</h2>
            <p>The proposed research aims to investigate how an LLM's linguistic expression of uncertainty affects user trust calibration in the context of energy-saving advice. The methodology integrates elements from Attari et al. (2010) for baseline energy knowledge assessment and Steyvers et al. (2025) for manipulating and measuring AI confidence communication.</p>

            <h3>A. Study Design</h3>
            <ul>
                <li><strong>Overall Design:</strong> Mixed-factorial design.</li>
                <li><strong>Phase 1 (Baseline Assessment):</strong> Correlational. Participants' baseline energy knowledge is assessed.</li>
                <li><strong>Phase 2 (AI Interaction Task):</strong>
                    <ul>
                        <li><strong>Within-Subjects Factors:</strong>
                            <ul>
                                <li><strong>Item:</strong> (e.g., 4-8 different energy appliances/scenarios, counterbalanced order). Each item will have a known ground-truth energy value.</li>
                                <li><strong>AI Advice Correctness:</strong> (Correct, Incorrect – pre-determined for each AI presentation of an item, balanced across items).</li>
                            </ul>
                        </li>
                        <li><strong>Between-Subjects Factor (or carefully counterbalanced Within-Subjects):</strong>
                            <ul>
                                <li><strong>AI Linguistic Uncertainty Expression Style:</strong>
                                    <ol>
                                        <li><strong>Condition 1 (Calibrated Uncertainty):</strong> AI uses confident language for correct advice (where internal model confidence would be high) and hedged language (e.g., medium or low uncertainty) for incorrect advice (where internal model confidence would be low). <em>This is the key condition for testing calibration benefits.</em></li>
                                        <li><strong>Condition 2 (Consistently Confident):</strong> AI always uses confident language, regardless of its correctness or internal confidence. (Control)</li>
                                        <li><strong>Condition 3 (Miscalibrated/Random Uncertainty - Optional):</strong> AI uses confident/hedged language randomly or in a manner mismatched with its internal confidence/correctness. (Advanced control).</li>
                                    </ol>
                                </li>
                            </ul>
                        </li>
                    </ul>
                </li>
            </ul>

            <h3>C. Materials & Measures</h3>
            <ol class="list-decimal pl-5 space-y-2">
                <li><strong>Appliance Set:</strong> List of 8-10 common household appliances/activities with established average energy consumption values (e.g., from Attari et al., NREL, EnergyStar). These will be used in both Phase 1 and Phase 2.</li>
                <li><strong>Phase 1 - Baseline Energy Knowledge:</strong>
                    <ul>
                        <li><strong>Appliance Energy Estimation Task:</strong> "Relative to a 100-watt incandescent light bulb which uses 100 units of energy per hour, how many units of energy do you think the following device typically uses in one hour?" (for all appliances).</li>
                        <li><strong>Accuracy Score:</strong> Calculated for each participant based on their Phase 1 estimates compared to ground truth (e.g., log-ratio accuracy, absolute error).</li>
                    </ul>
                </li>
                <li><strong>Phase 2 - AI Energy Advisor Interaction:</strong>
                    <ul>
                        <li><strong>AI-Generated Advice:</strong> For each appliance, pre-scripted AI estimates and explanations.
                            <ul>
                                <li><strong>Estimates:</strong> One "correct" (close to true value) and one "incorrect" (e.g., systematically overestimated for low-use items, underestimated for high-use items, mimicking Attari's findings but from the AI).</li>
                                <li><strong>Explanations (Linguistic Manipulation based on assigned condition):</strong>
                                    <ul>
                                        <li><em>Confident Tone:</em> Clear, direct language, assertive statements (e.g., "A stereo typically uses <strong>20 units per hour</strong>. This is because most standard stereo systems consume between 10 to 50 watts during operation.")</li>
                                        <li><em>Hedged Tone (Medium/Low):</em> Softer language, qualifiers, uncertainty markers (e.g., "My estimate is around <strong>20 units</strong> for the 'Stereo.' It seems that the estimated energy use for a 'Stereo' could be in the ballpark of <strong>20 units per hour</strong>, though this can vary. This might be typical for many standard stereo systems that often consume somewhere between 10 to 50 watts, but it's just an estimate.")</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li><strong>Dependent Variables (Trial-Level):</strong>
                            <ul>
                                <li><strong>AI Confidence Rating:</strong> "How confident are you that the AI's estimate shown above is correct for this item?" (0-100% slider).</li>
                                <li><strong>Reliance (Forced Choice):</strong> "Considering your own initial estimate and the AI's estimate, which do you believe is more accurate?" (Buttons: "My Initial Estimate," "The AI's Estimate").</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li><strong>Individual Difference Measures (Administered before Phase 1 or between Phase 1 and 2):</strong>
                    <ul>
                        <li>AI Literacy Scale (e.g., AICOS-SV - Markus et al., 2025).</li>
                        <li>Numeracy Scale (e.g., Berlin Numeracy Test - Cokely et al., 2012).</li>
                        <li>Propensity to Trust Automation (e.g., Jian–Bisantz–Drury Trust in Automation Scale).</li>
                        <li>Demographics: Age, education, prior experience with LLMs.</li>
                    </ul>
                </li>
                <li><strong>Post-Task Measures:</strong>
                    <ul>
                        <li>Overall Trust in the AI Advisor (re-administer trust scale).</li>
                        <li>Manipulation Check (e.g., "How certain or uncertain did the AI generally seem in its advice?").</li>
                        <li>Open-ended feedback.</li>
                    </ul>
                </li>
            </ol>
            <p><strong>Reliance Operationalization:</strong> "Reliance" in this task is primarily measured by the participant's choice in the forced-choice task (choosing AI's estimate vs. their own).</p>

            <h3>D. Procedure Diagram</h3>
            <div class="methods-diagram-container my-6">
                <div class="methods-step">
                    <strong>1. Consent & Demographics</strong>
                    <p class="text-sm">Obtain informed consent, collect basic demographics and LLM familiarity.</p>
                </div>
                <div class="methods-arrow">↓</div>
                <div class="methods-step">
                    <strong>2. Individual Difference Measures</strong>
                    <p class="text-sm">Administer scales for AI literacy, numeracy, propensity to trust automation, etc.</p>
                </div>
                <div class="methods-arrow">↓</div>
                <div class="methods-step">
                    <strong>3. Phase 1: Baseline Energy Estimation</strong>
                    <p class="text-sm">Participants estimate energy use for items (Attari-like task). Estimates recorded.</p>
                </div>
                <div class="methods-arrow">↓</div>
                <div class="methods-step">
                    <strong>4. Instructions for Phase 2</strong>
                    <p class="text-sm">Introduce "AI Energy Advisor," explain confidence rating and forced-choice task.</p>
                </div>
                <div class="methods-arrow">↓</div>
                <div class="methods-step">
                    <strong>5. Phase 2: AI Interaction Task (Trial-by-Trial Loop for N items)</strong>
                    <p class="text-sm">
                        For each item: <br>
                        a) Present AI estimate & (manipulated) explanation.<br>
                        b) Participant rates confidence in AI (slider).<br>
                        c) Show participant's Phase 1 estimate alongside AI's.<br>
                        d) Participant makes forced choice (My Estimate / AI's Estimate).
                    </p>
                </div>
                <div class="methods-arrow">↓</div>
                <div class="methods-step">
                    <strong>6. Post-Task Measures</strong>
                    <p class="text-sm">Administer overall trust scale, manipulation check, open-ended feedback.</p>
                </div>
                 <div class="methods-arrow">↓</div>
                <div class="methods-step">
                    <strong>7. Debriefing</strong>
                    <p class="text-sm">Explain study purpose, manipulations, provide accurate energy info.</p>
                </div>
            </div>
        </div>

        <div id="implications" class="tab-content">
            <h2>Research Questions, Hypotheses & Implications for "Calibrating Trust in an LLM Energy Advisor"</h2>

            <h3>Re-engineered Research Questions (RQs)</h3>
            <table class="w-full text-sm text-left text-gray-500">
                <thead class="text-xs text-gray-700 uppercase bg-gray-50">
                    <tr>
                        <th scope="col" class="px-6 py-3">ID</th>
                        <th scope="col" class="px-6 py-3">Refined Statement</th>
                        <th scope="col" class="px-6 py-3">Justification</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="bg-white border-b">
                        <td class="px-6 py-4 font-medium">RQ1</td>
                        <td class="px-6 py-4 italic">How does aligning the LLM’s expressed confidence with its internal probability estimates affect users’ ability to discriminate between accurate and inaccurate AI advice?</td>
                        <td class="px-6 py-4">Extends Steyvers gap-reduction claim to energy domain.</td>
                    </tr>
                    <tr class="bg-white border-b">
                        <td class="px-6 py-4 font-medium">RQ2</td>
                        <td class="px-6 py-4 italic">Does linguistic confidence framing modulate reliance differently for users with high vs. low baseline energy literacy?</td>
                        <td class="px-6 py-4">Interaction of advisor cue with judge competence (Bonaccio & Dalal, 2006).</td>
                    </tr>
                    <tr class="bg-white border-b">
                        <td class="px-6 py-4 font-medium">RQ3</td>
                        <td class="px-6 py-4 italic">What metacognitive signals (self-reported confidence, verification time, information seeking) mediate the relation between uncertainty framing and behavioral reliance?</td>
                        <td class="px-6 py-4">Tests two-stage advice model (COM-BI).</td>
                    </tr>
                    <tr class="bg-white">
                        <td class="px-6 py-4 font-medium">RQ4</td>
                        <td class="px-6 py-4 italic">Does minimal outcome feedback recalibrate reliance trajectories over time, and is this moderated by calibration-presence?</td>
                        <td class="px-6 py-4">Examines learning under partial feedback.</td>
                    </tr>
                </tbody>
            </table>

            <h3 class="mt-6">Hypotheses (Directional)</h3>
             <table class="w-full text-sm text-left text-gray-500">
                <thead class="text-xs text-gray-700 uppercase bg-gray-50">
                    <tr>
                        <th scope="col" class="px-6 py-3">ID</th>
                        <th scope="col" class="px-6 py-3">Hypothesis</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="bg-white border-b">
                        <td class="px-6 py-4 font-medium">H1 (Calibration Effect)</td>
                        <td class="px-6 py-4">When calibration is present (AI's expressed uncertainty aligns with its internal confidence/accuracy), Expected Calibration Error (ECE) between human confidence and ground truth will be lower than in calibration-absent (e.g., always confident) conditions.</td>
                    </tr>
                    <tr class="bg-white border-b">
                        <td class="px-6 py-4 font-medium">H2 (Appropriate Reliance)</td>
                        <td class="px-6 py-4">Under *low* expressed AI confidence (hedged language), participants will choose the AI estimate less often, and this effect will be more pronounced *only* when the AI is wrong (positive discrimination in reliance).</td>
                    </tr>
                    <tr class="bg-white border-b">
                        <td class="px-6 py-4 font-medium">H3 (Competence-Cue Interaction)</td>
                        <td class="px-6 py-4">The negative effect of low AI confidence on reliance (i.e., choosing AI less) will be larger for high-energy-literacy users than low-energy-literacy users, as high-literacy users can better discern when to trust their own judgment.</td>
                    </tr>
                    <tr class="bg-white border-b">
                        <td class="px-6 py-4 font-medium">H4 (Explanation Length Bias)</td>
                        <td class="px-6 py-4">Longer AI explanations will increase human confidence in the AI's estimate, independent of the AI's actual accuracy or expressed uncertainty tone, replicating Steyvers et al. (AUC for discrimination may remain unchanged while confidence increases).</td>
                    </tr>
                     <tr class="bg-white">
                        <td class="px-6 py-4 font-medium">H5 (Feedback Learning)</td>
                        <td class="px-6 py-4">After receiving minimal outcome feedback (e.g., on a subset of trials), reliance on incorrect high-confidence AI answers will decrease more sharply in calibration-present conditions compared to calibration-absent conditions.</td>
                    </tr>
                </tbody>
            </table>
            <p class="text-sm mt-2">Note: The original H1a, H1b, H2a, H2b, H3, H4a, H4b from the prompt have been integrated or refined into the table above for clarity and alignment with the "Re-engineered" section.</p>


            <h3 class="mt-6">Potential Implications of the Research</h3>
            <ul>
                <li><strong>Improved Human-AI Interaction Design:</strong> Findings could inform how to design AI systems (especially LLMs) that communicate uncertainty more effectively, leading to better user understanding and more appropriate reliance.</li>
                <li><strong>Enhanced User Calibration:</strong> If expressing uncertainty appropriately helps users calibrate their trust, they may be more likely to trust accurate AI advice and be skeptical of inaccurate advice, reducing over-reliance or under-reliance.</li>
                <li><strong>Addressing Energy Misperceptions:</strong> By combining AI advice with an understanding of user's prior knowledge (Attari et al.), the research could suggest ways AI might help correct widespread energy misperceptions.</li>
                <li><strong>Personalized AI Advising:</strong> Understanding how individual differences (energy knowledge, AI literacy) moderate responses to AI uncertainty could pave the way for AI advisors that adapt their communication style to individual users.</li>
                <li><strong>Theoretical Contributions:</strong> The study can contribute to theories of trust in automation, advice-taking, metacognition, and human-computer interaction by examining these processes in the context of modern LLMs and a critical real-world domain.</li>
                 <li><strong>Outcome Focus Alignment:</strong> This work aims to inform the design of energy-advisor chatbots that neither over- nor under-persuade, advancing both behavioral energy science and Explainable AI (XAI).</li>
            </ul>
        </div>

    </div>

    <script>
        // --- Tab Navigation ---
        function openTab(evt, tabName) {
            let i, tabcontent, tablinks;
            tabcontent = document.getElementsByClassName("tab-content");
            for (i = 0; i < tabcontent.length; i++) {
                tabcontent[i].style.display = "none";
                tabcontent[i].classList.remove("active");
            }
            tablinks = document.getElementsByClassName("tab");
            for (i = 0; i < tablinks.length; i++) {
                tablinks[i].classList.remove("active");
            }
            document.getElementById(tabName).style.display = "block";
            document.getElementById(tabName).classList.add("active");
            if (evt) evt.currentTarget.classList.add("active");

            if (tabName === 'attari') {
                initializeAttariSimulation();
            } else if (tabName === 'steyvers') {
                initializeSteyversSimulation();
            } else if (tabName === 'simulation') {
                initializeProposedStudySimulation();
            }
        }

        function toggleAnnotations(chartType) {
            const chartElement = event.target.closest('.visualization');
            if (!chartElement) return;
            const annotations = chartElement.querySelectorAll('.annotation');
            let anyVisible = false;
            annotations.forEach(ann => {
                if (ann.style.display === 'block') anyVisible = true;
            });
            annotations.forEach(ann => {
                ann.style.display = anyVisible ? 'none' : 'block';
            });
            event.target.textContent = anyVisible ? 'Show Annotations' : 'Hide Annotations';
        }

        const attariEnergyItems = [
            { name: "Central AC (1hr)", actual: 3500, perceived: 1250 },
            { name: "Clothes Dryer (1hr)", actual: 3000, perceived: 1071 },
            { name: "Desktop Computer (1hr)", actual: 100, perceived: 35 },
            { name: "100W Bulb (1hr)", actual: 100, perceived: 100 },
            { name: "Laptop (1hr)", actual: 35, perceived: 50 },
            { name: "CFL Bulb (1hr)", actual: 25, perceived: 40 }
        ];
        const attariConservationActions = [
            { name: "Efficient Dryer", type: "Efficiency", actual: 700, perceived: 200, color: '#2ecc71' },
            { name: "Turn off Lights", type: "Curtailment", actual: 100, perceived: 150, color: '#f39c12'  },
            { name: "Efficient Bulbs", type: "Efficiency", actual: 75, perceived: 60, color: '#2ecc71' },
            { name: "Unplug Devices", type: "Curtailment", actual: 50, perceived: 80, color: '#f39c12'  }
        ];

        let attariCompressionSlider, attariSlopeSlider, attariNumeracySlider;
        let attariCompressionValue, attariSlopeValue, attariNumeracyValue;

        function initializeAttariSimulation() {
            attariCompressionSlider = document.getElementById('attari-compression-slider');
            attariSlopeSlider = document.getElementById('attari-slope-slider');
            attariNumeracySlider = document.getElementById('attari-numeracy-slider');
            attariCompressionValue = document.getElementById('attari-compression-value');
            attariSlopeValue = document.getElementById('attari-slope-value');
            attariNumeracyValue = document.getElementById('attari-numeracy-value');

            if (!attariCompressionSlider || attariCompressionSlider.dataset.initialized === 'true') return;

            attariCompressionSlider.oninput = updateAttariVisuals;
            attariSlopeSlider.oninput = updateAttariVisuals;
            attariNumeracySlider.oninput = updateAttariVisuals;
            updateAttariVisuals();
            attariCompressionSlider.dataset.initialized = 'true';
        }

        function updateAttariVisuals() {
            if (!attariCompressionSlider) return;
            const compression = parseFloat(attariCompressionSlider.value);
            const slope = parseFloat(attariSlopeSlider.value);
            const numeracy = parseInt(attariNumeracySlider.value);
            const numeracyLabels = ['Low', 'Average', 'High'];

            attariCompressionValue.textContent = compression.toFixed(1);
            attariSlopeValue.textContent = slope.toFixed(2);
            attariNumeracyValue.textContent = numeracyLabels[numeracy - 1];

            let adjustedSlope = slope;
            if (numeracy === 1) adjustedSlope *= 0.8;
            if (numeracy === 3) adjustedSlope *= 1.2;

            const processedEnergyData = attariEnergyItems.map(item => {
                const logActual = Math.log10(item.actual + 1);
                const meanLogActual = Math.log10(100 + 1);
                let perceived;
                if (item.actual > 100) perceived = item.actual / compression;
                else if (item.actual < 100) perceived = item.actual * (compression / 2);
                else perceived = item.actual;
                const logPerceivedInitial = Math.log10(Math.max(1, perceived));
                const logPerceivedFinal = meanLogActual + adjustedSlope * (logPerceivedInitial - meanLogActual);
                perceived = Math.pow(10, logPerceivedFinal) -1;
                return { ...item, perceivedDisplay: Math.max(1, Math.round(perceived)) };
            });
            renderEnergyPerceptionChart(processedEnergyData);
            renderCurtailmentEfficiencyChart(attariConservationActions, adjustedSlope, compression);
        }

        function renderEnergyPerceptionChart(data) {
            const container = d3.select("#energy-perception-chart");
            container.selectAll("svg").remove();
            if (!container.node()) return;


            const margin = {top: 40, right: 50, bottom: 60, left: 70};
            const width = container.node().clientWidth - margin.left - margin.right;
            const height = 350 - margin.top - margin.bottom;
            if (width <=0 || height <=0) return;


            const svg = container.append("svg")
                .attr("width", width + margin.left + margin.right)
                .attr("height", height + margin.top + margin.bottom)
              .append("g")
                .attr("transform", `translate(${margin.left},${margin.top})`);

            const x = d3.scaleLog().base(10)
                .domain([Math.min(10, d3.min(data, d => d.actual)), d3.max(data, d => d.actual)])
                .range([0, width]);

            const y = d3.scaleLog().base(10)
                .domain([Math.min(10, d3.min(data, d => d.perceivedDisplay)), Math.max(d3.max(data, d => d.actual), d3.max(data, d => d.perceivedDisplay))])
                .range([height, 0]);

            svg.append("g").attr("transform", `translate(0,${height})`).call(d3.axisBottom(x).ticks(5, ".0s")).selectAll("text").style("font-size", "10px");
            svg.append("text").attr("text-anchor", "end").attr("x", width / 2 + margin.left/2).attr("y", height + margin.top + 10).text("Actual Energy (Wh, log scale)").style("font-size", "12px");
            svg.append("g").call(d3.axisLeft(y).ticks(5, ".0s")).selectAll("text").style("font-size", "10px");
            svg.append("text").attr("text-anchor", "end").attr("transform", "rotate(-90)").attr("y", -margin.left + 20).attr("x", -height / 2).text("Perceived Energy (Wh, log scale)").style("font-size", "12px");

            svg.append("line").attr("x1", x(d3.min(data, d => d.actual))).attr("y1", y(d3.min(data, d => d.actual))).attr("x2", x(d3.max(data, d => d.actual))).attr("y2", y(d3.max(data, d => d.actual)))
                .attr("stroke", "#3498db").attr("stroke-width", 2).attr("stroke-dasharray", "5,5");

            svg.append("path").datum(data.sort((a,b) => a.actual - b.actual)).attr("fill", "none").attr("stroke", "#e74c3c").attr("stroke-width", 2.5)
                .attr("d", d3.line().x(d => x(d.actual)).y(d => y(d.perceivedDisplay)));

            svg.selectAll("dot").data(data).enter().append("circle").attr("cx", d => x(d.actual)).attr("cy", d => y(d.perceivedDisplay)).attr("r", 5).attr("fill", "#e74c3c")
                .on("mouseover", function(event, d) {
                    const tooltip = d3.select("body").append("div").attr("class", "tooltip").style("opacity", 0);
                    tooltip.transition().duration(200).style("opacity", .9);
                    tooltip.html(`<strong>${d.name}</strong><br/>Actual: ${d.actual} Wh<br/>Perceived: ${d.perceivedDisplay} Wh`)
                        .style("left", (event.pageX + 5) + "px").style("top", (event.pageY - 28) + "px");
                }).on("mouseout", function(d) { d3.selectAll(".tooltip").transition().duration(500).style("opacity", 0).remove(); });
            
            svg.append("text").attr("x", width / 2).attr("y", 0 - (margin.top / 2) + 5).attr("text-anchor", "middle").style("font-size", "14px").style("font-weight", "bold").text("Energy Perception (Attari et al.)");
        }

        function renderCurtailmentEfficiencyChart(data, slope, compressionFactor) {
            const container = d3.select("#curtailment-efficiency-chart");
            container.selectAll("svg").remove();
            if (!container.node()) return;

            const margin = {top: 40, right: 20, bottom: 70, left: 150};
            const width = container.node().clientWidth - margin.left - margin.right;
            const height = 450 - margin.top - margin.bottom;
            if (width <=0 || height <=0) return;

            const svg = container.append("svg").attr("width", width + margin.left + margin.right).attr("height", height + margin.top + margin.bottom).append("g").attr("transform", `translate(${margin.left},${margin.top})`);

            const processedData = data.map(d => {
                let perceived = d.actual / compressionFactor;
                if (d.actual < 100) perceived = d.actual * (compressionFactor / 2);
                const logActual = Math.log10(d.actual + 1);
                const meanLogActual = Math.log10(100 + 1);
                const logPerceivedInitial = Math.log10(Math.max(1, perceived));
                const logPerceivedFinal = meanLogActual + slope * (logPerceivedInitial - meanLogActual);
                perceived = Math.pow(10, logPerceivedFinal) -1;
                return {...d, perceivedDisplay: Math.max(1, Math.round(perceived))};
            });

            const y = d3.scaleBand().range([0, height]).domain(processedData.map(d => d.name)).padding(0.2);
            svg.append("g").call(d3.axisLeft(y)).selectAll("text").style("font-size", "11px").attr("fill", d => processedData.find(item => item.name === d)?.color || '#333');

            const x = d3.scaleLinear().domain([0, d3.max(processedData, d => Math.max(d.actual, d.perceivedDisplay))]).range([ 0, width]);
            svg.append("g").attr("transform", `translate(0,${height})`).call(d3.axisBottom(x).ticks(5)).selectAll("text").style("font-size", "10px");
            svg.append("text").attr("text-anchor", "end").attr("x", width / 2 + margin.left/2).attr("y", height + margin.top).text("Energy Savings (Wh)").style("font-size", "12px");

            svg.selectAll(".bar-actual").data(processedData).enter().append("rect").attr("class", "bar-actual").attr("y", d => y(d.name)).attr("x", x(0)).attr("width", d => Math.max(0, x(d.actual))).attr("height", y.bandwidth() / 2 - 1).attr("fill", "#3498db")
                .on("mouseover", function(event, d) {
                    const tooltip = d3.select("body").append("div").attr("class", "tooltip").style("opacity", 0);
                    tooltip.transition().duration(200).style("opacity", .9);
                    tooltip.html(`<strong>${d.name} (Actual)</strong><br/>Savings: ${d.actual} Wh<br/>Type: ${d.type}`).style("left", (event.pageX + 5) + "px").style("top", (event.pageY - 28) + "px");
                }).on("mouseout", function(d) { d3.selectAll(".tooltip").transition().duration(500).style("opacity", 0).remove(); });

            svg.selectAll(".bar-perceived").data(processedData).enter().append("rect").attr("class", "bar-perceived").attr("y", d => y(d.name) + y.bandwidth() / 2 + 1).attr("x", x(0)).attr("width", d => Math.max(0,x(d.perceivedDisplay))).attr("height", y.bandwidth() / 2 - 1).attr("fill", "#e74c3c")
                 .on("mouseover", function(event, d) {
                    const tooltip = d3.select("body").append("div").attr("class", "tooltip").style("opacity", 0);
                    tooltip.transition().duration(200).style("opacity", .9);
                    tooltip.html(`<strong>${d.name} (Perceived)</strong><br/>Savings: ${d.perceivedDisplay} Wh<br/>Type: ${d.type}`).style("left", (event.pageX + 5) + "px").style("top", (event.pageY - 28) + "px");
                }).on("mouseout", function(d) { d3.selectAll(".tooltip").transition().duration(500).style("opacity", 0).remove(); });
            
            svg.append("text").attr("x", width / 2).attr("y", 0 - (margin.top / 2) + 5).attr("text-anchor", "middle").style("font-size", "14px").style("font-weight", "bold").text("Curtailment vs. Efficiency Actions");
        }

        const steyversLLMExamples = [
            { question: "Capital of Australia?", answer: "Canberra.", modelConfidence: { low: 0.85, medium: 0.9, high: 0.95 }, humanConfidenceBase: { low: 0.7, medium: 0.85, high: 0.92 }, correct: true, explanations: { low: "I think it's Canberra.", medium: "Canberra is the capital.", high: "It is definitively Canberra."} },
            { question: "Largest Ocean?", answer: "Pacific.", modelConfidence: { low: 0.9, medium: 0.95, high: 0.98 }, humanConfidenceBase: { low: 0.75, medium: 0.88, high: 0.96 }, correct: true, explanations: { low: "Probably the Pacific.", medium: "The Pacific Ocean.", high: "The Pacific Ocean is the largest."} },
            { question: "Author of '1984'?", answer: "George Orwell.", modelConfidence: { low: 0.8, medium: 0.88, high: 0.92 }, humanConfidenceBase: { low: 0.65, medium: 0.80, high: 0.90 }, correct: true, explanations: {low: "Maybe Orwell?", medium: "George Orwell wrote it.", high: "George Orwell is the author."} },
            { question: "Speed of Light (km/s)?", answer: "Approx 300,000 km/s.", modelConfidence: { low: 0.75, medium: 0.85, high: 0.90 }, humanConfidenceBase: { low: 0.60, medium: 0.78, high: 0.88 }, correct: true, explanations: {low: "Around 300k km/s, I believe.", medium: "It's about 300,000 km/s.", high: "The speed is ~300,000 km/s."} },
             { question: "Is Mars bigger than Earth?", answer: "No, Earth is bigger.", modelConfidence: { low: 0.70, medium: 0.80, high: 0.85 }, humanConfidenceBase: { low: 0.55, medium: 0.70, high: 0.80 }, correct: false, explanations: {low: "I'm not sure, perhaps Mars is slightly larger?", medium: "Mars is a large planet, similar in size to Earth.", high: "Mars is definitely larger than Earth due to its extensive volcanic structures."} }
        ];
        let steyversStyleSlider, steyversLengthSlider, steyversAccuracySlider;
        let steyversStyleValue, steyversLengthValue, steyversAccuracyValue;
        let currentSteyversExampleIdx = 0;

        function initializeSteyversSimulation() {
            steyversStyleSlider = document.getElementById('steyvers-style-slider');
            steyversLengthSlider = document.getElementById('steyvers-length-slider');
            steyversAccuracySlider = document.getElementById('steyvers-accuracy-slider');
            steyversStyleValue = document.getElementById('steyvers-style-value');
            steyversLengthValue = document.getElementById('steyvers-length-value');
            steyversAccuracyValue = document.getElementById('steyvers-accuracy-value');

            if(!steyversStyleSlider || steyversStyleSlider.dataset.initialized === 'true') return;

            steyversStyleSlider.oninput = updateSteyversVisuals;
            steyversLengthSlider.oninput = updateSteyversVisuals;
            steyversAccuracySlider.oninput = updateSteyversVisuals;
            updateSteyversVisuals();
            steyversStyleSlider.dataset.initialized = 'true';
        }

        function updateSteyversVisuals() {
            if(!steyversStyleSlider) return;
            const style = parseInt(steyversStyleSlider.value);
            const length = parseInt(steyversLengthSlider.value);
            const accuracyPct = parseInt(steyversAccuracySlider.value);

            const styleLabels = ['Uncertain', 'Medium', 'Confident'];
            const lengthLabels = ['Short', 'Medium', 'Long'];
            steyversStyleValue.textContent = styleLabels[style - 1];
            steyversLengthValue.textContent = lengthLabels[length - 1];
            steyversAccuracyValue.textContent = accuracyPct + '%';
            
            currentSteyversExampleIdx = (currentSteyversExampleIdx + 1) % steyversLLMExamples.length;
            const example = steyversLLMExamples[currentSteyversExampleIdx];

            let modelConf, humanConfBase, explanationText;
            if (style === 1) { modelConf = example.modelConfidence.low; humanConfBase = example.humanConfidenceBase.low; explanationText = example.explanations.low; }
            else if (style === 2) { modelConf = example.modelConfidence.medium; humanConfBase = example.humanConfidenceBase.medium; explanationText = example.explanations.medium; }
            else { modelConf = example.modelConfidence.high; humanConfBase = example.humanConfidenceBase.high; explanationText = example.explanations.high; }

            let finalHumanConf = humanConfBase;
            if (length === 1) finalHumanConf -= 0.05;
            if (length === 3) finalHumanConf += 0.10;
            finalHumanConf = finalHumanConf * (accuracyPct / 70);
            finalHumanConf = Math.max(0.1, Math.min(0.99, finalHumanConf));
            modelConf = Math.max(0.1, Math.min(0.99, modelConf * (accuracyPct / 70)));

            document.getElementById('steyvers-question-text').textContent = `Question: ${example.question}`;
            const llmAnswerDiv = document.getElementById('steyvers-llm-answer');
            llmAnswerDiv.innerHTML = `<strong>Answer:</strong> ${example.answer}<br><strong>Explanation:</strong> ${explanationText}`;
            llmAnswerDiv.className = 'ai-explanation';
            if (style === 1) llmAnswerDiv.classList.add('hedged-low');
            else if (style === 2) llmAnswerDiv.classList.add('hedged-medium');
            else llmAnswerDiv.classList.add('confident');

            document.getElementById('steyvers-model-confidence-value').textContent = `${(modelConf * 100).toFixed(0)}%`;
            document.getElementById('steyvers-human-confidence-value').textContent = `${(finalHumanConf * 100).toFixed(0)}%`;
            document.getElementById('steyvers-calibration-gap-value').textContent = `${(Math.abs(finalHumanConf - modelConf) * 100).toFixed(0)}%`;
            const isCorrectEl = document.getElementById('steyvers-is-correct');
            isCorrectEl.textContent = example.correct ? 'Yes' : 'No';
            isCorrectEl.style.color = example.correct ? 'green' : 'red';

            renderAICalibrationChart(modelConf, finalHumanConf, accuracyPct / 100);
            renderDiscriminationChart(modelConf, finalHumanConf, style, length, accuracyPct / 100);
        }

        function renderAICalibrationChart(modelConfidence, humanPerception, actualAccuracy) {
            const container = d3.select("#ai-calibration-chart");
            container.selectAll("svg").remove();
            if (!container.node()) return;

            const margin = {top: 40, right: 20, bottom: 60, left: 60};
            const width = container.node().clientWidth - margin.left - margin.right;
            const height = 350 - margin.top - margin.bottom;
             if (width <=0 || height <=0) return;

            const svg = container.append("svg").attr("width", width + margin.left + margin.right).attr("height", height + margin.top + margin.bottom).append("g").attr("transform", `translate(${margin.left},${margin.top})`);
            const x = d3.scaleLinear().domain([0, 1]).range([0, width]);
            const y = d3.scaleLinear().domain([0, 1]).range([height, 0]);

            svg.append("g").attr("transform", `translate(0,${height})`).call(d3.axisBottom(x).ticks(5, "%"));
            svg.append("text").attr("text-anchor", "end").attr("x", width / 2 + margin.left/2).attr("y", height + margin.top -10).text("Confidence").style("font-size", "12px");
            svg.append("g").call(d3.axisLeft(y).ticks(5, "%"));
            svg.append("text").attr("text-anchor", "end").attr("transform", "rotate(-90)").attr("y", -margin.left + 20).attr("x", -height / 2).text("Accuracy").style("font-size", "12px");
            svg.append("line").attr("x1", x(0)).attr("y1", y(0)).attr("x2", x(1)).attr("y2", y(1)).attr("stroke", "#3498db").attr("stroke-width", 2).attr("stroke-dasharray", "5,5");

            const dataPoints = [
                { type: "model", confidence: modelConfidence * 0.5, accuracy: actualAccuracy * modelConfidence * 0.5 },
                { type: "model", confidence: modelConfidence, accuracy: actualAccuracy * modelConfidence },
                { type: "model", confidence: modelConfidence * 1.2 > 1 ? 1 : modelConfidence * 1.2 , accuracy: actualAccuracy * (modelConfidence*1.2 > 1 ? 1 : modelConfidence * 1.2) },
                { type: "human", confidence: humanPerception * 0.5, accuracy: actualAccuracy * humanPerception * 0.5 },
                { type: "human", confidence: humanPerception, accuracy: actualAccuracy * humanPerception },
                { type: "human", confidence: humanPerception * 1.2 > 1 ? 1 : humanPerception * 1.2, accuracy: actualAccuracy * (humanPerception*1.2 > 1 ? 1 : humanPerception * 1.2) },
            ].map(d => ({...d, confidence: Math.min(1, Math.max(0,d.confidence)), accuracy: Math.min(1, Math.max(0,d.accuracy)) }));

            const modelLine = d3.line().x(d => x(d.confidence)).y(d => y(d.accuracy * (d.confidence / modelConfidence)));
            svg.append("path").datum(dataPoints.filter(d => d.type === 'model').sort((a,b) => a.confidence - b.confidence)).attr("fill", "none").attr("stroke", "#e74c3c").attr("stroke-width", 2.5).attr("d", modelLine);
            svg.selectAll(".dot-model").data(dataPoints.filter(d => d.type === 'model')).enter().append("circle").attr("class", "dot-model").attr("cx", d => x(d.confidence)).attr("cy", d => y(d.accuracy * (d.confidence / modelConfidence))).attr("r", 5).attr("fill", "#e74c3c");

            const humanLine = d3.line().x(d => x(d.confidence)).y(d => y(d.accuracy * (d.confidence / humanPerception)));
            svg.append("path").datum(dataPoints.filter(d => d.type === 'human').sort((a,b) => a.confidence - b.confidence)).attr("fill", "none").attr("stroke", "#2ecc71").attr("stroke-width", 2.5).attr("d", humanLine);
            svg.selectAll(".dot-human").data(dataPoints.filter(d => d.type === 'human')).enter().append("circle").attr("class", "dot-human").attr("cx", d => x(d.confidence)).attr("cy", d => y(d.accuracy * (d.confidence / humanPerception))).attr("r", 5).attr("fill", "#2ecc71");
            
            svg.append("text").attr("x", width / 2).attr("y", 0 - (margin.top / 2) + 5).attr("text-anchor", "middle").style("font-size", "14px").style("font-weight", "bold").text("AI Confidence Calibration (Steyvers et al.)");
        }

        function renderDiscriminationChart(modelConf, humanConf, style, length, actualAccuracy) {
            const container = d3.select("#discrimination-chart");
            container.selectAll("svg").remove();
             if (!container.node()) return;

            const margin = {top: 40, right: 20, bottom: 60, left: 60};
            const width = container.node().clientWidth - margin.left - margin.right;
            const height = 350 - margin.top - margin.bottom;
            if (width <=0 || height <=0) return;

            const svg = container.append("svg").attr("width", width + margin.left + margin.right).attr("height", height + margin.top + margin.bottom).append("g").attr("transform", `translate(${margin.left},${margin.top})`);

            let modelAUC = 0.75 + (actualAccuracy - 0.7) * 0.2;
            let humanAUC = 0.59;
            if (style === 1) humanAUC += 0.05; if (style === 3) humanAUC -= 0.03;
            if (length === 1) humanAUC += 0.02; if (length === 3) humanAUC -= 0.04;
            humanAUC = Math.max(0.5, Math.min(modelAUC - 0.05, humanAUC));
            modelAUC = Math.max(0.5, Math.min(0.9, modelAUC));

            const data = [ { name: "Model AUC", value: modelAUC, color: "#3498db" }, { name: "Human AUC", value: humanAUC, color: "#e74c3c" }, { name: "Gap", value: Math.max(0, modelAUC - humanAUC), color: "#9b59b6" }];
            const x = d3.scaleBand().range([0, width]).domain(data.map(d => d.name)).padding(0.3);
            const y = d3.scaleLinear().domain([0.4, 1]).range([height, 0]);

            svg.append("g").call(d3.axisLeft(y).ticks(6, "%"));
            svg.append("text").attr("text-anchor", "end").attr("transform", "rotate(-90)").attr("y", -margin.left + 20).attr("x", -height / 2).text("AUC Score").style("font-size", "12px");
            svg.append("g").attr("transform", `translate(0,${height})`).call(d3.axisBottom(x)).selectAll("text").style("font-size", "11px");

            svg.selectAll(".bar").data(data).enter().append("rect").attr("class", "bar").attr("x", d => x(d.name)).attr("y", d => y(d.value)).attr("width", x.bandwidth()).attr("height", d => Math.max(0, height - y(d.value))).attr("fill", d => d.color)
                .on("mouseover", function(event, d) {
                    const tooltip = d3.select("body").append("div").attr("class", "tooltip").style("opacity", 0);
                    tooltip.transition().duration(200).style("opacity", .9);
                    tooltip.html(`<strong>${d.name}</strong><br/>Value: ${d.value.toFixed(2)}`).style("left", (event.pageX + 5) + "px").style("top", (event.pageY - 28) + "px");
                }).on("mouseout", function(d) { d3.selectAll(".tooltip").transition().duration(500).style("opacity", 0).remove(); });
            
            svg.append("line").attr("x1", 0).attr("y1", y(0.5)).attr("x2", width).attr("y2", y(0.5)).attr("stroke", "grey").attr("stroke-width", 1.5).attr("stroke-dasharray", "4,4");
            svg.append("text").attr("x", width - 5).attr("y", y(0.5) - 5).attr("text-anchor", "end").style("font-size", "10px").attr("fill", "grey").text("Chance (0.5)");
            svg.append("text").attr("x", width / 2).attr("y", 0 - (margin.top / 2) + 5).attr("text-anchor", "middle").style("font-size", "14px").style("font-weight", "bold").text("Discrimination Ability (AUC)");
        }

        const simulationItems = [
            { id: 1, name: "Refrigerator (annual kWh)", actual: 600, aiCorrectEstimate: 580, aiIncorrectEstimate: 300 },
            { id: 2, name: "Clothes Washer (per load, hot wash kWh)", actual: 2.0, aiCorrectEstimate: 2.2, aiIncorrectEstimate: 0.5 },
            { id: 3, name: "Gaming Console (annual kWh, 2hrs/day)", actual: 150, aiCorrectEstimate: 160, aiIncorrectEstimate: 400 },
            { id: 4, name: "LED Light Bulb (60W equiv, annual kWh, 3hrs/day)", actual: 10, aiCorrectEstimate: 9, aiIncorrectEstimate: 50 }
        ];
        let userEstimates = {};
        let currentSimulationItemIndex = 0;
        let simulationResults = [];

        const aiExplanationsTemplates = {
            confident: { correct: "Based on extensive data, the typical energy consumption is definitely around {estimate} kWh. This figure is well-established.", incorrect: "It's quite clear that the energy usage is approximately {estimate} kWh. Standard calculations consistently point to this value." },
            hedgedMedium: { correct: "The estimated energy consumption is likely in the range of {estimate} kWh. This is a reasonable figure.", incorrect: "It seems the energy usage could be around {estimate} kWh. This is a plausible estimate." },
            hedgedLow: { correct: "I'm not entirely certain, but the energy consumption might be somewhere near {estimate} kWh. This is a tentative estimate.", incorrect: "It's difficult to say with high confidence, but perhaps the energy usage is in the ballpark of {estimate} kWh." }
        };

        function initializeProposedStudySimulation() {
            const estimatesContainer = document.getElementById('user-energy-estimates-input');
            if (!estimatesContainer || estimatesContainer.dataset.initialized === 'true' && document.getElementById('user-est-1')) return; // Prevent re-init if already done and inputs exist
            
            estimatesContainer.innerHTML = '';
            simulationItems.forEach(item => {
                estimatesContainer.innerHTML += `
                    <div class="mb-3">
                        <label for="user-est-${item.id}" class="block text-sm font-medium text-gray-700">${item.name}:</label>
                        <input type="number" id="user-est-${item.id}" name="user-est-${item.id}" class="mt-1 p-2 border border-gray-300 rounded-md shadow-sm focus:ring-indigo-500 focus:border-indigo-500 block w-full sm:text-sm" placeholder="Enter kWh">
                    </div>`;
            });
            estimatesContainer.dataset.initialized = 'true';


            document.getElementById('submit-user-estimates').onclick = startAIInteractionPhase;
            document.getElementById('next-item-simulation').onclick = nextSimulationItem;
            document.getElementById('choose-user-estimate').onclick = () => recordChoice('user');
            document.getElementById('choose-ai-estimate').onclick = () => recordChoice('ai');
            document.getElementById('sim-energy-knowledge').onchange = updateIndividualDifferencesFeedback;
            document.getElementById('sim-ai-literacy').onchange = updateIndividualDifferencesFeedback;
            updateIndividualDifferencesFeedback();
        }
        
        function updateIndividualDifferencesFeedback() {
            const energyKnowledgeEl = document.getElementById('sim-energy-knowledge');
            const aiLiteracyEl = document.getElementById('sim-ai-literacy');
            const feedbackEl = document.getElementById('individual-differences-feedback');
            if (!energyKnowledgeEl || !aiLiteracyEl || !feedbackEl) return;

            const energyKnowledge = energyKnowledgeEl.value;
            const aiLiteracy = aiLiteracyEl.value;
            
            let feedbackText = "Individual differences can significantly impact how users interact with AI advice:<ul>";
            if (energyKnowledge === 'high') feedbackText += "<li class='mt-1'><strong>High Energy Knowledge:</strong> May better identify inaccurate AI estimates and rely more on own knowledge.</li>";
            else if (energyKnowledge === 'low') feedbackText += "<li class='mt-1'><strong>Low Energy Knowledge:</strong> May be more susceptible to AI's estimates and benefit more from clear uncertainty cues.</li>";
            else feedbackText += "<li class='mt-1'><strong>Medium Energy Knowledge:</strong> Mixed reliance, capable of some independent assessment.</li>";
            if (aiLiteracy === 'high') feedbackText += "<li class='mt-1'><strong>High AI Literacy:</strong> Better understands AI confidence/uncertainty, leading to more nuanced trust.</li>";
            else if (aiLiteracy === 'low') feedbackText += "<li class='mt-1'><strong>Low AI Literacy:</strong> May take AI statements at face value, potentially over-trusting or confused by uncertainty.</li>";
            else feedbackText += "<li class='mt-1'><strong>Medium AI Literacy:</strong> Basic understanding, but may not grasp all nuances of LLM confidence.</li>";
            feedbackText += "</ul>";
            feedbackEl.innerHTML = feedbackText;
        }

        function startAIInteractionPhase() {
            userEstimates = {};
            let allFilled = true;
            simulationItems.forEach(item => {
                const inputEl = document.getElementById(`user-est-${item.id}`);
                if (!inputEl || inputEl.value === '' || isNaN(parseFloat(inputEl.value))) allFilled = false;
                userEstimates[item.id] = parseFloat(inputEl.value) || 0;
            });

            if (!allFilled) { alert("Please provide an estimate for all items."); return; }

            document.getElementById('user-energy-estimates-input').classList.add('hidden');
            document.getElementById('submit-user-estimates').classList.add('hidden');
            document.getElementById('simulation-intro-text').classList.add('hidden');
            document.getElementById('ai-interaction-area').classList.remove('hidden');
            currentSimulationItemIndex = 0;
            simulationResults = [];
            loadSimulationItem();
        }

        function loadSimulationItem() {
            if (currentSimulationItemIndex >= simulationItems.length) { endSimulation(); return; }
            const item = simulationItems[currentSimulationItemIndex];
            document.getElementById('current-item-text').textContent = `Current Item: ${item.name}`;
            document.getElementById('user-prior-estimate').textContent = userEstimates[item.id];
            
            const isAICorrect = currentSimulationItemIndex < 2; 
            const aiEstimate = isAICorrect ? item.aiCorrectEstimate : item.aiIncorrectEstimate;
            document.getElementById('ai-estimate-text').textContent = aiEstimate;

            const linguisticTone = document.getElementById('ai-linguistic-tone').value;
            const explanationTemplate = aiExplanationsTemplates[linguisticTone][isAICorrect ? 'correct' : 'incorrect'];
            const explanationHTML = explanationTemplate.replace('{estimate}', `<strong>${aiEstimate}</strong>`);
            
            const explanationDiv = document.getElementById('ai-explanation-text');
            explanationDiv.innerHTML = explanationHTML;
            explanationDiv.className = 'ai-explanation my-3';
            explanationDiv.classList.add(linguisticTone);

            document.getElementById('user-confidence-ai-slider').value = 50;
            document.getElementById('user-confidence-ai-value').textContent = '50';
            document.getElementById('choose-user-estimate').disabled = false;
            document.getElementById('choose-ai-estimate').disabled = false;
            document.getElementById('next-item-simulation').classList.add('hidden');
            document.getElementById('user-confidence-ai-slider').oninput = function() { document.getElementById('user-confidence-ai-value').textContent = this.value; };
        }

        function recordChoice(choice) {
            const item = simulationItems[currentSimulationItemIndex];
            const userConfidenceInAI = parseInt(document.getElementById('user-confidence-ai-slider').value);
            const aiEstimate = parseFloat(document.getElementById('ai-estimate-text').textContent);
            const linguisticTone = document.getElementById('ai-linguistic-tone').value;
            const isAICorrect = currentSimulationItemIndex < 2;

            simulationResults.push({
                itemName: item.name, userInitialEstimate: userEstimates[item.id], aiEstimate: aiEstimate,
                aiCorrectness: isAICorrect ? 'Correct' : 'Incorrect', linguisticTone: linguisticTone,
                userConfidenceInAI: userConfidenceInAI, chosenEstimate: choice,
                actualCorrectnessOfChoice: (choice === 'ai' && isAICorrect) || (choice === 'user' && Math.abs(userEstimates[item.id] - item.actual) < Math.abs(aiEstimate - item.actual)) ? 'Correct Choice' : 'Incorrect Choice'
            });

            document.getElementById('choose-user-estimate').disabled = true;
            document.getElementById('choose-ai-estimate').disabled = true;
            document.getElementById('next-item-simulation').classList.remove('hidden');
        }

        function nextSimulationItem() { currentSimulationItemIndex++; loadSimulationItem(); }

        function endSimulation() {
            document.getElementById('ai-interaction-area').classList.add('hidden');
            const resultsDiv = document.getElementById('simulation-results-summary');
            resultsDiv.innerHTML = '<h4>Simulation Completed! Results:</h4>';
            let tableHTML = '<table class="min-w-full divide-y divide-gray-200 text-xs md:text-sm"><thead><tr><th class="px-2 py-1">Item</th><th class="px-2 py-1">Your Est.</th><th class="px-2 py-1">AI Est.</th><th class="px-2 py-1">AI Tone</th><th class="px-2 py-1">AI Actual</th><th class="px-2 py-1">Your Conf. in AI</th><th class="px-2 py-1">Choice</th><th class="px-2 py-1">Choice Correct?</th></tr></thead><tbody>';
            simulationResults.forEach(res => {
                tableHTML += `<tr><td class="border px-2 py-1">${res.itemName}</td><td class="border px-2 py-1">${res.userInitialEstimate}</td><td class="border px-2 py-1">${res.aiEstimate}</td><td class="border px-2 py-1">${res.linguisticTone}</td><td class="border px-2 py-1">${res.aiCorrectness}</td><td class="border px-2 py-1">${res.userConfidenceInAI}%</td><td class="border px-2 py-1">${res.chosenEstimate.toUpperCase()}</td><td class="border px-2 py-1 ${res.actualCorrectnessOfChoice === 'Correct Choice' ? 'text-green-600' : 'text-red-600'}">${res.actualCorrectnessOfChoice}</td></tr>`;
            });
            tableHTML += '</tbody></table>';
            resultsDiv.innerHTML += tableHTML;

            const avgConfidence = simulationResults.reduce((sum, r) => sum + r.userConfidenceInAI, 0) / simulationResults.length || 0;
            const relianceOnAI = (simulationResults.filter(r => r.chosenEstimate === 'ai').length / simulationResults.length || 0) * 100;
            const correctChoices = (simulationResults.filter(r => r.actualCorrectnessOfChoice === 'Correct Choice').length / simulationResults.length || 0) * 100;

            resultsDiv.innerHTML += `<div class="mt-4 p-3 bg-gray-100 rounded"><p><strong>Average Confidence in AI:</strong> ${avgConfidence.toFixed(1)}%</p><p><strong>Reliance on AI:</strong> ${relianceOnAI.toFixed(1)}%</p><p><strong>Overall Correct Choices:</strong> ${correctChoices.toFixed(1)}%</p></div>`;
            resultsDiv.innerHTML += `<p class="mt-4"><button onclick="resetSimulation()" class="button-primary">Run Simulation Again</button></p>`;
        }
        
        function resetSimulation() {
            document.getElementById('user-energy-estimates-input').classList.remove('hidden');
            document.getElementById('submit-user-estimates').classList.remove('hidden');
            document.getElementById('simulation-intro-text').classList.remove('hidden'); // Show intro text again
            document.getElementById('ai-interaction-area').classList.add('hidden');
            document.getElementById('simulation-results-summary').innerHTML = '<p>Complete the interaction to see a summary of your choices and confidence ratings.</p>';
             // Clear old inputs if any and re-initialize
            const estimatesContainer = document.getElementById('user-energy-estimates-input');
            estimatesContainer.dataset.initialized = 'false'; // Allow re-initialization
            initializeProposedStudySimulation();
        }

        document.addEventListener('DOMContentLoaded', function() {
            openTab(null, 'overview');
        });

    </script>
</body>
</html>
