<!DOCTYPE html><html lang="en"><head><meta http-equiv="Content-Security-Policy" content="default-src 'self' 'unsafe-inline' 'unsafe-eval' data: blob: https://cdnjs.cloudflare.com https://cdn.jsdelivr.net https://code.jquery.com https://unpkg.com https://d3js.org https://threejs.org https://cdn.plot.ly https://stackpath.bootstrapcdn.com https://maps.googleapis.com https://cdn.tailwindcss.com https://ajax.googleapis.com https://kit.fontawesome.com https://cdn.datatables.net https://maxcdn.bootstrapcdn.com https://code.highcharts.com https://tako-static-assets-production.s3.amazonaws.com https://www.youtube.com https://fonts.googleapis.com https://fonts.gstatic.com https://pfst.cf2.poecdn.net https://puc.poecdn.net https://i.imgur.com https://wikimedia.org https://*.icons8.com https://*.giphy.com https://picsum.photos https://images.unsplash.com; frame-src 'self' https://www.youtube.com https://trytako.com; child-src 'self'; manifest-src 'self'; worker-src 'self'; upgrade-insecure-requests; block-all-mixed-content;">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Calibrating Trust in an LLM Energy Advisor</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        :root {
            --primary-color: #3498db;
            --secondary-color: #2ecc71;
            --accent-color: #e74c3c;
            --text-color: #2c3e50;
            --light-bg: #f8f9fa;
            --tab-active: #3498db;
            --tab-inactive: #78a8c7;
            --border-color: #ddd;
            --gradient-start: #2c3e50;
            --gradient-end: #4ca1af;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            max-width: 1200px;
            margin: 0 auto;
            padding: 0;
            background-color: var(--light-bg);
        }
        
        .header {
            background: linear-gradient(135deg, var(--gradient-start) 0%, var(--gradient-end) 100%);
            color: white;
            padding: 30px 20px;
            border-radius: 0 0 10px 10px;
            margin-bottom: 20px;
            text-align: center;
        }
        
        .header h1 {
            margin: 0;
            font-size: 32px;
        }
        
        .header p {
            margin: 10px 0 0;
            font-size: 18px;
            opacity: 0.9;
        }
        
        .tab-container {
            margin: 0 20px;
        }
        
        .tabs {
            display: flex;
            margin-bottom: 0;
            padding-left: 0;
            list-style: none;
            border-bottom: 2px solid var(--border-color);
            overflow-x: auto;
            white-space: nowrap;
        }
        
        .tabs li {
            margin-right: 5px;
        }
        
        .tabs li a {
            display: block;
            padding: 12px 20px;
            text-decoration: none;
            color: var(--tab-inactive);
            border-radius: 5px 5px 0 0;
            font-weight: 500;
            transition: all 0.2s ease;
        }
        
        .tabs li a:hover {
            background-color: rgba(52, 152, 219, 0.1);
            color: var(--tab-active);
        }
        
        .tabs li a.active {
            color: var(--tab-active);
            border-bottom: 3px solid var(--tab-active);
            background-color: white;
        }
        
        .tab-content {
            background-color: white;
            border-radius: 0 0 10px 10px;
            padding: 25px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
            min-height: 500px;
        }
        
        .tab-pane {
            display: none;
        }
        
        .tab-pane.active {
            display: block;
        }
        
        h2, h3 {
            color: var(--text-color);
        }
        
        h2 {
            font-size: 24px;
            margin-top: 0;
            padding-bottom: 10px;
            border-bottom: 1px solid #f0f0f0;
        }
        
        .section {
            margin-bottom: 30px;
        }
        
        .chart-container {
            position: relative;
            height: 400px;
            width: 100%;
            margin: 25px 0;
        }
        
        .flex-container {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            margin-bottom: 20px;
        }
        
        .flex-item {
            flex: 1;
            min-width: 300px;
        }
        
        .control-panel {
            background-color: #f5f7fa;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 20px;
            border: 1px solid var(--border-color);
        }
        
        .slider-container {
            margin-bottom: 20px;
        }
        
        .slider-container label {
            display: block;
            margin-bottom: 8px;
            font-weight: 600;
            color: var(--text-color);
        }
        
        input[type="range"] {
            width: 100%;
            height: 8px;
            background: #d7dcdf;
            border-radius: 5px;
            outline: none;
        }
        
        input[type="range"]::-webkit-slider-thumb {
            appearance: none;
            width: 16px;
            height: 16px;
            border-radius: 50%;
            background: var(--primary-color);
            cursor: pointer;
        }
        
        .slider-value {
            display: inline-block;
            min-width: 45px;
            text-align: right;
            margin-left: 10px;
            font-weight: bold;
        }
        
        .btn {
            background-color: var(--primary-color);
            color: white;
            border: none;
            padding: 10px 18px;
            border-radius: 6px;
            cursor: pointer;
            transition: all 0.3s;
            font-weight: 600;
            font-size: 14px;
        }
        
        .btn:hover {
            background-color: #2980b9;
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        
        .simulation-area {
            background-color: #fff;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 25px;
            margin-top: 20px;
        }
        
        .appliance-info {
            display: flex;
            align-items: center;
            margin-bottom: 20px;
        }
        
        .appliance-icon {
            width: 60px;
            height: 60px;
            margin-right: 20px;
            background-color: #f1f8fe;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 28px;
        }
        
        .advice-box {
            border-left: 4px solid var(--primary-color);
            padding: 15px 20px;
            background-color: #f1f8fe;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }
        
        .advice-high {
            border-left-color: #2ecc71;
            background-color: #eafaf1;
        }
        
        .advice-medium {
            border-left-color: #f39c12;
            background-color: #fef9e7;
        }
        
        .advice-low {
            border-left-color: #e74c3c;
            background-color: #fdedeb;
        }
        
        .user-controls {
            display: flex;
            flex-direction: column;
            gap: 15px;
            margin-top: 25px;
        }
        
        .confidence-slider-container {
            margin: 15px 0;
        }
        
        .confidence-labels {
            display: flex;
            justify-content: space-between;
            margin-top: 5px;
            font-size: 13px;
            color: #666;
        }
        
        .decision-btns {
            display: flex;
            gap: 15px;
            margin-top: 10px;
        }
        
        .decision-btns .btn {
            flex: 1;
        }
        
        .user-btn {
            background-color: #3498db;
        }
        
        .ai-btn {
            background-color: #9b59b6;
        }
        
        .results-display {
            margin-top: 25px;
            padding: 20px;
            background-color: #f8f9fa;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            font-size: 14px;
        }
        
        table, th, td {
            border: 1px solid #e0e0e0;
        }
        
        th, td {
            padding: 10px 15px;
            text-align: left;
        }
        
        th {
            background-color: #f2f6f9;
            font-weight: 600;
        }
        
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }
        
        .correct {
            color: #2ecc71;
            font-weight: bold;
        }
        
        .incorrect {
            color: #e74c3c;
        }
        
        .research-question {
            background-color: #eef5fd;
            border-left: 4px solid var(--primary-color);
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }
        
        .methods-diagram {
            max-width: 100%;
            height: auto;
            margin: 20px 0;
        }
        
        .insight-box {
            background-color: #f9f9f9;
            border-left: 4px solid #9b59b6;
            padding: 15px;
            margin: 15px 0;
            border-radius: 0 8px 8px 0;
        }
        
        .paper-citation {
            background-color: #f7f9fc;
            border: 1px solid #e1e8ed;
            border-radius: 8px;
            padding: 15px;
            margin-bottom: 20px;
        }
        
        .paper-citation h3 {
            margin-top: 0;
            font-size: 18px;
        }
        
        .paper-citation p {
            margin-bottom: 10px;
        }
        
        .paper-citation .authors {
            font-style: italic;
            color: #5a6268;
        }
        
        .measures-table {
            width: 100%;
            margin-bottom: 20px;
        }
        
        .measures-table th {
            background-color: #eef5fd;
        }
        
        .hypothesis {
            margin-bottom: 20px;
            padding-bottom: 15px;
            border-bottom: 1px solid #eee;
        }
        
        .hypothesis h4 {
            color: var(--primary-color);
            margin-bottom: 8px;
        }
        
        .variables-list {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin: 20px 0;
        }
        
        .variable-pill {
            background-color: #e8f4fc;
            border-radius: 15px;
            padding: 5px 12px;
            font-size: 14px;
            display: inline-block;
        }
        
        .iv-pill {
            background-color: #e1f5fe;
            border: 1px solid #81d4fa;
        }
        
        .dv-pill {
            background-color: #e8f5e9;
            border: 1px solid #a5d6a7;
        }
        
        .mod-pill {
            background-color: #fff3e0;
            border: 1px solid #ffcc80;
        }
        
        .phase-box {
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 15px;
            margin-bottom: 20px;
        }
        
        .phase-box h4 {
            color: #495057;
            margin-top: 0;
            margin-bottom: 15px;
            border-bottom: 1px solid #e9ecef;
            padding-bottom: 10px;
        }
        
        .prediction-chart {
            background-color: white;
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 25px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.05);
        }
        
        .prediction-chart h3 {
            margin-top: 0;
            color: #495057;
            font-size: 20px;
            padding-bottom: 10px;
            border-bottom: 1px solid #f0f0f0;
        }
        
        .prediction-chart p {
            color: #666;
            margin-bottom: 20px;
        }
        
        .hypothesis-summary {
            font-weight: bold;
            color: #2c3e50;
            margin-top: 15px;
        }
        
        .llm-diagram {
            max-width: 100%;
            height: auto;
            margin: 30px 0;
            overflow-x: auto;
        }
        
        @media (max-width: 768px) {
            .tabs li a {
                padding: 10px 15px;
                font-size: 14px;
            }
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Calibrating Trust in an LLM Energy Advisor</h1>
        <p>The Impact of Linguistic Uncertainty on User Trust and Decision Making</p>
    </div>
    
    <div class="tab-container">
        <ul class="tabs">
            <li><a href="#overview" class="active">Overview</a></li>
            <li><a href="#energy">Energy Perception</a></li>
            <li><a href="#ai-confidence">AI Confidence</a></li>
            <li><a href="#llm-advisor">LLM Energy Advisor</a></li>
            <li><a href="#experimental-design">Experimental Design</a></li>
            <li><a href="#individual-differences">Individual Differences</a></li>
            <li><a href="#predictions">Predictions</a></li>
        </ul>
        
        <div class="tab-content">
            <!-- Overview Tab -->
            <div id="overview" class="tab-pane active">
                <h2>Understanding Human-AI Trust in Energy Decision Making</h2>
                
                <div class="research-question">
                    <p><strong>Research Question:</strong> "Given that people often misunderstand energy consumption (Attari), how will they react when an LLM energy advisor gives them advice, especially when we deliberately make that AI express different levels of linguistic certainty (inspired by Steyvers)?"</p>
                </div>
                
                <div class="flex-container">
                    <div class="flex-item">
                        <h3>The Problem: Energy Misconceptions</h3>
                        <p>Attari et al. (2010) demonstrated that people systematically:</p>
                        <ul>
                            <li><strong style="color:#e74c3c">Underestimate</strong> the energy used by high-consumption appliances (like air conditioners)</li>
                            <li><strong style="color:#2ecc71">Overestimate</strong> the energy used by low-consumption ones (like light bulbs)</li>
                            <li>Focus on less effective <strong>"curtailment"</strong> actions rather than more impactful <strong>"efficiency"</strong> improvements</li>
                        </ul>
                    </div>
                    <div class="flex-item">
                        <h3>The Challenge: AI Trust Calibration</h3>
                        <p>Steyvers et al. (2025) showed that:</p>
                        <ul>
                            <li>A <strong>"calibration gap"</strong> exists between an LLM's internal confidence and human perception</li>
                            <li>Users typically <strong style="color:#e74c3c">overestimate</strong> LLM accuracy, especially with longer or confident-sounding explanations</li>
                            <li>Using <strong>appropriate hedging language</strong> when confidence is low helps narrow this gap</li>
                        </ul>
                    </div>
                </div>
                
                <div class="section">
                    <h3>Research Questions</h3>
                    
                    <div class="hypothesis">
                        <h4>RQ1: Reliance &amp; Accuracy Discrimination</h4>
                        <p>How does the LLM's linguistic expression of uncertainty influence participants' reliance on its energy estimates when forced to choose between the AI's estimate and their own prior estimate? Specifically, does appropriately hedged language from the AI improve participants' ability to discriminate between correct and incorrect AI advice?</p>
                    </div>
                    
                    <div class="hypothesis">
                        <h4>RQ2: Subjective Confidence &amp; Trust</h4>
                        <p>How does the LLM's linguistic expression of uncertainty affect participants' subjective confidence in the AI's specific estimate for each item, and their overall post-task trust in the LLM advisor?</p>
                    </div>
                    
                    <div class="hypothesis">
                        <h4>RQ3: Calibration of Confidence</h4>
                        <p>Does aligning the LLM's linguistic expression of uncertainty with its (simulated) internal confidence lead to better calibration between participants' confidence in the AI's estimates and the actual accuracy of those estimates?</p>
                    </div>
                    
                    <div class="hypothesis">
                        <h4>RQ4: Role of Prior Knowledge &amp; Individual Differences</h4>
                        <p>How do participants' baseline energy knowledge accuracy and individual differences (e.g., AI literacy, numeracy) moderate the effects of the LLM's expressed uncertainty on their reliance, confidence, discrimination, and calibration?</p>
                    </div>
                </div>
                
                <div class="section">
                    <h3>Hypotheses</h3>
                    
                    <div class="hypothesis">
                        <h4>H1: Appropriate Reliance &amp; Discrimination</h4>
                        <p><strong>H1a (Reliance Shift):</strong> Participants will be less likely to choose the AI's estimate when it is expressed with hedged language compared to confident language.</p>
                        <p><strong>H1b (Improved Discrimination):</strong> The reduction in reliance due to hedged language will be significantly greater when the AI's advice is incorrect than when it is correct, manifesting as improved discrimination when uncertainty cues are calibrated.</p>
                    </div>
                    
                    <div class="hypothesis">
                        <h4>H2: Subjective Confidence &amp; Trust</h4>
                        <p><strong>H2a (Item-Level Confidence):</strong> Participants will report lower confidence in the AI's estimate when its explanation is hedged compared to when it is confident.</p>
                        <p><strong>H2b (Overall Trust):</strong> Overall post-task trust might be more nuanced: it could be lower with consistent hedging, or, ideally, better calibrated in the calibrated uncertainty condition.</p>
                    </div>
                    
                    <div class="hypothesis">
                        <h4>H3: Improved Metacognitive Calibration</h4>
                        <p>Participants exposed to AI estimates with calibrated linguistic uncertainty cues will exhibit better metacognitive calibration compared to participants exposed only to confidently expressed AI estimates.</p>
                    </div>
                    
                    <div class="hypothesis">
                        <h4>H4: Moderation by Individual Differences</h4>
                        <p><strong>H4a (Prior Knowledge):</strong> Participants with more accurate baseline energy knowledge will demonstrate more appropriate reliance and better calibration overall. The benefits of calibrated AI uncertainty cues might be more pronounced for those with initially poorer knowledge.</p>
                        <p><strong>H4b (AI Literacy/Numeracy):</strong> Participants with higher AI literacy and numeracy will be more sensitive to the AI's linguistic uncertainty cues, leading to more pronounced effects on appropriate reliance and better calibration.</p>
                    </div>
                </div>
            </div>
            
            <!-- Energy Perception Tab -->
            <div id="energy" class="tab-pane">
                <h2>Energy Perception: Systematic Misconceptions</h2>
                
                <div class="paper-citation">
                    <h3>Attari et al. (2010): Public Perceptions of Energy Consumption and Savings</h3>
                    <p class="authors">Shahzeen Z. Attari, Michael L. DeKay, Cliff I. Davidson, Wändi Bruine de Bruin</p>
                    <p>This seminal study demonstrated that Americans systematically underestimate energy use for high-energy activities and overestimate it for low-energy activities, creating a "compression" effect in energy perceptions.</p>
                    <p>The research also found that people tend to focus on curtailment (turning off, using less) rather than efficiency improvements (using better technology), despite the latter often having greater energy-saving potential.</p>
                </div>
                
                <div class="chart-container">
                    <canvas id="energyPerceptionChart"></canvas>
                </div>
                
                <div class="control-panel">
                    <div class="slider-container">
                        <label for="underestimationFactor">High-Energy Appliance Underestimation Factor: <span id="underestimationValue" class="slider-value">2.8</span>x</label>
                        <input type="range" id="underestimationFactor" min="1" max="5" step="0.1" value="2.8">
                    </div>
                    <div class="slider-container">
                        <label for="overestimationFactor">Low-Energy Appliance Overestimation Factor: <span id="overestimationValue" class="slider-value">1.6</span>x</label>
                        <input type="range" id="overestimationFactor" min="1" max="3" step="0.1" value="1.6">
                    </div>
                    <button id="updateEnergyChart" class="btn">Update Perception Model</button>
                </div>
                
                <div class="section">
                    <h3>Key Findings from Attari et al. (2010)</h3>
                    
                    <div class="flex-container">
                        <div class="flex-item">
                            <h4>Perception Biases</h4>
                            <ul>
                                <li>Underestimation for high-energy items (~2.8x factor)</li>
                                <li>Overestimation for low-energy items (~1.6x factor)</li>
                                <li>Creates a "compressed" mental model of energy differences</li>
                                <li>People with higher numeracy showed somewhat more accurate perceptions</li>
                            </ul>
                        </div>
                        <div class="flex-item">
                            <h4>Energy Saving Misconceptions</h4>
                            <ul>
                                <li>People focus on curtailment actions (e.g., turning off lights)</li>
                                <li>Efficiency improvements (e.g., better appliances) are undervalued</li>
                                <li>This misalignment can lead to suboptimal conservation choices</li>
                                <li>Policy implications: need for better communication of relative impacts</li>
                            </ul>
                        </div>
                    </div>
                    
                    <div class="insight-box">
                        <p><strong>Implications for our study:</strong> If people already have systematically inaccurate beliefs about energy consumption, how will this affect their interactions with an AI energy advisor? Will they trust incorrect AI advice that aligns with their own misconceptions? Can linguistic uncertainty cues help overcome these biases?</p>
                    </div>
                </div>
            </div>
            
            <!-- AI Confidence Tab -->
            <div id="ai-confidence" class="tab-pane">
                <h2>AI Confidence: The Calibration Gap</h2>
                
                <div class="paper-citation">
                    <h3>Steyvers et al. (2025): Calibrating Trust in Large Language Models</h3>
                    <p class="authors">Mark Steyvers, Ananya Kumar, Pang Wei Koh, Pulkit Agrawal et al.</p>
                    <p>This forward-looking work identified a significant "calibration gap" between an LLM's internal confidence (based on token probabilities) and human perception of its accuracy. The research demonstrated that manipulating the linguistic expression of uncertainty can help narrow this gap.</p>
                </div>
                
                <!-- LLM Internal State Diagram -->
                <div class="section">
                    <h3>LLM Internal State and Trust Calibration Process</h3>
                    <p>Understanding how LLM confidence translates to linguistic outputs and influences human trust:</p>
                    
                    <div id="llmInternalDiagram" class="llm-diagram">
                        <!-- SVG diagram will be injected here by JavaScript -->
                    </div>
                </div>
                
                <div class="chart-container">
                    <canvas id="calibrationGapChart"></canvas>
                </div>
                
                <div class="control-panel">
                    <div class="slider-container">
                        <label for="calibrationGapSize">Calibration Gap Size: <span id="calibrationGapValue" class="slider-value">20</span>%</label>
                        <input type="range" id="calibrationGapSize" min="0" max="50" step="5" value="20">
                    </div>
                    <div class="slider-container">
                        <label for="hedgingEffect">Effect of Hedging Language: <span id="hedgingEffectValue" class="slider-value">15</span>% reduction</label>
                        <input type="range" id="hedgingEffect" min="0" max="30" step="5" value="15">
                    </div>
                    <button id="updateCalibrationChart" class="btn">Update Calibration Model</button>
                </div>
                
                <div class="section">
                    <h3>Key Findings from Steyvers et al. (2025)</h3>
                    
                    <div class="flex-container">
                        <div class="flex-item">
                            <h4>The Calibration Gap</h4>
                            <ul>
                                <li>LLMs have internal confidence indicators (token probabilities)</li>
                                <li>Users systematically overestimate LLM accuracy</li>
                                <li>This gap is larger with confident-sounding or longer explanations</li>
                                <li>Result: inappropriate reliance on AI advice</li>
                            </ul>
                        </div>
                        <div class="flex-item">
                            <h4>Bridging the Gap with Linguistic Uncertainty</h4>
                            <ul>
                                <li>Hedging language can signal LLM uncertainty</li>
                                <li>Examples: "I think," "probably," "might be," etc.</li>
                                <li>When calibrated with actual LLM confidence, hedging improves trust calibration</li>
                                <li>Users can better discriminate between reliable and unreliable advice</li>
                            </ul>
                        </div>
                    </div>
                    
                    <div class="insight-box">
                        <p><strong>Implications for our study:</strong> By applying the linguistic uncertainty techniques from Steyvers et al. to the domain of energy advice, we can test whether these approaches help users overcome both the AI overconfidence problem and their own energy misconceptions.</p>
                    </div>
                </div>
                
                <div class="section">
                    <h3>Examples of Linguistic Uncertainty Expressions</h3>
                    
                    <table>
                        <tbody><tr>
                            <th>Confidence Level</th>
                            <th>Linguistic Expression</th>
                            <th>Example</th>
                        </tr>
                        <tr>
                            <td><strong>High Confidence</strong></td>
                            <td>Assertive, direct statements</td>
                            <td>"A central AC unit uses 3500 units of energy per hour. This is because AC systems require significant power to cool air through the compression cycle."</td>
                        </tr>
                        <tr>
                            <td><strong>Medium Confidence</strong></td>
                            <td>Qualified assertions, some hedges</td>
                            <td>"A central AC unit typically uses around 3500 units of energy per hour. This is generally because most residential AC systems need substantial power for cooling, though exact usage can vary by model."</td>
                        </tr>
                        <tr>
                            <td><strong>Low Confidence</strong></td>
                            <td>Multiple hedges, alternatives presented</td>
                            <td>"I think a central AC might use approximately 3500 units of energy per hour, but I'm not entirely certain. Different AC systems can vary considerably, possibly ranging from 2000 to 5000 units depending on size, efficiency, and operating conditions."</td>
                        </tr>
                    </tbody></table>
                </div>
            </div>
            
            <!-- LLM Energy Advisor Tab -->
            <div id="llm-advisor" class="tab-pane">
                <h2>Interactive Simulation: LLM Energy Advisor</h2>
                <p>This simulation allows you to experience how different expressions of uncertainty affect trust in an LLM's energy advice:</p>
                
                <div class="control-panel">
                    <div class="slider-container">
                        <label for="uncertaintyLevel">LLM Uncertainty Expression Style:</label>
                        <select id="uncertaintyLevel" style="width: 100%; padding: 8px; border-radius: 5px; border: 1px solid #ccc;">
                            <option value="0">Always Confident Language</option>
                            <option value="1" selected="">Calibrated Uncertainty (Hedge When Uncertain)</option>
                        </select>
                    </div>
                    <div class="slider-container">
                        <label for="userKnowledgeLevel">User Energy Knowledge Level: <span id="userKnowledgeValue" class="slider-value">Medium</span></label>
                        <input type="range" id="userKnowledgeLevel" min="0" max="2" step="1" value="1">
                    </div>
                    <div class="slider-container">
                        <label for="aiAccuracy">AI Advice Accuracy: <span id="aiAccuracyValue" class="slider-value">70</span>%</label>
                        <input type="range" id="aiAccuracy" min="40" max="100" step="5" value="70">
                    </div>
                    <button id="startSimulation" class="btn">Start New Simulation</button>
                </div>
                
                <div class="simulation-area" id="simulationArea">
                    <p>Configure the parameters above and click "Start New Simulation" to begin the experiment.</p>
                </div>
                
                <div class="results-display" id="simulationResults" style="display: none;">
                    <h3>Simulation Results</h3>
                    <p>Complete the simulation to see your results.</p>
                </div>
                
                <div class="section">
                    <h3>Simulation Measures</h3>
                    
                    <div class="flex-container">
                        <div class="flex-item">
                            <h4>Primary Measures</h4>
                            <ul>
                                <li><strong>Appropriate Reliance:</strong> Choosing AI advice when it's correct, rejecting when incorrect</li>
                                <li><strong>Confidence Calibration:</strong> Alignment between confidence ratings and actual correctness</li>
                            </ul>
                        </div>
                        <div class="flex-item">
                            <h4>Scoring</h4>
                            <ul>
                                <li><strong>Brier Score/ECE:</strong> Discrepancy between confidence and accuracy (lower is better)</li>
                                <li><strong>AUC:</strong> Area under the ROC curve for discrimination (higher is better)</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            
            <!-- Experimental Design Tab -->
            <div id="experimental-design" class="tab-pane">
                <h2>Experimental Design Methodology</h2>
                
                <div id="methodsDiagram" class="methods-diagram">
                    <!-- SVG diagram will be rendered here -->
                </div>
                
                <div class="section">
                    <h3>Study Design: Mixed-Factorial</h3>
                    
                    <div class="variables-list">
                        <div class="variable-pill iv-pill">IV1: AI Uncertainty Communication Style (Between-Subjects)</div>
                        <div class="variable-pill iv-pill">IV2: AI Advice Correctness (Within-Subjects)</div>
                        <div class="variable-pill dv-pill">DV1: User Reliance (Choice)</div>
                        <div class="variable-pill dv-pill">DV2: Confidence in AI Estimate</div>
                        <div class="variable-pill dv-pill">DV3: Trust Calibration</div>
                        <div class="variable-pill mod-pill">Moderator: User Energy Knowledge</div>
                        <div class="variable-pill mod-pill">Moderator: AI Literacy</div>
                        <div class="variable-pill mod-pill">Moderator: Numeracy</div>
                    </div>
                    
                    <div class="phase-box">
                        <h4>Phase 1: Baseline Energy Knowledge Assessment</h4>
                        <p>Participants estimate energy consumption of various appliances, establishing both their baseline knowledge and their own estimation biases.</p>
                        <p><strong>Task:</strong> "Relative to a 100-watt incandescent light bulb which uses 100 units of energy per hour, how many units of energy do you think the following device typically uses in one hour?"</p>
                        <p><strong>Measure:</strong> Accuracy of energy estimations compared to ground truth values</p>
                    </div>
                    
                    <div class="phase-box">
                        <h4>Phase 2: AI Energy Advisor Interaction</h4>
                        <p>Participants interact with an LLM providing energy estimates, with expressions of uncertainty manipulated based on condition.</p>
                        <p><strong>Between-Subjects Factor (Primary Manipulation):</strong></p>
                        <ul>
                            <li><strong>Calibrated Uncertainty:</strong> AI uses confident language for correct advice and hedged language for incorrect advice (with hedging level potentially based on error magnitude)</li>
                            <li><strong>Always Confident:</strong> AI always uses confident language for both correct and incorrect advice</li>
                        </ul>
                        <p><strong>Within-Subjects Factors:</strong></p>
                        <ul>
                            <li><strong>Item:</strong> 10-12 energy appliances/scenarios (counterbalanced order)</li>
                            <li><strong>AI Advice Correctness:</strong> (Correct, Incorrect - pre-determined and counterbalanced)</li>
                        </ul>
                        <p><strong>Measures:</strong></p>
                        <ul>
                            <li>Confidence in AI's estimate (0-100% slider)</li>
                            <li>Forced choice between own estimate and AI's estimate</li>
                        </ul>
                    </div>
                </div>
                
                <div class="section">
                    <h3>Procedure</h3>
                    <ol>
                        <li>Consent, Demographics, Individual Difference Measures (AI Literacy, Numeracy, Propensity to Trust Automation)</li>
                        <li>Phase 1: Baseline Energy Estimation task for all items</li>
                        <li>Instructions for Phase 2: Introduce AI advisor, explain tasks</li>
                        <li>Phase 2: Trial-by-trial interaction for each item:
                            <ul>
                                <li>Present AI estimate + explanation (tone per condition)</li>
                                <li>Participant rates confidence in AI estimate</li>
                                <li>Participant's Phase 1 estimate shown; participant makes forced choice</li>
                            </ul>
                        </li>
                        <li>Post-Task Measures: Overall Trust Scale, Manipulation Check, Open-ended feedback</li>
                        <li>Debriefing (no trial-by-trial feedback during the main task)</li>
                    </ol>
                </div>
                
                <div class="section">
                    <h3>Key Analyses</h3>
                    <ul>
                        <li>Mixed-effects models for reliance choices and confidence ratings</li>
                        <li>Calculation of Brier/ECE for calibration (H3)</li>
                        <li>Calculation of AUC for discrimination (H1b)</li>
                        <li>Moderation analyses for H4 (including interaction effects with individual differences)</li>
                    </ul>
                </div>
            </div>
            
            <!-- Individual Differences Tab -->
            <div id="individual-differences" class="tab-pane">
                <h2>Individual Differences as Moderators</h2>
                
                <div class="section">
                    <h3>Key Individual Difference Measures</h3>
                    
                    <div class="flex-container">
                        <div class="flex-item">
                            <h4>Energy Domain Knowledge</h4>
                            <p>Measured through Phase 1 energy estimation accuracy:</p>
                            <ul>
                                <li>Accuracy of initial energy estimates</li>
                                <li>Pattern of errors (underestimation vs. overestimation)</li>
                                <li>Self-reported familiarity with energy concepts</li>
                            </ul>
                            <p><strong>Hypothesis H4a:</strong> Higher energy knowledge will lead to better discrimination between correct and incorrect AI advice. The benefits of calibrated AI uncertainty cues may be more pronounced for those with initially poorer knowledge.</p>
                        </div>
                        <div class="flex-item">
                            <h4>AI Literacy</h4>
                            <p>Measured using established scales:</p>
                            <ul>
                                <li>AICOS-SV (AI Conceptual and Operational Skills)</li>
                                <li>Previous experience with LLMs</li>
                                <li>Understanding of AI capabilities and limitations</li>
                            </ul>
                            <p><strong>Hypothesis H4b:</strong> Higher AI literacy will lead to better interpretation of linguistic uncertainty cues and more appropriate reliance adjustments.</p>
                        </div>
                    </div>
                    
                    <div class="flex-container">
                        <div class="flex-item">
                            <h4>Numeracy</h4>
                            <p>Measured using Berlin Numeracy Test:</p>
                            <ul>
                                <li>Understanding of numerical concepts</li>
                                <li>Ability to interpret probabilities</li>
                                <li>Comfort with quantitative reasoning</li>
                            </ul>
                            <p><strong>Hypothesis H4b (continued):</strong> Higher numeracy will improve calibration between confidence ratings and actual accuracy, and enhance sensitivity to linguistic uncertainty cues.</p>
                        </div>
                        <div class="flex-item">
                            <h4>Trust Propensity</h4>
                            <p>Measured using Jian-Bisantz-Drury Trust Scale:</p>
                            <ul>
                                <li>General tendency to trust automation</li>
                                <li>Skepticism toward AI systems</li>
                                <li>Complacency potential</li>
                            </ul>
                            <p><strong>Additional Moderator:</strong> High trust propensity may moderate the effect of uncertainty expressions, potentially requiring stronger uncertainty signals.</p>
                        </div>
                    </div>
                </div>
                
                <div class="chart-container">
                    <canvas id="knowledgeInteractionChart"></canvas>
                </div>
                
                <div class="insight-box">
                    <h4>Key Hypothesized Interactions:</h4>
                    <ul>
                        <li><strong>Energy Knowledge × Uncertainty Expression:</strong> Users with medium energy knowledge may benefit most from calibrated uncertainty, as high-knowledge users already have good discrimination and low-knowledge users may lack the basis to evaluate advice.</li>
                        <li><strong>AI Literacy × Uncertainty Expression:</strong> High AI literacy users will show stronger effects of uncertainty cues, with better discrimination between confident and hedged advice.</li>
                        <li><strong>Numeracy × Calibration:</strong> Higher numeracy will correlate with better trust calibration overall, with smaller calibration errors for all conditions.</li>
                    </ul>
                </div>
            </div>
            
            <!-- Predictions Tab -->
            <div id="predictions" class="tab-pane">
                <h2>Empirical Predictions</h2>
                <p>Based on our theoretical framework and hypotheses, we can make specific empirical predictions about the expected patterns of results.</p>
                
                <div class="prediction-chart">
                    <h3>H1: Impact of Linguistic Uncertainty on Reliance by AI Correctness</h3>
                    <p>We predict an interaction effect where hedged language will reduce reliance more for incorrect AI advice than for correct AI advice, improving discrimination.</p>
                    <div class="chart-container">
                        <canvas id="relianceInteractionChart"></canvas>
                    </div>
                    <p class="hypothesis-summary">H1a &amp; H1b: Hedged language reduces reliance overall, but this effect is stronger for incorrect advice, improving discrimination.</p>
                </div>
                
                <div class="prediction-chart">
                    <h3>H2: Impact of Linguistic Uncertainty on User Confidence</h3>
                    <p>We predict that hedged language will reduce participants' confidence in the AI's estimates, with a larger effect when the AI is incorrect.</p>
                    <div class="chart-container">
                        <canvas id="confidenceChart"></canvas>
                    </div>
                    <p class="hypothesis-summary">H2a: Hedged language reduces confidence in AI estimates, especially for incorrect advice.</p>
                </div>
                
                <div class="prediction-chart">
                    <h3>H3: Impact of Linguistic Uncertainty on Trust Calibration</h3>
                    <p>We predict that calibrated linguistic uncertainty will improve metacognitive calibration across user knowledge levels.</p>
                    <div class="chart-container">
                        <canvas id="calibrationErrorChart"></canvas>
                    </div>
                    <p class="hypothesis-summary">H3: Calibrated uncertainty expressions lead to better calibration (lower ECE) than consistently confident language.</p>
                </div>
                
                <div class="prediction-chart">
                    <h3>H4a: Moderation by Energy Domain Knowledge</h3>
                    <p>We predict that the benefits of calibrated uncertainty will vary depending on users' baseline energy knowledge.</p>
                    <div class="chart-container">
                        <canvas id="knowledgeModerationChart"></canvas>
                    </div>
                    <p class="hypothesis-summary">H4a: Medium-knowledge users may show the largest improvements from calibrated uncertainty cues.</p>
                </div>
                
                <div class="prediction-chart">
                    <h3>H4b: Moderation by AI Literacy</h3>
                    <p>We predict that higher AI literacy will enhance the effect of uncertainty expressions on appropriate reliance.</p>
                    <div class="chart-container">
                        <canvas id="aiLiteracyModerationChart"></canvas>
                    </div>
                    <p class="hypothesis-summary">H4b: High AI literacy users will show stronger discrimination improvements with calibrated uncertainty.</p>
                </div>
            </div>
        </div>
    </div>
    
    <script>
        // Tab functionality
        document.querySelectorAll('.tabs a').forEach(tab => {
            tab.addEventListener('click', function(e) {
                e.preventDefault();
                
                // Remove active class from all tabs and panes
                document.querySelectorAll('.tabs a').forEach(t => t.classList.remove('active'));
                document.querySelectorAll('.tab-pane').forEach(p => p.classList.remove('active'));
                
                // Add active class to clicked tab and corresponding pane
                this.classList.add('active');
                document.querySelector(this.getAttribute('href')).classList.add('active');
            });
        });
        
        // Data for energy perception chart
        const appliances = [
            { name: 'Central AC', actualEnergy: 3500, perceivedEnergyFactor: 1/2.8 },
            { name: 'Water Heater', actualEnergy: 2500, perceivedEnergyFactor: 1/2.5 },
            { name: 'Refrigerator', actualEnergy: 1800, perceivedEnergyFactor: 1/2.2 },
            { name: 'Clothes Dryer', actualEnergy: 1500, perceivedEnergyFactor: 1/2.0 },
            { name: 'TV', actualEnergy: 500, perceivedEnergyFactor: 1.2 },
            { name: 'Computer', actualEnergy: 300, perceivedEnergyFactor: 1.4 },
            { name: 'Stereo', actualEnergy: 100, perceivedEnergyFactor: 1.6 },
            { name: 'Light Bulb', actualEnergy: 60, perceivedEnergyFactor: 1.8 }
        ];
        
        // Generate data for calibration gap chart
        function generateCalibrationData(gapSize, hedgingEffect) {
            const confidences = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100];
            
            const perfectCalibration = confidences.map(conf => ({ x: conf, y: conf }));
            
            // Human perception with default (confident) LLM explanations
            const humanPerceptionDefault = confidences.map(conf => {
                // Apply gap - higher overestimation for mid-range confidence
                const overestimation = gapSize * Math.sin((conf / 100) * Math.PI);
                return { x: conf, y: Math.min(100, conf + overestimation) };
            });
            
            // Human perception with hedged LLM explanations
            const humanPerceptionHedged = confidences.map(conf => {
                const overestimation = Math.max(0, (gapSize - hedgingEffect) * Math.sin((conf / 100) * Math.PI));
                return { x: conf, y: Math.min(100, conf + overestimation) };
            });
            
            return {
                perfectCalibration,
                humanPerceptionDefault,
                humanPerceptionHedged
            };
        }
        
        // Generate data for energy perception chart
        function generateEnergyPerceptionData(underestimationFactor, overestimationFactor) {
            return appliances.map(appliance => {
                let factor;
                if (appliance.actualEnergy > 1000) {
                    factor = 1 / underestimationFactor; // Underestimation for high-energy
                } else {
                    factor = overestimationFactor; // Overestimation for low-energy
                }
                
                return {
                    name: appliance.name,
                    actualEnergy: appliance.actualEnergy,
                    perceivedEnergy: appliance.actualEnergy * factor
                };
            });
        }
        
        // Initialize charts
        let energyPerceptionChart, calibrationGapChart, knowledgeInteractionChart;
        let relianceInteractionChart, confidenceChart, calibrationErrorChart;
        let knowledgeModerationChart, aiLiteracyModerationChart;
        
        function initEnergyPerceptionChart() {
            const underestimationFactor = parseFloat(document.getElementById('underestimationFactor').value);
            const overestimationFactor = parseFloat(document.getElementById('overestimationFactor').value);
            
            const data = generateEnergyPerceptionData(underestimationFactor, overestimationFactor);
            
            const ctx = document.getElementById('energyPerceptionChart').getContext('2d');
            
            // Destroy existing chart if it exists
            if (energyPerceptionChart) {
                energyPerceptionChart.destroy();
            }
            
            // Create perfect accuracy line data points
            const maxEnergy = Math.max(...data.map(d => d.actualEnergy)) * 1.1;
            
            energyPerceptionChart = new Chart(ctx, {
                type: 'scatter',
                data: {
                    datasets: [
                        {
                            label: 'Perfect Accuracy Line',
                            data: [
                                { x: 0, y: 0 },
                                { x: maxEnergy, y: maxEnergy }
                            ],
                            borderColor: 'rgba(75, 192, 192, 0.8)',
                            backgroundColor: 'rgba(0, 0, 0, 0)',
                            borderDash: [5, 5],
                            borderWidth: 2,
                            pointRadius: 0,
                            showLine: true,
                            order: 1
                        },
                        {
                            label: 'Human Perception',
                            data: data.map(item => ({
                                x: item.actualEnergy,
                                y: item.perceivedEnergy
                            })),
                            backgroundColor: 'rgba(255, 99, 132, 0.7)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 1,
                            pointRadius: 8,
                            pointHoverRadius: 10,
                            order: 2
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: {
                            title: {
                                display: true,
                                text: 'Actual Energy (Units)',
                                font: {
                                    weight: 'bold'
                                }
                            },
                            min: 0,
                            max: maxEnergy
                        },
                        y: {
                            title: {
                                display: true,
                                text: 'Perceived Energy (Units)',
                                font: {
                                    weight: 'bold'
                                }
                            },
                            min: 0,
                            max: maxEnergy
                        }
                    },
                    plugins: {
                        title: {
                            display: true,
                            text: 'Energy Perception vs. Reality (Attari et al. 2010)',
                            font: {
                                size: 16,
                                weight: 'bold'
                            }
                        },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    const index = context.dataIndex;
                                    if (context.datasetIndex === 1) {
                                        const appliance = data[index];
                                        return `${appliance.name}: Actual: ${appliance.actualEnergy}, Perceived: ${Math.round(appliance.perceivedEnergy)}`;
                                    }
                                    return '';
                                }
                            }
                        },
                        legend: {
                            labels: {
                                usePointStyle: true
                            }
                        }
                    }
                }
            });
        }
        
        function initCalibrationGapChart() {
            const gapSize = parseInt(document.getElementById('calibrationGapSize').value);
            const hedgingEffect = parseInt(document.getElementById('hedgingEffect').value);
            
            const calibrationData = generateCalibrationData(gapSize, hedgingEffect);
            
            const ctx = document.getElementById('calibrationGapChart').getContext('2d');
            
            // Destroy existing chart if it exists
            if (calibrationGapChart) {
                calibrationGapChart.destroy();
            }
            
            calibrationGapChart = new Chart(ctx, {
                type: 'scatter',
                data: {
                    datasets: [
                        {
                            label: 'Perfect Calibration',
                            data: calibrationData.perfectCalibration,
                            borderColor: 'rgba(75, 192, 192, 0.8)',
                            backgroundColor: 'rgba(75, 192, 192, 0.1)',
                            borderDash: [5, 5],
                            borderWidth: 2,
                            pointRadius: 0,
                            showLine: true,
                            fill: false
                        },
                        {
                            label: 'Human Perception (Confident Language)',
                            data: calibrationData.humanPerceptionDefault,
                            borderColor: 'rgba(255, 99, 132, 0.8)',
                            backgroundColor: 'rgba(255, 99, 132, 0.1)',
                            borderWidth: 2,
                            pointRadius: 5,
                            showLine: true,
                            fill: false
                        },
                        {
                            label: 'Human Perception (Hedged Language)',
                            data: calibrationData.humanPerceptionHedged,
                            borderColor: 'rgba(54, 162, 235, 0.8)',
                            backgroundColor: 'rgba(54, 162, 235, 0.1)',
                            borderWidth: 2,
                            pointRadius: 5,
                            showLine: true,
                            fill: false
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: {
                            title: {
                                display: true,
                                text: 'Model Confidence (%)',
                                font: {
                                    weight: 'bold'
                                }
                            },
                            min: 0,
                            max: 100
                        },
                        y: {
                            title: {
                                display: true,
                                text: 'Human Perception of Accuracy (%)',
                                font: {
                                    weight: 'bold'
                                }
                            },
                            min: 0,
                            max: 100
                        }
                    },
                    plugins: {
                        title: {
                            display: true,
                            text: 'Calibration Gap: Model vs. Human Confidence (Steyvers et al. 2025)',
                            font: {
                                size: 16,
                                weight: 'bold'
                            }
                        },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    const conf = context.parsed.x;
                                    const perc = context.parsed.y;
                                    if (context.datasetIndex === 0) {
                                        return `Perfect Calibration: ${conf}%`;
                                    } else if (context.datasetIndex === 1) {
                                        return `Model Confidence: ${conf}%, Human Perception: ${perc.toFixed(1)}% (Confident Language)`;
                                    } else if (context.datasetIndex === 2) {
                                        return `Model Confidence: ${conf}%, Human Perception: ${perc.toFixed(1)}% (Hedged Language)`;
                                    }
                                }
                            }
                        },
                        legend: {
                            labels: {
                                usePointStyle: true
                            }
                        }
                    }
                }
            });
        }
        
        function initKnowledgeInteractionChart() {
            const ctx = document.getElementById('knowledgeInteractionChart').getContext('2d');
            
            // Destroy existing chart if it exists
            if (knowledgeInteractionChart) {
                knowledgeInteractionChart.destroy();
            }
            
            knowledgeInteractionChart = new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: ['Low Knowledge', 'Medium Knowledge', 'High Knowledge'],
                    datasets: [
                        {
                            label: 'Calibration Error (Confident Language)',
                            data: [0.35, 0.28, 0.15],
                            backgroundColor: 'rgba(255, 99, 132, 0.7)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 1
                        },
                        {
                            label: 'Calibration Error (Hedged Language)',
                            data: [0.25, 0.15, 0.10],
                            backgroundColor: 'rgba(54, 162, 235, 0.7)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 1
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            title: {
                                display: true,
                                text: 'Calibration Error (lower is better)',
                                font: {
                                    weight: 'bold'
                                }
                            },
                            min: 0,
                            max: 0.4
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'User Energy Knowledge Level',
                                font: {
                                    weight: 'bold'
                                }
                            }
                        }
                    },
                    plugins: {
                        title: {
                            display: true,
                            text: 'Impact of Linguistic Uncertainty on Trust Calibration by User Knowledge Level',
                            font: {
                                size: 16,
                                weight: 'bold'
                            }
                        },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    return `${context.dataset.label}: ${context.parsed.y.toFixed(2)}`;
                                }
                            }
                        }
                    }
                }
            });
        }
        
        // Initialize the prediction charts
        function initRelianceInteractionChart() {
            const ctx = document.getElementById('relianceInteractionChart').getContext('2d');
            
            // Destroy existing chart if it exists
            if (relianceInteractionChart) {
                relianceInteractionChart.destroy();
            }
            
            relianceInteractionChart = new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: ['Correct AI Advice', 'Incorrect AI Advice'],
                    datasets: [
                        {
                            label: 'Reliance Rate (Confident Language)',
                            data: [0.82, 0.68],
                            backgroundColor: 'rgba(255, 99, 132, 0.7)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 1
                        },
                        {
                            label: 'Reliance Rate (Hedged Language)',
                            data: [0.75, 0.40],
                            backgroundColor: 'rgba(54, 162, 235, 0.7)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 1
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            title: {
                                display: true,
                                text: 'Proportion of Trials Relying on AI Advice',
                                font: {
                                    weight: 'bold'
                                }
                            },
                            min: 0,
                            max: 1.0
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'AI Advice Correctness',
                                font: {
                                    weight: 'bold'
                                }
                            }
                        }
                    },
                    plugins: {
                        title: {
                            display: true,
                            text: 'Predicted Reliance Rate by Linguistic Style and AI Advice Correctness',
                            font: {
                                size: 16,
                                weight: 'bold'
                            }
                        },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    return `${context.dataset.label}: ${(context.parsed.y * 100).toFixed(1)}%`;
                                }
                            }
                        }
                    }
                }
            });
        }
        
        function initConfidenceChart() {
            const ctx = document.getElementById('confidenceChart').getContext('2d');
            
            // Destroy existing chart if it exists
            if (confidenceChart) {
                confidenceChart.destroy();
            }
            
            confidenceChart = new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: ['Correct AI Advice', 'Incorrect AI Advice'],
                    datasets: [
                        {
                            label: 'User Confidence (Confident Language)',
                            data: [78, 72],
                            backgroundColor: 'rgba(255, 99, 132, 0.7)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 1
                        },
                        {
                            label: 'User Confidence (Hedged Language)',
                            data: [65, 45],
                            backgroundColor: 'rgba(54, 162, 235, 0.7)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 1
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            title: {
                                display: true,
                                text: 'User Confidence in AI Advice (%)',
                                font: {
                                    weight: 'bold'
                                }
                            },
                            min: 0,
                            max: 100
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'AI Advice Correctness',
                                font: {
                                    weight: 'bold'
                                }
                            }
                        }
                    },
                    plugins: {
                        title: {
                            display: true,
                            text: 'Predicted User Confidence by Linguistic Style and AI Advice Correctness',
                            font: {
                                size: 16,
                                weight: 'bold'
                            }
                        },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    return `${context.dataset.label}: ${context.parsed.y}%`;
                                }
                            }
                        }
                    }
                }
            });
        }
        
        function initCalibrationErrorChart() {
            const ctx = document.getElementById('calibrationErrorChart').getContext('2d');
            
            // Destroy existing chart if it exists
            if (calibrationErrorChart) {
                calibrationErrorChart.destroy();
            }
            
            // This reproduces the chart shown in the image from the prompt
            calibrationErrorChart = new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: ['Low Knowledge', 'Medium Knowledge', 'High Knowledge'],
                    datasets: [
                        {
                            label: 'Calibration Error (Confident Language)',
                            data: [0.35, 0.28, 0.15],
                            backgroundColor: 'rgba(255, 99, 132, 0.7)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 1
                        },
                        {
                            label: 'Calibration Error (Hedged Language)',
                            data: [0.25, 0.15, 0.10],
                            backgroundColor: 'rgba(54, 162, 235, 0.7)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 1
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            title: {
                                display: true,
                                text: 'Calibration Error (lower is better)',
                                font: {
                                    weight: 'bold'
                                }
                            },
                            min: 0,
                            max: 0.4
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'User Energy Knowledge Level',
                                font: {
                                    weight: 'bold'
                                }
                            }
                        }
                    },
                    plugins: {
                        title: {
                            display: true,
                            text: 'Impact of Linguistic Uncertainty on Trust Calibration by User Knowledge Level',
                            font: {
                                size: 16,
                                weight: 'bold'
                            }
                        },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    return `${context.dataset.label}: ${context.parsed.y.toFixed(2)}`;
                                }
                            }
                        }
                    }
                }
            });
        }
        
        function initKnowledgeModerationChart() {
            const ctx = document.getElementById('knowledgeModerationChart').getContext('2d');
            
            // Destroy existing chart if it exists
            if (knowledgeModerationChart) {
                knowledgeModerationChart.destroy();
            }
            
            knowledgeModerationChart = new Chart(ctx, {
                type: 'line',
                data: {
                    labels: ['Low Knowledge', 'Medium Knowledge', 'High Knowledge'],
                    datasets: [
                        {
                            label: 'Discrimination (AUC) - Confident Language',
                            data: [0.55, 0.62, 0.75],
                            backgroundColor: 'rgba(255, 99, 132, 0.7)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 2,
                            tension: 0.1
                        },
                        {
                            label: 'Discrimination (AUC) - Hedged Language',
                            data: [0.58, 0.72, 0.82],
                            backgroundColor: 'rgba(54, 162, 235, 0.7)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 2,
                            tension: 0.1
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            title: {
                                display: true,
                                text: 'Discrimination (AUC, higher is better)',
                                font: {
                                    weight: 'bold'
                                }
                            },
                            min: 0.5,
                            max: 0.9
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'User Energy Knowledge Level',
                                font: {
                                    weight: 'bold'
                                }
                            }
                        }
                    },
                    plugins: {
                        title: {
                            display: true,
                            text: 'Predicted User Discrimination by Knowledge Level and Linguistic Style',
                            font: {
                                size: 16,
                                weight: 'bold'
                            }
                        },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    return `${context.dataset.label}: ${context.parsed.y.toFixed(2)}`;
                                }
                            }
                        }
                    }
                }
            });
        }
        
        function initAILiteracyModerationChart() {
            const ctx = document.getElementById('aiLiteracyModerationChart').getContext('2d');
            
            // Destroy existing chart if it exists
            if (aiLiteracyModerationChart) {
                aiLiteracyModerationChart.destroy();
            }
            
            aiLiteracyModerationChart = new Chart(ctx, {
                type: 'line',
                data: {
                    labels: ['Low AI Literacy', 'Medium AI Literacy', 'High AI Literacy'],
                    datasets: [
                        {
                            label: 'Reliance Discrimination Index - Confident Language',
                            data: [0.08, 0.14, 0.18],
                            backgroundColor: 'rgba(255, 99, 132, 0.7)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 2,
                            tension: 0.1
                        },
                        {
                            label: 'Reliance Discrimination Index - Hedged Language',
                            data: [0.12, 0.25, 0.42],
                            backgroundColor: 'rgba(54, 162, 235, 0.7)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 2,
                            tension: 0.1
                        }
                    ]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            title: {
                                display: true,
                                text: 'Reliance Discrimination Index (higher is better)',
                                font: {
                                    weight: 'bold'
                                }
                            },
                            min: 0,
                            max: 0.5
                        },
                        x: {
                            title: {
                                display: true,
                                text: 'AI Literacy Level',
                                font: {
                                    weight: 'bold'
                                }
                            }
                        }
                    },
                    plugins: {
                        title: {
                            display: true,
                            text: 'Predicted Effect of AI Literacy on Reliance Discrimination',
                            font: {
                                size: 16,
                                weight: 'bold'
                            }
                        },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    return `${context.dataset.label}: ${context.parsed.y.toFixed(2)}`;
                                }
                            }
                        }
                    }
                }
            });
        }
        
        // Create the methods diagram using SVG
        function createMethodsDiagram() {
            const diagramContainer = document.getElementById('methodsDiagram');
            
            // SVG diagram for the methods
            const svgContent = `
            <svg width="100%" height="500" viewBox="0 0 1000 500" xmlns="http://www.w3.org/2000/svg">
                <!-- Background -->
                <rect x="0" y="0" width="1000" height="500" fill="#f8f9fa" rx="10" ry="10"/>
                
                <!-- Title -->
                <text x="500" y="40" font-size="20" text-anchor="middle" font-weight="bold" fill="#2c3e50">Experimental Design: Trust Calibration in an LLM Energy Advisor</text>
                
                <!-- Phase 1: Baseline Energy Knowledge Assessment -->
                <rect x="50" y="80" width="280" height="180" fill="#e3f2fd" stroke="#2196f3" stroke-width="2" rx="10" ry="10"/>
                <text x="190" y="110" font-size="18" text-anchor="middle" font-weight="bold" fill="#2c3e50">Phase 1: Baseline Assessment</text>
                <text x="190" y="140" font-size="14" text-anchor="middle" fill="#2c3e50">Participants estimate energy</text>
                <text x="190" y="160" font-size="14" text-anchor="middle" fill="#2c3e50">consumption of 10-12 appliances</text>
                <text x="190" y="190" font-size="14" text-anchor="middle" fill="#2c3e50">Measure: Initial accuracy of</text>
                <text x="190" y="210" font-size="14" text-anchor="middle" fill="#2c3e50">participant energy knowledge</text>
                <text x="190" y="240" font-size="14" text-anchor="middle" font-style="italic" fill="#2c3e50">(Based on Attari et al. 2010)</text>
                
                <!-- Phase 2: AI Interaction -->
                <rect x="360" y="80" width="280" height="340" fill="#e8f5e9" stroke="#4caf50" stroke-width="2" rx="10" ry="10"/>
                <text x="500" y="110" font-size="18" text-anchor="middle" font-weight="bold" fill="#2c3e50">Phase 2: AI Interaction</text>
                <text x="500" y="140" font-size="14" text-anchor="middle" fill="#2c3e50">Participants see LLM energy advice</text>
                <text x="500" y="160" font-size="14" text-anchor="middle" fill="#2c3e50">with explanations that vary in:</text>
                
                <!-- Experimental conditions -->
                <rect x="390" y="180" width="220" height="100" fill="white" stroke="#4caf50" stroke-width="1" rx="5" ry="5"/>
                <text x="500" y="200" font-size="16" text-anchor="middle" font-weight="bold" fill="#2c3e50">Between-Subjects Factor</text>
                <text x="500" y="225" font-size="14" text-anchor="middle" fill="#2c3e50">• Calibrated Uncertainty</text>
                <text x="500" y="250" font-size="14" text-anchor="middle" fill="#2c3e50">• Always Confident</text>
                <text x="500" y="270" font-size="14" text-anchor="middle" font-style="italic" fill="#2c3e50">(Based on Steyvers et al. 2025)</text>
                
                <!-- Measures -->
                <rect x="390" y="290" width="220" height="110" fill="white" stroke="#4caf50" stroke-width="1" rx="5" ry="5"/>
                <text x="500" y="310" font-size="16" text-anchor="middle" font-weight="bold" fill="#2c3e50">Measures</text>
                <text x="500" y="335" font-size="14" text-anchor="middle" fill="#2c3e50">1. Confidence in AI estimate</text>
                <text x="500" y="360" font-size="14" text-anchor="middle" fill="#2c3e50">2. Choice: Own vs AI estimate</text>
                <text x="500" y="385" font-size="14" text-anchor="middle" fill="#2c3e50">3. Overall trust in AI advisor</text>
                
                <!-- Phase 3: Analysis -->
                <rect x="670" y="80" width="280" height="180" fill="#fff3e0" stroke="#ff9800" stroke-width="2" rx="10" ry="10"/>
                <text x="810" y="110" font-size="18" text-anchor="middle" font-weight="bold" fill="#2c3e50">Phase 3: Analysis</text>
                <text x="810" y="140" font-size="14" text-anchor="middle" fill="#2c3e50">Measure trust calibration:</text>
                <text x="810" y="165" font-size="14" text-anchor="middle" fill="#2c3e50">1. Confidence calibration (Brier/ECE)</text>
                <text x="810" y="190" font-size="14" text-anchor="middle" fill="#2c3e50">2. Appropriate reliance (AUC)</text>
                <text x="810" y="215" font-size="14" text-anchor="middle" fill="#2c3e50">3. Moderation analyses (H4)</text>
                <text x="810" y="240" font-size="14" text-anchor="middle" fill="#2c3e50">4. Mixed-effects modeling</text>
                
                <!-- Key Hypothesis -->
                <rect x="670" y="280" width="280" height="140" fill="#f3e5f5" stroke="#9c27b0" stroke-width="2" rx="10" ry="10"/>
                <text x="810" y="310" font-size="18" text-anchor="middle" font-weight="bold" fill="#2c3e50">Key Hypotheses</text>
                <text x="810" y="340" font-size="14" text-anchor="middle" fill="#2c3e50">Calibrated uncertainty will:</text>
                <text x="810" y="365" font-size="14" text-anchor="middle" fill="#2c3e50">1. Improve discrimination (H1)</text>
                <text x="810" y="390" font-size="14" text-anchor="middle" fill="#2c3e50">2. Improve trust calibration (H3)</text>
                
                <!-- Arrows -->
                <path d="M 330 170 L 360 170" stroke="#2c3e50" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
                <path d="M 640 170 L 670 170" stroke="#2c3e50" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
                
                <!-- Arrowhead marker -->
                <defs>
                    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                        <polygon points="0 0, 10 3.5, 0 7" fill="#2c3e50"/>
                    </marker>
                </defs>
            </svg>
            `;
            
            diagramContainer.innerHTML = svgContent;
        }
        
        // Create the LLM internal state diagram
        function createLLMInternalDiagram() {
            const diagramContainer = document.getElementById('llmInternalDiagram');
            
            // SVG diagram for the LLM internal state with improved layout
            const svgContent = `
            <svg width="100%" height="570" viewBox="0 0 900 570" xmlns="http://www.w3.org/2000/svg">
                <!-- Background -->
                <rect x="0" y="0" width="900" height="570" fill="#f8f9fa" rx="10" ry="10"/>
                
                <!-- Title -->
                <text x="450" y="30" font-size="18" text-anchor="middle" font-weight="bold" fill="#2c3e50">LLM Internal State and Trust Calibration Process</text>
                
                <!-- LLM Internal State -->
                <rect x="320" y="50" width="260" height="70" fill="#e3f2fd" stroke="#2196f3" stroke-width="2" rx="10" ry="10"/>
                <text x="450" y="80" font-size="16" text-anchor="middle" font-weight="bold" fill="#2c3e50">LLM INTERNAL STATE</text>
                <text x="450" y="100" font-size="14" text-anchor="middle" fill="#2c3e50">• Represented knowledge</text>
                <text x="450" y="120" font-size="14" text-anchor="middle" fill="#2c3e50">• *Internal* confidence</text>
                
                <!-- Arrow for read-out -->
                <path d="M 450 120 L 450 140" stroke="#2c3e50" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
                <text x="540" y="134" font-size="11" fill="#666666" font-style="italic">read-out of token likelihoods</text>
                
                <!-- Branches from Internal State -->
                <path d="M 450 140 L 200 170" stroke="#2c3e50" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
                <path d="M 450 140 L 700 170" stroke="#2c3e50" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
                
                <!-- Model Confidence (left branch) -->
                <rect x="80" y="170" width="240" height="70" fill="#e8f5e9" stroke="#4caf50" stroke-width="2" rx="10" ry="10"/>
                <text x="200" y="195" font-size="16" text-anchor="middle" font-weight="bold" fill="#2c3e50">MODEL CONFIDENCE</text>
                <text x="200" y="215" font-size="14" text-anchor="middle" fill="#2c3e50">• Numeric probability</text>
                <text x="200" y="235" font-size="14" text-anchor="middle" fill="#2c3e50">*researcher extracts*</text>
                
                <!-- LLM Output (right branch) -->
                <rect x="580" y="170" width="240" height="90" fill="#e8f5e9" stroke="#4caf50" stroke-width="2" rx="10" ry="10"/>
                <text x="700" y="195" font-size="16" text-anchor="middle" font-weight="bold" fill="#2c3e50">LLM OUTPUT</text>
                <text x="700" y="215" font-size="14" text-anchor="middle" fill="#2c3e50">• Numeric estimate</text>
                <text x="700" y="235" font-size="14" text-anchor="middle" fill="#2c3e50">• Explanation text</text>
                <text x="700" y="255" font-size="14" text-anchor="middle" fill="#2c3e50">• Linguistic hedging</text>
                
                <!-- Control arrow from Confidence to Output -->
                <path d="M 320 205" stroke="#2c3e50" stroke-width="2" fill="none" marker-end="url(#arrowhead)" d="M320,205 L450,205 L580,205"/>
                <text x="450" y="195" font-size="11" text-anchor="middle" fill="#666666" font-style="italic">controls hedging</text>
                
                <!-- Human Cognition -->
                <rect x="580" y="290" width="240" height="100" fill="#fff3e0" stroke="#ff9800" stroke-width="2" rx="10" ry="10"/>
                <text x="700" y="315" font-size="16" text-anchor="middle" font-weight="bold" fill="#2c3e50">HUMAN COGNITION</text>
                <text x="700" y="335" font-size="14" text-anchor="middle" fill="#2c3e50">• Interpret answer</text>
                <text x="700" y="355" font-size="14" text-anchor="middle" fill="#2c3e50">• Form perceived accuracy</text>
                <text x="700" y="375" font-size="14" text-anchor="middle" fill="#2c3e50">• Generate confidence</text>
                <text x="700" y="395" font-size="14" text-anchor="middle" fill="#2c3e50">• Make reliance choice</text>
                
                <!-- Arrow from Output to Human -->
                <path d="M 700 260 L 700 290" stroke="#2c3e50" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
                <text x="750" y="280" font-size="11" text-anchor="middle" fill="#666666" font-style="italic">presented</text>
                
                <!-- Ground Truth Dataset -->
                <rect x="80" y="290" width="240" height="60" fill="#f3e5f5" stroke="#9c27b0" stroke-width="2" rx="10" ry="10"/>
                <text x="200" y="315" font-size="16" text-anchor="middle" font-weight="bold" fill="#2c3e50">GROUND-TRUTH DATASET</text>
                <text x="200" y="335" font-size="14" text-anchor="middle" fill="#2c3e50">• Actual energy values</text>
                
                <!-- Human Response Data -->
                <rect x="580" y="420" width="240" height="70" fill="#f3e5f5" stroke="#9c27b0" stroke-width="2" rx="10" ry="10"/>
                <text x="700" y="445" font-size="16" text-anchor="middle" font-weight="bold" fill="#2c3e50">HUMAN RESPONSE DATA</text>
                <text x="700" y="465" font-size="14" text-anchor="middle" fill="#2c3e50">• Human confidence (0-100)</text>
                <text x="700" y="485" font-size="14" text-anchor="middle" fill="#2c3e50">• Reliance decision</text>
                
                <!-- Arrow from Human to Response Data -->
                <path d="M 700 390 L 700 420" stroke="#2c3e50" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
                <text x="810" y="410" font-size="11" text-anchor="middle" fill="#666666" font-style="italic">confidence rating & choice recorded</text>
                
                <!-- Research Metrics -->
                <rect x="260" y="410" width="300" height="120" fill="#fce4ec" stroke="#e91e63" stroke-width="2" rx="10" ry="10"/>
                <text x="410" y="435" font-size="16" text-anchor="middle" font-weight="bold" fill="#2c3e50">RESEARCH METRICS</text>
                <text x="410" y="455" font-size="14" text-anchor="middle" fill="#2c3e50">• Calibration Gap = |HumanConf - ModelConf|</text>
                <text x="410" y="475" font-size="14" text-anchor="middle" fill="#2c3e50">• Expected Calibration Error (ECE)</text>
                <text x="410" y="495" font-size="14" text-anchor="middle" fill="#2c3e50">• Brier score (accuracy-weighted error)</text>
                <text x="410" y="515" font-size="14" text-anchor="middle" fill="#2c3e50">• AUC (discrimination)</text>
                <text x="410" y="535" font-size="14" text-anchor="middle" fill="#2c3e50">• Appropriate-reliance index</text>
                
                <!-- Arrows into Research Metrics -->
                <path d="M 200 350 L 320 410" stroke="#2c3e50" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
                <text x="250" y="385" font-size="11" text-anchor="middle" fill="#666666" font-style="italic">supplies</text>
                
                <path d="M 200 240 L 300 410" stroke="#2c3e50" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
                <text x="210" y="340" font-size="11" text-anchor="middle" fill="#666666" font-style="italic">supplies</text>
                
                <path d="M 580 455 L 560 455" stroke="#2c3e50" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
                <text x="570" y="435" font-size="11" text-anchor="middle" fill="#666666" font-style="italic">supplies</text>
                
                <!-- Decision Quality & Outcomes -->
                <rect x="260" y="540" width="300" height="25" fill="#e8eaf6" stroke="#3f51b5" stroke-width="2" rx="10" ry="10"/>
                <text x="410" y="557" font-size="14" text-anchor="middle" font-weight="bold" fill="#2c3e50">DECISION QUALITY & OUTCOMES</text>
                
                <!-- Arrow to Outcomes -->
                <path d="M 410 532 L 410 540" stroke="#2c3e50" stroke-width="2" fill="none" marker-end="url(#arrowhead)"/>
                <text x="430" y="536" font-size="11" text-anchor="middle" fill="#666666" font-style="italic">informs</text>
                
                <!-- Arrowhead marker -->
                <defs>
                    <marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto">
                        <polygon points="0 0, 10 3.5, 0 7" fill="#2c3e50"/>
                    </marker>
                </defs>
            </svg>
            `;
            
            diagramContainer.innerHTML = svgContent;
        }
        
        // The appliance data for simulation
        const simulationAppliances = [
            { name: 'Central Air Conditioner', actualEnergy: 3500, icon: '❄️' },
            { name: 'Water Heater', actualEnergy: 2500, icon: '🔥' },
            { name: 'Refrigerator', actualEnergy: 1800, icon: '🧊' },
            { name: 'Clothes Dryer', actualEnergy: 1500, icon: '👕' },
            { name: 'Television', actualEnergy: 500, icon: '📺' },
            { name: 'Computer', actualEnergy: 300, icon: '💻' },
            { name: 'Stereo System', actualEnergy: 100, icon: '🔊' },
            { name: 'LED Light Bulb', actualEnergy: 60, icon: '💡' }
        ];
        
        // Advice confidence levels and corresponding language templates
        const confidenceLanguage = {
            high: {
                prefix: "Based on typical usage patterns, a {appliance} definitely uses about",
                suffix: "units of energy per hour. This is because {appliance}s typically operate at {power} watts, which converts directly to this energy consumption rate."
            },
            medium: {
                prefix: "I estimate that a {appliance} probably uses approximately",
                suffix: "units of energy per hour. This is based on typical {appliance}s that tend to operate around {power} watts, though there can be some variation depending on the model and usage patterns."
            },
            low: {
                prefix: "My best guess is that a {appliance} might use roughly",
                suffix: "units of energy per hour, but I'm not entirely certain. Different models of {appliance}s can vary quite a bit, possibly ranging from {powerLow} to {powerHigh} watts depending on efficiency and features. You might want to check the specific model information."
            }
        };
        
        // Variable to keep track of simulation state
        let simulationState = {
            currentStep: 0,
            maxSteps: 5,
            appliances: [],
            userEstimates: {},
            aiEstimates: {},
            userChoices: [],
            userConfidence: [],
            correctChoices: 0,
            results: {
                calibrationError: 0,
                appropriateReliance: 0
            }
        };
        
        // Function to start a new simulation
        function startNewSimulation() {
            // Reset simulation state
            simulationState = {
                currentStep: 0,
                maxSteps: 5,
                appliances: [],
                userEstimates: {},
                aiEstimates: {},
                userChoices: [],
                userConfidence: [],
                correctChoices: 0,
                results: {
                    calibrationError: 0,
                    appropriateReliance: 0
                }
            };
            
            // Get parameter values
            const uncertaintyLevel = parseInt(document.getElementById('uncertaintyLevel').value);
            const userKnowledgeLevel = parseInt(document.getElementById('userKnowledgeLevel').value);
            const aiAccuracy = parseInt(document.getElementById('aiAccuracy').value);
            
            // Select 5 random appliances for the simulation
            simulationState.appliances = [...simulationAppliances]
                .sort(() => 0.5 - Math.random())
                .slice(0, simulationState.maxSteps);
            
            // Generate user estimates based on knowledge level
            simulationState.appliances.forEach(appliance => {
                let errorFactor;
                if (appliance.actualEnergy > 1000) {
                    // For high-energy appliances
                    errorFactor = userKnowledgeLevel === 0 ? 0.3 : (userKnowledgeLevel === 1 ? 0.5 : 0.8);
                } else {
                    // For low-energy appliances
                    errorFactor = userKnowledgeLevel === 0 ? 3 : (userKnowledgeLevel === 1 ? 2 : 1.2);
                }
                
                // Add some noise to make it more realistic
                const noise = 0.9 + Math.random() * 0.2; // 0.9 - 1.1
                simulationState.userEstimates[appliance.name] = Math.round(appliance.actualEnergy * errorFactor * noise);
            });
            
            // Generate AI estimates based on accuracy
            simulationState.appliances.forEach(appliance => {
                // Determine if this estimate will be correct or not
                const isCorrect = Math.random() * 100 < aiAccuracy;
                
                if (isCorrect) {
                    simulationState.aiEstimates[appliance.name] = {
                        value: appliance.actualEnergy,
                        confidence: 'high',
                        isCorrect: true
                    };
                } else {
                    // Generate an incorrect estimate
                    let errorDirection;
                    if (appliance.actualEnergy > 1000) {
                        errorDirection = -1; // Underestimate high-energy appliances
                    } else {
                        errorDirection = 1; // Overestimate low-energy appliances
                    }
                    
                    const errorFactor = 0.5 + Math.random() * 1.5; // Error factor between 0.5x and 2x
                    const aiEstimate = Math.round(appliance.actualEnergy * (1 + errorDirection * errorFactor));
                    
                    // Assign confidence based on how wrong it is and uncertainty level setting
                    let confidence;
                    const errorPercentage = Math.abs(aiEstimate - appliance.actualEnergy) / appliance.actualEnergy;
                    
                    if (uncertaintyLevel === 0) {
                        // Always confident regardless of correctness
                        confidence = 'high';
                    } else {
                        // Calibrated uncertainty - hedging proportional to error
                        if (errorPercentage < 0.3) {
                            confidence = 'medium';
                        } else {
                            confidence = 'low';
                        }
                    }
                    
                    simulationState.aiEstimates[appliance.name] = {
                        value: aiEstimate,
                        confidence: confidence,
                        isCorrect: false
                    };
                }
            });
            
            // Hide results area until simulation is complete
            document.getElementById('simulationResults').style.display = 'none';
            
            // Start the first step
            showSimulationStep();
        }
        
        // Function to show the current simulation step
        function showSimulationStep() {
            if (simulationState.currentStep >= simulationState.maxSteps) {
                // Simulation is complete, show results
                showSimulationResults();
                return;
            }
            
            const appliance = simulationState.appliances[simulationState.currentStep];
            const userEstimate = simulationState.userEstimates[appliance.name];
            const aiEstimate = simulationState.aiEstimates[appliance.name];
            
            // Create the simulation step UI
            const simulationArea = document.getElementById('simulationArea');
            simulationArea.innerHTML = `
                <h3>Step ${simulationState.currentStep + 1} of ${simulationState.maxSteps}: Energy Estimation</h3>
                
                <div class="appliance-info">
                    <div class="appliance-icon">${appliance.icon}</div>
                    <h4>${appliance.name}</h4>
                </div>
                
                <p>In the previous phase, you estimated that a ${appliance.name} uses <strong>${userEstimate} units</strong> of energy per hour.</p>
                
                <div class="advice-box advice-${aiEstimate.confidence}">
                    <p><strong>AI Energy Advisor:</strong></p>
                    <p>
                        ${getAdviceText(appliance, aiEstimate)}
                    </p>
                </div>
                
                <div class="user-controls">
                    <p><strong>How confident are you that the AI's estimate is correct?</strong></p>
                    <div class="confidence-slider-container">
                        <input type="range" id="confidenceSlider" min="0" max="100" value="50" style="width: 100%;">
                        <div class="confidence-labels">
                            <span>Not confident at all (0%)</span>
                            <span id="confidenceValue">50%</span>
                            <span>Extremely confident (100%)</span>
                        </div>
                    </div>
                    
                    <p style="margin-top: 20px;"><strong>Whose estimate do you think is more accurate?</strong></p>
                    <div class="decision-btns">
                        <button id="chooseUser" class="btn user-btn">My Estimate (${userEstimate} units)</button>
                        <button id="chooseAI" class="btn ai-btn">AI's Estimate (${aiEstimate.value} units)</button>
                    </div>
                </div>
            `;
            
            // Add event listeners
            document.getElementById('confidenceSlider').addEventListener('input', function() {
                document.getElementById('confidenceValue').textContent = this.value + '%';
            });
            
            document.getElementById('chooseUser').addEventListener('click', function() {
                const confidenceValue = parseInt(document.getElementById('confidenceSlider').value);
                recordUserChoice('user', confidenceValue);
            });
            
            document.getElementById('chooseAI').addEventListener('click', function() {
                const confidenceValue = parseInt(document.getElementById('confidenceSlider').value);
                recordUserChoice('ai', confidenceValue);
            });
        }
        
        // Function to get AI advice text based on confidence and appliance
        function getAdviceText(appliance, aiEstimate) {
            const language = confidenceLanguage[aiEstimate.confidence];
            
            // Calculate approximate power in watts (for the explanation)
            const power = Math.round(aiEstimate.value);
            const powerLow = Math.round(power * 0.7);
            const powerHigh = Math.round(power * 1.3);
            
            let prefix = language.prefix.replace('{appliance}', appliance.name.toLowerCase());
            let suffix = language.suffix
                .replace('{appliance}', appliance.name.toLowerCase())
                .replace('{power}', power)
                .replace('{powerLow}', powerLow)
                .replace('{powerHigh}', powerHigh);
                
            return `${prefix} <strong>${aiEstimate.value}</strong> ${suffix}`;
        }
        
        // Function to record user's choice and move to next step
        function recordUserChoice(choice, confidenceValue) {
            const appliance = simulationState.appliances[simulationState.currentStep];
            const aiEstimate = simulationState.aiEstimates[appliance.name];
            const userEstimate = simulationState.userEstimates[appliance.name];
            
            // Determine if the user's choice was correct
            const aiError = Math.abs(aiEstimate.value - appliance.actualEnergy);
            const userError = Math.abs(userEstimate - appliance.actualEnergy);
            const correctChoice = aiError < userError ? 'ai' : 'user';
            
            // Record the choice and confidence
            simulationState.userChoices.push({
                step: simulationState.currentStep,
                appliance: appliance.name,
                choice: choice,
                correctChoice: correctChoice,
                userEstimate: userEstimate,
                aiEstimate: aiEstimate.value,
                aiConfidence: aiEstimate.confidence,
                actualEnergy: appliance.actualEnergy,
                isCorrectChoice: choice === correctChoice
            });
            
            simulationState.userConfidence.push(confidenceValue);
            
            if (choice === correctChoice) {
                simulationState.correctChoices++;
            }
            
            // Move to the next step
            simulationState.currentStep++;
            showSimulationStep();
        }
        
        // Function to show the simulation results
        function showSimulationResults() {
            // Calculate results metrics
            calculateSimulationResults();
            
            const resultsArea = document.getElementById('simulationResults');
            resultsArea.style.display = 'block';
            
            // Create a summary table
            let tableHTML = `
                <h3>Simulation Results</h3>
                <p>You correctly identified the more accurate estimate ${simulationState.correctChoices} out of ${simulationState.maxSteps} times (${Math.round(simulationState.correctChoices/simulationState.maxSteps*100)}% accuracy).</p>
                
                <h4>Decision Summary:</h4>
                <table>
                    <tr>
                        <th>Appliance</th>
                        <th>Your Estimate</th>
                        <th>AI Estimate</th>
                        <th>Actual Value</th>
                        <th>AI Certainty</th>
                        <th>Your Choice</th>
                        <th>Correct?</th>
                    </tr>
            `;
            
            simulationState.userChoices.forEach(choice => {
                tableHTML += `
                    <tr>
                        <td>${choice.appliance}</td>
                        <td>${choice.userEstimate}</td>
                        <td>${choice.aiEstimate}</td>
                        <td>${choice.actualEnergy}</td>
                        <td>${choice.aiConfidence.charAt(0).toUpperCase() + choice.aiConfidence.slice(1)}</td>
                        <td>${choice.choice === 'user' ? 'Your Estimate' : 'AI Estimate'}</td>
                        <td class="${choice.isCorrectChoice ? 'correct' : 'incorrect'}">${choice.isCorrectChoice ? '✓' : '✗'}</td>
                    </tr>
                `;
            });
            
            tableHTML += `</table>`;
            
            // Add calibration analysis
            tableHTML += `
                <h4>Trust Calibration Analysis:</h4>
                <p>Calibration Error: ${simulationState.results.calibrationError.toFixed(2)} (lower is better)</p>
                <p>Appropriate Reliance: ${simulationState.results.appropriateReliance.toFixed(2)} (higher is better)</p>
                
                <div class="insight-box">
                    <h4>Key Insights:</h4>
                    <ul>
                        <li>When the AI expressed ${getAIUncertaintyLevelText()}, you relied on its advice ${getRelianceRateText()}.</li>
                        <li>Your confidence in the AI's estimates was ${getConfidenceCalibrationText()}.</li>
                        <li>Your energy knowledge level (${getUserKnowledgeLevelText()}) appeared to ${getKnowledgeImpactText()}.</li>
                    </ul>
                </div>
                
                <button id="newSimulation" class="btn" style="margin-top: 15px;">Start New Simulation</button>
            `;
            
            resultsArea.innerHTML = tableHTML;
            
            // Add event listener for new simulation button
            document.getElementById('newSimulation').addEventListener('click', function() {
                startNewSimulation();
            });
        }
        
        // Helper functions for generating the results text
        function getAIUncertaintyLevelText() {
            const level = parseInt(document.getElementById('uncertaintyLevel').value);
            return level === 0 ? "high confidence" : "calibrated uncertainty";
        }
        
        function getRelianceRateText() {
            const aiChoiceCount = simulationState.userChoices.filter(c => c.choice === 'ai').length;
            const rate = aiChoiceCount / simulationState.maxSteps;
            
            if (rate > 0.8) return "very frequently";
            if (rate > 0.6) return "frequently";
            if (rate > 0.4) return "moderately";
            if (rate > 0.2) return "occasionally";
            return "rarely";
        }
        
        function getConfidenceCalibrationText() {
            if (simulationState.results.calibrationError < 0.15) return "well-calibrated";
            if (simulationState.results.calibrationError < 0.25) return "moderately calibrated";
            return "poorly calibrated";
        }
        
        function getUserKnowledgeLevelText() {
            const level = parseInt(document.getElementById('userKnowledgeLevel').value);
            return level === 0 ? "low" : (level === 1 ? "medium" : "high");
        }
        
        function getKnowledgeImpactText() {
            const correctRate = simulationState.correctChoices / simulationState.maxSteps;
            if (correctRate > 0.8) return "significantly improve your decision-making";
            if (correctRate > 0.6) return "help you make better decisions";
            if (correctRate > 0.4) return "somewhat influence your decisions";
            return "not strongly impact your decision quality";
        }
        
        // Calculate result metrics for the simulation
        function calculateSimulationResults() {
            // Calculate calibration error (simplified Brier score)
            let squaredErrorSum = 0;
            
            simulationState.userChoices.forEach((choice, index) => {
                const confidence = simulationState.userConfidence[index] / 100;
                const outcome = choice.isCorrectChoice ? 1 : 0;
                squaredErrorSum += (confidence - outcome) ** 2;
            });
            
            simulationState.results.calibrationError = squaredErrorSum / simulationState.maxSteps;
            
            // Calculate appropriate reliance (discrimination index)
            const aiCorrectChoices = simulationState.userChoices.filter(c => {
                const aiIsCorrect = Math.abs(c.aiEstimate - c.actualEnergy) < Math.abs(c.userEstimate - c.actualEnergy);
                return (aiIsCorrect && c.choice === 'ai') || (!aiIsCorrect && c.choice === 'user');
            });
            
            simulationState.results.appropriateReliance = aiCorrectChoices.length / simulationState.maxSteps;
        }
        
        // Event listeners for the sliders
        document.getElementById('underestimationFactor').addEventListener('input', function() {
            document.getElementById('underestimationValue').textContent = this.value;
        });
        
        document.getElementById('overestimationFactor').addEventListener('input', function() {
            document.getElementById('overestimationValue').textContent = this.value;
        });
        
        document.getElementById('calibrationGapSize').addEventListener('input', function() {
            document.getElementById('calibrationGapValue').textContent = this.value;
        });
        
        document.getElementById('hedgingEffect').addEventListener('input', function() {
            document.getElementById('hedgingEffectValue').textContent = this.value;
        });
        
        document.getElementById('userKnowledgeLevel').addEventListener('input', function() {
            const value = parseInt(this.value);
            const levelText = value === 0 ? 'Low' : (value === 1 ? 'Medium' : 'High');
            document.getElementById('userKnowledgeValue').textContent = levelText;
        });
        
        document.getElementById('aiAccuracy').addEventListener('input', function() {
            document.getElementById('aiAccuracyValue').textContent = this.value;
        });
        
        // Event listeners for the button actions
        document.getElementById('updateEnergyChart').addEventListener('click', function() {
            initEnergyPerceptionChart();
        });
        
        document.getElementById('updateCalibrationChart').addEventListener('click', function() {
            initCalibrationGapChart();
        });
        
        document.getElementById('startSimulation').addEventListener('click', function() {
            startNewSimulation();
        });
        
        // Initialize the dashboard on page load
        window.onload = function() {
            initEnergyPerceptionChart();
            initCalibrationGapChart();
            initKnowledgeInteractionChart();
            createMethodsDiagram();
            createLLMInternalDiagram();
            
            // Initialize prediction charts
            initRelianceInteractionChart();
            initConfidenceChart();
            initCalibrationErrorChart();
            initKnowledgeModerationChart();
            initAILiteracyModerationChart();
        };
    </script>


</body></html>