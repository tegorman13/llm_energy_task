# Supplementary Materials: Survey Items

Rheu, M. (MJ), & Cho, J. (2025). The Trap of AI Literacy: The Paradoxical Relationships Between College Students’ Use of LLMs, AI Literacy, and Fact-checking Behavior. Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 1–7. https://doi.org/10.1145/3706599.3719681


This document includes the survey items used to measure the variables examined in the study by Rheu & Cho (2025).

## AI Literacy¹

Please read the following statements carefully and indicate the extent to which you agree or disagree with each statement by selecting the most appropriate response on a scale from 1 to 7.

(1= Strongly disagree, 2 = Disagree, 3 = Somewhat disagree, 4 = Neither disagree or agree, 5 = Somewhat agree, 6 = Agree, 7 = Strongly agree)

### Self-Efficacy

1.  I am confident I will perform well on LLM-related tasks.
2.  I am confident I will do well on LLMs related projects.
3.  I believe I can master LLMs knowledge and skills.
4.  I believe I can earn good grades in LLMs related assessments.

### Cognitive Learning

1.  I know what LLM is and its definition.
2.  I know how to use LLM applications (e.g., ChatGPT, Claude).
3.  I can compare the differences between AI concepts (e.g., deep learning, machine learning).
4.  I can apply LLM applications to solve problems
5.  I can create LLM-driven solutions (e.g.,Chatbots) to solve problems.
6.  I can evaluate LLM applications and concepts to different situations.

---

Please read the following statements carefully and indicate the extent to which you agree or disagree with each statement on a scale from 1 to 7.

I have knowledge of...

### AI Technology Knowledge

1.  The types of technology that LLM is built on
2.  how LLMs and other AI technologies are distinct.
3.  Use cases for LLMs.
4.  The roles that LLMs can have in human-AI interaction.

### AI-Steps Knowledge

1.  The input data requirements for LLMs.
2.  How input data is perceived by LLMs.
3.  Potential impacts that input data has on LLMs.
4.  Which input data types LLMs can use.
5.  LLMs processing methods and models.
6.  How information is presented for LLMs processing.
7.  The risks LLMs processing poses.
8.  Why LLMs processing can be described as a learning process.
9.  Using LLMs output and interpreting it.
10. LLMs output limitations.
11. How to handle LLMs output.
12. Which LLMs outputs are obtainable with current methods.

---

¹ AI literacy was measured using four sub-dimensions. The first two sub-dimensions (Self-efficacy and cognitive learning) were from Ng et al. (2024) and the other two dimensions (AI technology knowledge and AI steps knowledge) were from Pinski & Benlian (2023)

## Machine Heuristics

The following statements describe your perception of the LLMs applications you use, such as ChatGPT, Gemini, or Claude. Please read them carefully and indicate the extent to which you agree or disagree with each statement on a scale from 1 to 7.

(1= Strongly disagree, 2 = Disagree, 3 = Somewhat disagree, 4 = Neither disagree or agree, 5 = Somewhat agree, 6 = Agree, 7 = Strongly agree)

LLMs applications are...

1.  Knowledgeable
2.  Smart
3.  Clever
4.  Unbiased
5.  Non-discriminatory
6.  Impartial
7.  Not-prejudiced
8.  Even-handed
9.  Neutral
10. Precise
11. Accurate
12. Exact

*(Items 1-3 measure “expert” perception, items 4-9 measure “objective” perception, and items 10-12 measure “accurate” perception.)*

## Fact-checking Behavior

When using LLMs, how often do you engage in the following behaviors?

(1= Never, 2 = Rarely, 3 = Occasionally, 4 = Sometimes, 5 = Frequently, 6 = Most of the time, 7 = Always)

1.  Verify the credentials or qualifications of authors or source of information.
2.  Consider the currency of the information (e.g., how recent it is).
3.  Consider the reputation on trustworthiness of the website or source.
4.  Use other search engines to further research on the topic.
5.  Seek opinions or expertise of other individuals

## Credibility

In general, how credible do you perceive the LLMs you use?

*(Measured using a semantic differential scale from -3 to +3)*

*   Not Credible (-3) vs. Credible (+3)
*   Unreliable (-3) vs. Reliable (+3)
*   Not expert (-3) vs. Expert (+3)
*   Not trustworthy (-3) vs. Trustworthy (+3)
```